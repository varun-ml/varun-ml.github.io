<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Denoising Diffusion Models Part 3: Generating Characters and numbers with Diffusion Models | wity'ai</title><meta name=keywords content="diffusion model,emnist,generative ai,jax,emnist,u-net,haiku,optax,diffusion model series,diffusion model blog,diffusion model tutorial,google colab"><meta name=description content="Notebook Github Link Colab EMINST Denoising and Conditional generation Colab EMNIST Introduction We have introduced most of the concepts in the previous two blogs. In this blog post, we will see how the concepts translate to code. If you want to check out the earlier posts, you can find them here, diffusion model intro 1, and diffusion model intro 2.
EMNIST dataset Extended-MNIST dataset, as the name suggests, is an extension of the popular MNIST dataset."><meta name=author content="Varun Tulsian"><link rel=canonical href=https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-3/><meta name=google-site-verification content="G-1VE9P31T88"><link crossorigin=anonymous href=/assets/css/stylesheet.5b8287ef08b591d10eaf101babd7987c2c16890af54b921a596538e838629b23.css integrity="sha256-W4KH7wi1kdEOrxAbq9eYfCwWiQr1S5IaWWU46DhimyM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://varun-ml.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://varun-ml.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://varun-ml.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://varun-ml.github.io/apple-touch-icon.png><link rel=mask-icon href=https://varun-ml.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-3/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1VE9P31T88"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1VE9P31T88",{anonymize_ip:!1})}</script><meta property="og:title" content="Denoising Diffusion Models Part 3: Generating Characters and numbers with Diffusion Models"><meta property="og:description" content="Notebook Github Link Colab EMINST Denoising and Conditional generation Colab EMNIST Introduction We have introduced most of the concepts in the previous two blogs. In this blog post, we will see how the concepts translate to code. If you want to check out the earlier posts, you can find them here, diffusion model intro 1, and diffusion model intro 2.
EMNIST dataset Extended-MNIST dataset, as the name suggests, is an extension of the popular MNIST dataset."><meta property="og:type" content="article"><meta property="og:url" content="https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-3/"><meta property="og:image" content="https://varun-ml.github.io/images/denoising_varun.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-09T17:38:58+05:30"><meta property="article:modified_time" content="2022-12-09T17:38:58+05:30"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://varun-ml.github.io/images/denoising_varun.png"><meta name=twitter:title content="Denoising Diffusion Models Part 3: Generating Characters and numbers with Diffusion Models"><meta name=twitter:description content="Notebook Github Link Colab EMINST Denoising and Conditional generation Colab EMNIST Introduction We have introduced most of the concepts in the previous two blogs. In this blog post, we will see how the concepts translate to code. If you want to check out the earlier posts, you can find them here, diffusion model intro 1, and diffusion model intro 2.
EMNIST dataset Extended-MNIST dataset, as the name suggests, is an extension of the popular MNIST dataset."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://varun-ml.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Denoising Diffusion Models Part 3: Generating Characters and numbers with Diffusion Models","item":"https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Denoising Diffusion Models Part 3: Generating Characters and numbers with Diffusion Models","name":"Denoising Diffusion Models Part 3: Generating Characters and numbers with Diffusion Models","description":"Notebook Github Link Colab EMINST Denoising and Conditional generation Colab EMNIST Introduction We have introduced most of the concepts in the previous two blogs. In this blog post, we will see how the concepts translate to code. If you want to check out the earlier posts, you can find them here, diffusion model intro 1, and diffusion model intro 2.\nEMNIST dataset Extended-MNIST dataset, as the name suggests, is an extension of the popular MNIST dataset.","keywords":["diffusion model","emnist","generative ai","jax","emnist","u-net","haiku","optax","diffusion model series","diffusion model blog","diffusion model tutorial","google colab"],"articleBody":" Notebook Github Link Colab EMINST Denoising and Conditional generation Colab EMNIST Introduction We have introduced most of the concepts in the previous two blogs. In this blog post, we will see how the concepts translate to code. If you want to check out the earlier posts, you can find them here, diffusion model intro 1, and diffusion model intro 2.\nEMNIST dataset Extended-MNIST dataset, as the name suggests, is an extension of the popular MNIST dataset. It contains labelled 28*28*1 images of handwritten English characters (upper and lower case) and numbers.\nFigure 1: Samples from the EMNIST dataset\nLoading data We will use the Tensorflow datasets library to load the EMNIST dataset. The data will be loaded in batches of size 4*128, we will also normalize the data in the range of 0-1.\nA part of this code is adapted from the vdm - simple diffusion example colab notebook.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import jax import jax.numpy as jnp # using tensorflow libs to help load the dataset import tensorflow_datasets as tfds import tensorflow as tf from clu import deterministic_data dataset_builder = tfds.builder('emnist', data_dir=dataset_path) dataset_builder.download_and_prepare() train_split = tfds.split_for_jax_process('train+train', drop_remainder=True) def preprocess_fn(example): image = tf.cast(example['image'], 'float32') image = tf.transpose(image, (1, 0, 2,)) # normalizing values to 0-1 range image = image / 255.0 return (image, example[\"label\"] + 1) batch_size = 4 * 128 if colab else 64 train_ds = deterministic_data.create_dataset( dataset_builder, split=train_split, rng=jax.random.PRNGKey(0), shuffle_buffer_size=100, batch_dims=[jax.local_device_count(), batch_size // jax.device_count()], num_epochs=None, preprocess_fn=lambda x: preprocess_fn(x), shuffle=True) def create_input_iter(ds): def _prepare(xs): def _f(x): x = x._numpy() return x return jax.tree_util.tree_map(_f, xs) it = map(_prepare, ds) it = jax_utils.prefetch_to_device(it, 2) return it Utilities for visualizing EMNIST data Open section on utilities. We will be using these utilities to print the images generated from the diffusion models.\nA part of this code is adapted from the vdm - simple diffusion example colab notebook.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 import io import math from IPython.display import display_png import matplotlib as mpl import matplotlib.cm as cm def imify(arr, vmin=None, vmax=None, cmap=None, origin=None): \"\"\"Convert an array to an image. Arguments: arr : array-like The image data. The shape can be one of MxN (luminance), MxNx3 (RGB) or MxNx4 (RGBA). vmin : scalar, optional lower value. vmax : scalar, optional *vmin* and *vmax* set the color scaling for the image by fixing the values that map to the colormap color limits. If either *vmin* or *vmax* is None, that limit is determined from the *arr* min/max value. cmap : str or `~matplotlib.colors.Colormap`, optional A Colormap instance or registered colormap name. The colormap maps scalar data to colors. It is ignored for RGB(A) data. Defaults to :rc:`image.cmap` ('viridis'). origin : {'upper', 'lower'}, optional Indicates whether the ``(0, 0)`` index of the array is in the upper left or lower left corner of the axes. Defaults to :rc:`image.origin` ('upper'). Returns: A uint8 image array. \"\"\" sm = cm.ScalarMappable(cmap=cmap) sm.set_clim(vmin, vmax) if origin is None: origin = mpl.rcParams[\"image.origin\"] if origin == \"lower\": arr = arr[::-1] rgba = sm.to_rgba(arr, bytes=True) return rgba def rawarrview(array, **kwargs): \"\"\"Visualize an array as if it was an image in colab notebooks. Arguments: array: an array which will be turned into an image. **kwargs: Additional keyword arguments passed to imify. \"\"\" f = io.BytesIO() imarray = imify(array, **kwargs) plt.imsave(f, imarray, format=\"png\") f.seek(0) dat = f.read() f.close() display_png(dat, raw=True) def reshape_image_batch(array, cut=None, rows=None, axis=0): \"\"\"Given an array of shape [n, x, y, ...] reshape it to create an image field. Arguments: array: The array to reshape. cut: Optional cut on the number of images to view. Will default to whole array. rows: Number of rows to use. Will default to the integer less than the sqrt. axis: Axis to interpretate at the batch dimension. By default the image dimensions immediately follow. Returns: reshaped_array: An array of shape [rows * x, cut / rows * y, ...] \"\"\" original_shape = array.shape assert len(original_shape) \u003e= 2, \"array must be at least 3 Dimensional.\" if cut is None: cut = original_shape[axis] if rows is None: rows = int(math.sqrt(cut)) cols = cut // rows cut = cols * rows leading = original_shape[:axis] x_width = original_shape[axis + 1] y_width = original_shape[axis + 2] remaining = original_shape[axis + 3:] array = array[:cut] array = array.reshape(leading + (rows, cols, x_width, y_width) + remaining) array = np.moveaxis(array, axis + 2, axis + 1) array = array.reshape(leading + (rows * x_width, cols * y_width) + remaining) return array Setting up Diffusion Step Setting up a diffusion step involves:\nDefining a schedule for diffusion: $\\alpha$ Defining variables that we will be using during denoising and diffusion steps. Defining utilities to add noise to an image. You can find more details about diffusion in the earlier post.\nWe are going to use the cosine schedule for diffusion in this example.\n1 2 3 4 5 6 7 8 9 10 11 def cosine_beta_schedule(timesteps, s=0.008): \"\"\" cosine schedule as proposed in https://arxiv.org/abs/2102.09672 \"\"\" steps = timesteps + 1 x = jnp.linspace(0, timesteps, steps) alphas_cumprod = jnp.cos(((x / timesteps) + s) / (1 + s) * jnp.pi * 0.5) ** 2 alphas_cumprod = alphas_cumprod / alphas_cumprod[0] betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1]) # clipping this at 0.1 as I have found it difficult to work with higher values of beta. return jnp.clip(betas, 0.0001, 0.1) Setting up the variables:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 timesteps = 250 betas = cosine_beta_schedule(timesteps) alphas = 1 - betas alphas_ = jnp.cumprod(alphas, axis=0) variance = 1 - alphas_ sd = jnp.sqrt(variance) # these variables are used during diffusion and denoising step alphas_prev_ = jnp.pad(alphas_[:-1], [1, 0], \"constant\", constant_values=1.0) sigma_squared_q_t = (1 - alphas) * (1 - alphas_) / (1 - alphas_prev_) log_sigma_squared_q_t = jnp.log(1-alphas) + jnp.log(1-alphas_) - jnp.log(1-alphas_prev_) sigma_squared_q_t_corrected = jnp.exp(log_sigma_squared_q_t) ## following code here -- we are computing the posterior variance ## https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L196 ## https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L78 log_posterior_variance = jnp.log(jnp.hstack([posterior_variance[1], posterior_variance[1:]])) posterior_variance_corrected = jnp.exp(log_posterior_variance) Let’s define the utilities as well, adding noise using the re-parameterization trick. The diffusion step takes in data, time step value and adds noise to the data according to the equation below. It uses the schedule $\\alpha$ defined above.\n$$ \\begin{align} q_t(x_t|x_0) \u0026= \\sqrt{\\bar\\alpha_t}x_0 + \\sqrt{(1 - \\bar\\alpha_t )}\\ast\\epsilon_0^\\ast ; \\space where \\space \\epsilon_0^\\ast \\in N(0, I) \\cr \u0026= N(\\sqrt{\\bar\\alpha_t}x_0, (1 - \\bar\\alpha_t)I) \\cr \\end{align} $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # how to add noise to the data @jax.jit : jit compilazation to significantly speed up code def get_noisy(rng, batch, timestep): timestep = einops.repeat(timestep, 'b -\u003e b 28 28 1') # we will use the reparameterization trick # need to generate new keys everytime _, noise_key = jax.random.split(rng) noise_at_t = jax.random.normal(noise_key, shape=batch.shape) added_noise_at_t = jnp.add(batch * jnp.sqrt(alphas_[timestep]), noise_at_t * sd[timestep]) return added_noise_at_t, noise_at_t # recovering original data by removing noise def recover_original(batch, timestep, noise): true_data = jnp.subtract(batch, noise*sd[timestep])/(jnp.sqrt(alphas_[timestep])) return true_data Take a random data point and add noise over multiple steps. Click to see the code. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 random_index = 22 image = next(create_input_iter(train_ds))[0][0][random_index] fig = plt.figure() ims = [] noisy_images, _ = get_noisy(key, einops.repeat(image, 'h w c -\u003e b h w c', b=timesteps//5), jnp.arange(1, timesteps, 5)) if colab: noisy_images = einops.rearrange(noisy_images, 'b h w c -\u003e b h (w c)') noisy_images = unnormalize(noisy_images) for i in range(timesteps//10): im = plt.imshow(noisy_images[i], cmap=\"gray\", animated=True) ims.append([im]) animate = animation.ArtistAnimation(fig, ims, interval=5, blit=True, repeat_delay=3000) animate.save(gifs_dir+'diffusion.gif', writer='pillow') with open(gifs_dir + 'diffusion.gif','rb') as f: display(Image(data=f.read(), format='png')) Figure 2: Using the utilities defined above to add Gaussian Noise in multiple steps to a handwritten character ‘z’\nThe Denoising Model: U-Net In the earlier blog posts, we were working on 2-d samples. The denoising model was a multi layer perceptron with GeLU activations. In the case of EMNIST we will have to denoise images. U-Nets are the recommended models to do this.\nFigure 3: U-Net architecture. It’s similar to the one we will build.\nThe U-Net architecture has the following characteristics:\nIt’s typically represented as a U block. The first part of the U block downsamples the image. We go from a 28*28 image to a 7*7 image. This is done using Downsampling convolution blocks. With downsampling, we increase the number of channels (features). We go from 1 channel in the input to 192 channels at the end of the first part of the U-net. The 2nd part of the U block does the opposite of the 1st part. We go from a 7*7 image to a 28*28 image, and also go from 192 channels to 1 channel. Upsampling is done using Upsampling convolutional blocks. At the same time, the U-Net has residual connections, connecting layers in the downsampling part to the layers in the upsampling part, this makes training the network efficient. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # upsample operation in the UNET class Downsample(hk.Module): def __init__(self, output_channels): super().__init__() self.conv = hk.Conv2D(output_channels=output_channels, kernel_shape=(4, 4), stride=2, padding=[1, 1]) def __call__(self, x): return self.conv(x) # Downsample operation in the UNET class Upsample(hk.Module): def __init__(self, output_channels): super().__init__() self.conv = hk.Conv2D(output_channels=output_channels, kernel_shape=(3, 3), padding='SAME') def __call__(self, x): # scaling image to twice size x = einops.repeat(x, 'b h w c -\u003e b (a h) (aa w) c', a=2, aa=2) return self.conv(x) Time Step Embeddings: We will be using sinusoidal embeddings for time steps. This is following the discussion in the previous post.\n1 2 3 4 5 6 7 8 9 10 11 12 class TimeEmbeddings(hk.Module): def __init__(self, dim): super().__init__() half_dim = dim // 2 embeddings = math.log(10000) / (half_dim - 1) self.embeddings = jnp.exp(jnp.arange(half_dim) * -embeddings) def __call__(self, timesteps): embeddings = self.embeddings embeddings = jnp.multiply(timesteps[:, None], embeddings[:, None].T) embeddings = jnp.hstack([jnp.sin(embeddings), jnp.cos(embeddings)]) return embeddings Conditional Labels: EMNIST dataset has 62 labels, 26 characters in lowercase, 26 characters in uppercase, 10 numbers. We add another label to represent the masked label. We use the Haiku Embed class to generate embedding vectors for these labels. Labels are fused with the input as explained in the previous post. Instead of fusing label and time step information only at the start, we are fusing it with the input at every Block.\nNetwork Definition: After convolution layers, I am using a BatchNorm1 layer, using the Haiku BatchNorm module to normalize the values across the batch. This is shown to improve the convergence of deep models. Using BatchNorm complicates the implementation a bit since, BatchNorm layers need to maintain a buffer states for mean and variance of the activations. This video from Yannic is an excellent introduction to the different kind of Normalizations that one can apply to speed up training.\nThe code below follows the U-net implementation in LucidRains Diffusion model implementation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 # Unet class to predict noise from a given image class UNet(hk.Module): def __init__(self): super().__init__() self.init_conv = hk.Conv2D(output_channels=48, kernel_shape = (5,5), padding='SAME', with_bias=False) self.norm = hk.BatchNorm(True, True, decay_rate=0.9) self.silu = jax.nn.silu self.block1 = Block(output_channels=48, kernel_size=3, padding=1) self.downsample1 = Downsample(96) self.block2 = Block(output_channels=96, kernel_size=3, padding=1) self.downsample2 = Downsample(192) self.middle_block = Block(output_channels=192, kernel_size=3, padding=1) self.upsample1 = Upsample(96) self.block3 = Block(output_channels=96, kernel_size=3, padding=1) self.upsample2 = Upsample(48) self.block4 = Block(output_channels=48, kernel_size=3, padding=1) self.conv1 = hk.Conv2D(output_channels=48, kernel_shape = (3,3), padding='SAME', with_bias=False) self.norm1 = hk.BatchNorm(True, True, decay_rate=0.9) self.conv2 = hk.Conv2D(output_channels=1, kernel_shape = (5,5), padding='SAME') self.time_mlp = hk.Sequential([ hk.Linear(256), jax.nn.gelu, hk.Linear(256), ]) # conditional vectors encoding self.embedding_vectors = hk.Embed(10+26+26+1, 63) self.timestep_embeddings = TimeEmbeddings(96) def __call__(self, x, timesteps, cond=None, is_training=False): cond_embedding = None conditioning = None if timesteps is not None: timestep_embeddings = self.timestep_embeddings(timesteps) conditioning = timestep_embeddings if cond is not None: label_embeddings = self.embedding_vectors(cond) conditioning = jnp.concatenate([label_embeddings, conditioning], axis=1) if conditioning is not None: cond_embedding = self.time_mlp(conditioning) h = self.silu(self.norm(self.init_conv(x), is_training)) xx = jnp.copy(h) b1 = self.block1(h, cond_embedding, is_training) h = self.downsample1(b1) b2 = self.block2(h, cond_embedding, is_training) h = self.downsample2(b2) h = self.upsample1(self.middle_block(h, cond_embedding, is_training)) b3 = self.block3(jnp.concatenate((h, b2), axis=3), cond_embedding, is_training) h = self.upsample2(b3) b4 = self.block4(jnp.concatenate((h, b1), axis=3), cond_embedding, is_training) h = self.conv2(self.silu(self.norm1(self.conv1(jnp.concatenate((xx, b4), axis=3)), is_training))) return h class Block(hk.Module): # a basic resnet style convolutional block def __init__(self, output_channels, kernel_size, padding): super().__init__() self.proj = hk.Conv2D(output_channels=output_channels, kernel_shape=(kernel_size, kernel_size), padding='SAME', with_bias=False) # using batch norm instead of layernorm as the batch sizes are large # orig: self.norm = hk.LayerNorm(axis=(-3, -2, -1), create_scale=True, create_offset=True) self.norm = hk.BatchNorm(True, True, decay_rate=0.9) self.silu = jax.nn.silu self.conv1 = hk.Conv2D(output_channels=output_channels, kernel_shape=(kernel_size, kernel_size), padding='SAME', with_bias=False) self.norm1 = hk.BatchNorm(True, True, decay_rate=0.9) self.out_conv = hk.Conv2D(output_channels=output_channels, kernel_shape=(1, 1), padding='SAME') # self.time_mlp = None dims = output_channels self.time_mlp = hk.Sequential([ jax.nn.silu, hk.Linear(dims*2), ]) def __call__(self, x, timestep_embeddings=None, is_training=False): h = self.proj(x) h = self.norm(h, is_training) if timestep_embeddings is not None and self.time_mlp is not None: time_embedding = self.time_mlp(timestep_embeddings) time_embedding = einops.rearrange(time_embedding, 'b c -\u003e b 1 1 c') shift, scale = jnp.split(time_embedding, indices_or_sections=2, axis=-1) h = shift + (scale+1)*h h = self.silu(self.norm1(self.conv1(self.silu(h)), is_training)) return self.out_conv(x) + h Training Code: Loss function: I am using the Huber loss instead of using the L2 or the L1 loss. I would assume changing the loss function wouldn’t impact the results significantly.\nNote: the importance weight determines the importance of the sample for denoising. As noted in Improved Denoising Diffusion Probabilistic Models2, one idea to train diffusion models is to completely ignore this term. This helps the model training.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # using jax.jit to speed up computation of the loss function partial(jax.jit, static_argnums=(4,)) def compute_loss(params: hk.Params, state: hk.State, batch: Batch, is_energy_method: bool=False, is_training=False) -\u003e Tuple[jnp.ndarray, Tuple[jnp.ndarray, hk.State]]: \"\"\"Compute the loss of the network, including L2.\"\"\" x, label, timestep, noise = batch # not capturing state as it is not needed; it should be internally updated and maintaing by haiku and doesn't need gradient updates pred_data, state = net.apply(params, state, x, timestep, label, is_training) def error_func(): imp_weight = 1.0 # 1/2 * (1/sigma_squared_q_t_corrected[timestep]) * ((betas[timestep])**2 / (variance[timestep] * alphas[timestep])) # loss on prediction loss_ = jnp.mean(jnp.multiply(imp_weight, huber_loss(noise, pred_data))) return loss_ def energy_func(): ## Energy function interpretation imp_weight = 1.0 # 1/2 * (1/sigma_squared_q_t_corrected[timestep]) * ((betas[timestep])**2 / (alphas[timestep])) # loss on prediction loss_ = jnp.mean(jnp.multiply(imp_weight, huber_loss(pred_data, jnp.divide(noise, -sd[timestep])))) return loss_ loss_ = jax.lax.cond(is_energy_method, energy_func, error_func) return loss_, (loss_, state) Updating Model Weights:\nThis is a typical code for any Neural Network training in Haiku/Optax and JAX. We are finding the gradient of the loss with respect to the parameters of the Neural Network using JAX and updating the weights using Optax.\nAdditionally, we are doing exponential updates to the parameters. Paper on Polyak averaging.3\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 @jax.jit def update( params: hk.Params, state: hk.State, opt_state: optax.OptState, batch: Batch, is_energy_method: bool=False) -\u003e Tuple[jnp.ndarray, hk.Params, optax.OptState, hk.State]: \"\"\"Compute gradients and update the weights\"\"\" grads, (loss_value, state)= jax.grad(compute_loss, has_aux=True)(params, state, batch, is_energy_method, is_training=True) updates, opt_state = opt.update(grads, opt_state) new_params = optax.apply_updates(params, updates) return loss_value, new_params, opt_state, state @jax.jit def ema_update(params, avg_params): \"\"\"Incrementally update parameters via polyak averaging.\"\"\" # Polyak averaging tracks an (exponential moving) average of the past parameters of a model, for use at test/evaluation time. return optax.incremental_update(params, avg_params, step_size=0.95) Training Code The training code is pretty straight-forward. Below, I have removed the code to checkpoint the different models.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 # initialization def f(x, timesteps, label, is_training): unet = UNet() return unet(x, timesteps, label, is_training) f_t = hk.transform_with_state(f) net = hk.without_apply_rng(f_t) params, state = net.init(rng, image[0][0:batch_size], timesteps_, label[0][0:batch_size], is_training=True) opt = optax.adam(1e-3) avg_params = deepcopy(params) opt_state = opt.init(params) batches_iter = 10000 # maintaining a batch on which we will measure loss; we will save the model based on performance on this batch one_timestep = jnp.mod(jnp.arange(1, batch_size+1), timesteps) train = create_input_iter(train_ds) data_in_batch_, label_ = next(train) data_in_batch_ = data_in_batch_[0] label_ = label_[0] data_noisy_temp_, noise_temp_ = get_noisy(key, data_in_batch_, one_timestep) # main method for training def train_model(opt_state, params, avg_params, state, energy_method=False): best_loss = sys.float_info.max # initialization unique_key = jax.random.fold_in(key, batch_size) # same subkey being used for noise sampling, as it doesn't matter :) _, *timestep_subkeys = jax.random.split(unique_key, batches_iter+1) losses = [] for iteration in range(0, batches_iter): data_in_batch, label = next(train) data_in_batch = data_in_batch[0] label = label[0] idx = (jax.random.uniform(key=timestep_subkeys[iteration], shape=(batch_size, 1)) * (timesteps-1)).astype(int) idx = einops.rearrange(idx, 'a b -\u003e (a b)') timestep = idx + 1 data_noisy, noise = get_noisy(timestep_subkeys[iteration], data_in_batch, timestep) # todo: call gradient update function here loss_value, params, opt_state, state = update(params, state, opt_state, [data_noisy, label, timestep, noise], energy_method) avg_params = ema_update(params, avg_params) ## evaluating noise on a fixed timestep to calculate best model loss_temp, _ = jax.device_get(compute_loss(avg_params, state, [data_noisy_temp_, label_, one_timestep, noise_temp_], energy_method, is_training=False)) losses.append(loss_temp) if loss_temp \u003c best_loss: best_loss = loss_temp print(f\"saving iteration: {iteration} loss: {best_loss:\u003e7f}\") return data_noisy, data_in_batch, timestep, losses, avg_params, state Generating Conditional Samples Using all time steps for generation Code for generating samples is below. We start off with random samples from a Standard Gaussian Distribution and follow steps as described here. We will be using the naïve version of classifier guidance described here.\nStart by sampling a random image from a Sandard Gaussian distribution at time step $T$. Get an estimate of the error added to the input image from the U-net. Use the equation 5, in the link above, to calculate the image at the previous time step $T-1$. Repeat until time step 1. 1 2 3 4 5 6 7 8 9 # defining useful variables alphas_prev_ = jnp.pad(alphas_[:-1], [1, 0], \"constant\", constant_values=1.0) sigma_squared_q_t = (1 - alphas) * (1 - alphas_) / (1 - alphas_prev_) log_sigma_squared_q_t = jnp.log(1-alphas) + jnp.log(1-alphas_) - jnp.log(1-alphas_prev_) sigma_squared_q_t_corrected = jnp.exp(log_sigma_squared_q_t) key = jax.random.PRNGKey(42) mean_coeff_1 = jnp.sqrt(alphas)*(1 - alphas_prev_) / variance mean_coeff_2 = jnp.sqrt(alphas_prev_) * betas / variance Generate Samples:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import random def generate_data(avg_params, state, label, energy_method=False, clipped_version=False): batch_size_generation = len(label) unique_key = jax.random.fold_in(key, random.randint(1, 100)) _, subkey = jax.random.split(unique_key) _, *subkeys = jax.random.split(unique_key, timesteps+1) # need to generate new keys everytime data_noisy = jax.random.normal(subkey, shape=(batch_size_generation, 28, 28, 1)) data_in_batch = data_noisy datas = [] datas.append(jax.device_get(data_noisy)) for t in range(1, timesteps+1): timestep = timesteps-t t_repeated = jnp.repeat(jnp.array([timestep]), batch_size_generation) # data_stacked = torch.vstack([data_in_batch, labelled_values]) pred_data, _ = net.apply(avg_params, state, data_in_batch, t_repeated, label, is_training=False) # clipping an improvement as recommended in https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils.py#L171 # this helps in improving the samples generated as it keeps the random variables in the range of 0 to +1 x_reconstructed = jnp.subtract(data_in_batch, pred_data * sd[timestep])/ jnp.sqrt(alphas_[timestep]) if timestep \u003e= 0: x_reconstructed = jnp.clip(x_reconstructed, 0., 1.) mean_data_1 = data_in_batch * mean_coeff_1[timestep] mean_data_2 = x_reconstructed * mean_coeff_2[timestep] mean_data = jnp.add(mean_data_1, mean_data_2) posterior_data = posterior_variance_corrected[timestep] data_noisy = jax.random.normal(subkeys[t-1], shape=(batch_size_generation, 28, 28, 1)) data_in_batch = jnp.add(mean_data, jnp.sqrt(posterior_data) * data_noisy) datas.append(jax.device_get(data_in_batch)) return datas, data_in_batch Striding: Reducing steps needed for Generation We could reduce the number of steps needed for generation. One of the popular approaches to do this is by using Time Step Striding.\nInstead of moving stepsone at a time from $T$ to 1, range(T, 1, -1), we will move $s$ steps at a time, range(T, 1, -s). Doing this will speed up generation of new samples from the model by s times. I have been able to produce good quality samples with $s$ set to 5.\nWe just need to make minor adjustments to the variables we created so that the maths still works.\n1 2 3 4 5 6 7 8 9 10 11 12 strided_schedule = jnp.array(list(range(1, timesteps, 5)) + [timesteps]) alphas_strided_ = alphas_[strided_schedule] alphas_prev_strided_ = jnp.pad(alphas_strided_[:-1], [1, 0], \"constant\", constant_values=1.0) betas_strided = 1 - (alphas_strided_/alphas_prev_strided_) posterior_variance_new_schedule = betas_strided * (1 - alphas_prev_strided_)/ (1-alphas_strided_) log_posterior_variance = jnp.log(jnp.hstack([posterior_variance_new_schedule[1], posterior_variance_new_schedule[1:]])) posterior_variance_new_schedule_corrected = jnp.exp(log_posterior_variance) mean_coeff_1_strided = jnp.sqrt(1-betas_strided)*(1 - alphas_prev_strided_) / (1 - alphas_strided_) mean_coeff_2_strided = jnp.sqrt(alphas_prev_strided_) * betas_strided / (1 - alphas_strided_) Generate Samples:\nThe code to generate samples is pretty similar, now we need to use the strided variables defined above.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import random def generate_data_strided(avg_params, state, label, energy_method=False, clipped_version=False): batch_size_generation = len(label) unique_key = jax.random.fold_in(key, random.randint(1, 100)) _, subkey = jax.random.split(unique_key) _, *subkeys = jax.random.split(unique_key, len(strided_schedule)+1) data_noisy = jax.random.normal(subkey, shape=(batch_size_generation, 28, 28, 1)) data_in_batch = data_noisy datas = [] datas.append(jax.device_get(data_noisy)) for t in range(1, len(strided_schedule)+1): stride_timestep = len(strided_schedule)-t timestep = strided_schedule[stride_timestep] t_repeated = jnp.repeat(jnp.array([timestep]), batch_size_generation) # data_stacked = torch.vstack([data_in_batch, labelled_values]) pred_data, _ = net.apply(avg_params, state, data_in_batch, t_repeated, label, is_training=False) # Clipping helps in improving the samples generated as it keeps the random variables in the range of 0 to +1 x_reconstructed = jnp.subtract(data_in_batch, pred_data * sd[timestep])/ jnp.sqrt(alphas_[timestep]) if timestep \u003e= 0: x_reconstructed = jnp.clip(x_reconstructed, 0., 1.) mean_data_1 = data_in_batch * mean_coeff_1_strided[stride_timestep] mean_data_2 = x_reconstructed * mean_coeff_2_strided[stride_timestep] mean_data = jnp.add(mean_data_1, mean_data_2) posterior_data = posterior_variance_new_schedule_corrected[stride_timestep] data_noisy = jax.random.normal(subkeys[t-1], shape=(batch_size_generation, 28, 28, 1)) data_in_batch = jnp.add(mean_data, jnp.sqrt(posterior_data) * data_noisy) datas.append(jax.device_get(data_in_batch)) return datas, data_in_batch Generating Samples \u0026 Outputs Let’s first create a map between the label values and the characters.\n1 2 3 4 5 6 7 8 9 10 11 import string x = list(range(1, 63)) y = [str(i) for i in (list(range(0, 10)))] + list(string.ascii_uppercase + string.ascii_lowercase) dict_ = {} for i in x: dict_[y[i-1]] = i def get_label(ans): return jnp.array([dict_[str.upper(char)] for char in ans]) Generating Samples:\nGenerating ‘varun’ using the trained diffusion model.\n1 2 3 4 5 6 datas, d = generate_data_strided(avg_params, state, label= \"varun\", energy_method=False, clipped_version=True) datas_ = jnp.clip(jnp.array(datas[0: -1 :2]), 0., 1.) d_ = einops.rearrange(datas_, 'a b c d e -\u003e (b a) c d e') rawarrview(reshape_image_batch(datas[-1].squeeze(), rows=1), cmap='bone_r') rawarrview(reshape_image_batch(d_.squeeze(), rows=5), cmap='bone_r') Figure 4: ‘varun’ generated using strided sampling technique.\nFigure 5: Visualizing the steps in the diffusion process. If you notice carefully, not much is happening at the early stages.\nGenerating ‘tulsian’ using the trained diffusion model.\n1 2 3 4 5 datas, d = generate_data_strided(avg_params, state, label= \"tulsian\", energy_method=False, clipped_version=True) datas_ = jnp.clip(jnp.array(datas[0: -1 :2]), 0., 1.) d_ = einops.rearrange(datas_, 'a b c d e -\u003e (b a) c d e') rawarrview(reshape_image_batch(datas[-1].squeeze(), rows=1), cmap='bone_r') rawarrview(reshape_image_batch(d_.squeeze(), rows=7), cmap='bone_r') Figure 6: ’tulsian’ generated using strided sampling technique.\nFigure 7: Visualizing the steps in the diffusion process. Similar to earlier, not much is happening at the early stages.\nConclusion Denoising Diffusion models are a powerful algorithmic tool for Generative AI. Although much of the work done so far focusses on images, we could generate any distribution using these techniques.\nI sincerely hope this introduction was useful to you. Please explore additional resources here.\nIoffe et al. 2015 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift ↩︎\nNichol et al. 2021 Improved Denoising Diffusion Probabilistic Models ↩︎\nPolyak et al. 1991Acceleration of Stochastic Approximation by Averaging ↩︎\n","wordCount":"4019","inLanguage":"en","image":"https://varun-ml.github.io/images/denoising_varun.png","datePublished":"2022-12-09T17:38:58+05:30","dateModified":"2022-12-09T17:38:58+05:30","author":{"@type":"Person","name":"Varun Tulsian"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-3/"},"publisher":{"@type":"Organization","name":"wity'ai","logo":{"@type":"ImageObject","url":"https://varun-ml.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://varun-ml.github.io/ accesskey=h title="wity'ai (Alt + H)">wity'ai</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://varun-ml.github.io/posts title=Posts><span>Posts</span></a></li><li><a href=https://varun-ml.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://varun-ml.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://varun-ml.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://varun-ml.github.io/resume.pdf title=Resume><span>Resume</span></a></li><li><a href=https://varun-ml.github.io/faq title=Questions><span>Questions</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://varun-ml.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://varun-ml.github.io/posts/>Posts</a></div><h1 class=post-title>Denoising Diffusion Models Part 3: Generating Characters and numbers with Diffusion Models</h1><div class=post-meta><span title='2022-12-09 17:38:58 +0530 IST'>December 9, 2022</span>&nbsp;·&nbsp;19 min&nbsp;·&nbsp;4019 words&nbsp;·&nbsp;Varun Tulsian</div></header><figure class=entry-cover><img loading=lazy src=https://varun-ml.github.io/images/denoising_varun.png alt="Third part tutorial for density generation using diffusion models"><p>Denoising in action. Characters generated using EMNIST dataset. Left to Right: Noisy image is getting denoised.</p></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#emnist-dataset>EMNIST dataset</a><ul><li><a href=#loading-data>Loading data</a></li><li><a href=#utilities-for-visualizing-emnist-data>Utilities for visualizing EMNIST data</a></li></ul></li><li><a href=#setting-up-diffusion-step>Setting up Diffusion Step</a></li><li><a href=#the-denoising-model-u-net>The Denoising Model: U-Net</a></li><li><a href=#training-code>Training Code:</a><ul><li><a href=#training-code-1>Training Code</a></li></ul></li><li><a href=#generating-conditional-samples>Generating Conditional Samples</a><ul><li><a href=#using-all-time-steps-for-generation>Using all time steps for generation</a></li><li><a href=#striding-reducing-steps-needed-for-generation>Striding: Reducing steps needed for Generation</a></li></ul></li><li><a href=#generating-samples--outputs>Generating Samples & Outputs</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><table><thead><tr><th style=text-align:left>Notebook</th><th style=text-align:left>Github Link</th><th style=text-align:left>Colab</th></tr></thead><tbody><tr><td style=text-align:left>EMINST Denoising and Conditional generation</td><td style=text-align:left><a href=https://github.com/varun-ml/diffusion-models-tutorial/blob/master/emnist-colab-notebooks/colab_EMNIST_conditional.diffusion_model.large.with_batch_norm.ipynb target=_blank>Colab EMNIST</a></td><td style=text-align:left><a href=https://colab.research.google.com/github/varun-ml/diffusion-models-tutorial/blob/master/emnist-colab-notebooks/colab_EMNIST_conditional.diffusion_model.large.with_batch_norm.ipynb target=_blank><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Colab (Large)">
</a><a href=https://colab.research.google.com/github/varun-ml/diffusion-models-tutorial/blob/master/emnist-colab-notebooks/colab_EMNIST_conditional.diffusion_model.ipynb target=_blank><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Colab (Small)"></a></td></tr></tbody></table><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>We have introduced most of the concepts in the previous two blogs. In this blog post, we will see how the concepts translate to code. If you want to check out the earlier posts, you can find them here, <a href=/posts/diffusion-models/denoising-diffusion-models-1>diffusion model intro 1</a>, and <a href=/posts/diffusion-models/denoising-diffusion-models-2>diffusion model intro 2</a>.</p><h2 id=emnist-dataset>EMNIST dataset<a hidden class=anchor aria-hidden=true href=#emnist-dataset>#</a></h2><p><a href=https://www.tensorflow.org/datasets/catalog/emnist target=_blank>Extended-MNIST dataset</a>, as the name suggests, is an extension of the popular MNIST dataset. It contains labelled 28*28*1 images of handwritten English characters (upper and lower case) and numbers.</p><figure class=align-center><img loading=lazy src=/images/emnist-sample.png#center alt="Figure 1: Samples from the EMNIST dataset" width=50%><figcaption><p>Figure 1: Samples from the EMNIST dataset</p></figcaption></figure><h3 id=loading-data>Loading data<a hidden class=anchor aria-hidden=true href=#loading-data>#</a></h3><p>We will use the Tensorflow datasets library to load the EMNIST dataset. The data will be loaded in batches of size 4*128, we will also normalize the data in the range of 0-1.</p><p>A part of this code is adapted from the <a href=https://colab.research.google.com/github/google-research/vdm/blob/main/colab/SimpleDiffusionColab.ipynb target=_blank>vdm - simple diffusion example colab notebook</a>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-0-1><a class=lnlinks href=#hl-0-1> 1</a>
</span><span class=lnt id=hl-0-2><a class=lnlinks href=#hl-0-2> 2</a>
</span><span class=lnt id=hl-0-3><a class=lnlinks href=#hl-0-3> 3</a>
</span><span class=lnt id=hl-0-4><a class=lnlinks href=#hl-0-4> 4</a>
</span><span class=lnt id=hl-0-5><a class=lnlinks href=#hl-0-5> 5</a>
</span><span class=lnt id=hl-0-6><a class=lnlinks href=#hl-0-6> 6</a>
</span><span class=lnt id=hl-0-7><a class=lnlinks href=#hl-0-7> 7</a>
</span><span class=lnt id=hl-0-8><a class=lnlinks href=#hl-0-8> 8</a>
</span><span class=lnt id=hl-0-9><a class=lnlinks href=#hl-0-9> 9</a>
</span><span class=lnt id=hl-0-10><a class=lnlinks href=#hl-0-10>10</a>
</span><span class=lnt id=hl-0-11><a class=lnlinks href=#hl-0-11>11</a>
</span><span class=lnt id=hl-0-12><a class=lnlinks href=#hl-0-12>12</a>
</span><span class=lnt id=hl-0-13><a class=lnlinks href=#hl-0-13>13</a>
</span><span class=lnt id=hl-0-14><a class=lnlinks href=#hl-0-14>14</a>
</span><span class=lnt id=hl-0-15><a class=lnlinks href=#hl-0-15>15</a>
</span><span class=lnt id=hl-0-16><a class=lnlinks href=#hl-0-16>16</a>
</span><span class=lnt id=hl-0-17><a class=lnlinks href=#hl-0-17>17</a>
</span><span class=lnt id=hl-0-18><a class=lnlinks href=#hl-0-18>18</a>
</span><span class=lnt id=hl-0-19><a class=lnlinks href=#hl-0-19>19</a>
</span><span class=lnt id=hl-0-20><a class=lnlinks href=#hl-0-20>20</a>
</span><span class=lnt id=hl-0-21><a class=lnlinks href=#hl-0-21>21</a>
</span><span class=lnt id=hl-0-22><a class=lnlinks href=#hl-0-22>22</a>
</span><span class=lnt id=hl-0-23><a class=lnlinks href=#hl-0-23>23</a>
</span><span class=lnt id=hl-0-24><a class=lnlinks href=#hl-0-24>24</a>
</span><span class=lnt id=hl-0-25><a class=lnlinks href=#hl-0-25>25</a>
</span><span class=lnt id=hl-0-26><a class=lnlinks href=#hl-0-26>26</a>
</span><span class=lnt id=hl-0-27><a class=lnlinks href=#hl-0-27>27</a>
</span><span class=lnt id=hl-0-28><a class=lnlinks href=#hl-0-28>28</a>
</span><span class=lnt id=hl-0-29><a class=lnlinks href=#hl-0-29>29</a>
</span><span class=lnt id=hl-0-30><a class=lnlinks href=#hl-0-30>30</a>
</span><span class=lnt id=hl-0-31><a class=lnlinks href=#hl-0-31>31</a>
</span><span class=lnt id=hl-0-32><a class=lnlinks href=#hl-0-32>32</a>
</span><span class=lnt id=hl-0-33><a class=lnlinks href=#hl-0-33>33</a>
</span><span class=lnt id=hl-0-34><a class=lnlinks href=#hl-0-34>34</a>
</span><span class=lnt id=hl-0-35><a class=lnlinks href=#hl-0-35>35</a>
</span><span class=lnt id=hl-0-36><a class=lnlinks href=#hl-0-36>36</a>
</span><span class=lnt id=hl-0-37><a class=lnlinks href=#hl-0-37>37</a>
</span><span class=lnt id=hl-0-38><a class=lnlinks href=#hl-0-38>38</a>
</span><span class=lnt id=hl-0-39><a class=lnlinks href=#hl-0-39>39</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>jax</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>jax.numpy</span> <span class=k>as</span> <span class=nn>jnp</span>
</span></span><span class=line><span class=cl><span class=c1># using tensorflow libs to help load the dataset</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorflow_datasets</span> <span class=k>as</span> <span class=nn>tfds</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span></span><span class=line><span class=cl><span class=n>from</span><span class=err> </span><span class=n>clu</span><span class=err> </span><span class=n>import</span><span class=err> </span><span class=n>deterministic_data</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>dataset_builder</span> <span class=o>=</span> <span class=n>tfds</span><span class=o>.</span><span class=n>builder</span><span class=p>(</span><span class=s1>&#39;emnist&#39;</span><span class=p>,</span> <span class=n>data_dir</span><span class=o>=</span><span class=n>dataset_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>dataset_builder</span><span class=o>.</span><span class=n>download_and_prepare</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>train_split</span> <span class=o>=</span> <span class=n>tfds</span><span class=o>.</span><span class=n>split_for_jax_process</span><span class=p>(</span><span class=s1>&#39;train+train&#39;</span><span class=p>,</span> <span class=n>drop_remainder</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>preprocess_fn</span><span class=p>(</span><span class=n>example</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>image</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>example</span><span class=p>[</span><span class=s1>&#39;image&#39;</span><span class=p>],</span> <span class=s1>&#39;float32&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>image</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>image</span><span class=p>,</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,))</span>
</span></span><span class=line><span class=cl>  <span class=c1># normalizing values to 0-1 range</span>
</span></span><span class=line><span class=cl>  <span class=n>image</span> <span class=o>=</span> <span class=n>image</span> <span class=o>/</span> <span class=mf>255.0</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=p>(</span><span class=n>image</span><span class=p>,</span> <span class=n>example</span><span class=p>[</span><span class=s2>&#34;label&#34;</span><span class=p>]</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>4</span> <span class=o>*</span> <span class=mi>128</span> <span class=k>if</span> <span class=n>colab</span> <span class=k>else</span> <span class=mi>64</span>
</span></span><span class=line><span class=cl><span class=n>train_ds</span> <span class=o>=</span> <span class=n>deterministic_data</span><span class=o>.</span><span class=n>create_dataset</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>dataset_builder</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>split</span><span class=o>=</span><span class=n>train_split</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>rng</span><span class=o>=</span><span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>PRNGKey</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>shuffle_buffer_size</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_dims</span><span class=o>=</span><span class=p>[</span><span class=n>jax</span><span class=o>.</span><span class=n>local_device_count</span><span class=p>(),</span> <span class=n>batch_size</span> <span class=o>//</span> <span class=n>jax</span><span class=o>.</span><span class=n>device_count</span><span class=p>()],</span>
</span></span><span class=line><span class=cl>    <span class=n>num_epochs</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>preprocess_fn</span><span class=o>=</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>preprocess_fn</span><span class=p>(</span><span class=n>x</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>create_input_iter</span><span class=p>(</span><span class=n>ds</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>_prepare</span><span class=p>(</span><span class=n>xs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_f</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>_numpy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>jax</span><span class=o>.</span><span class=n>tree_util</span><span class=o>.</span><span class=n>tree_map</span><span class=p>(</span><span class=n>_f</span><span class=p>,</span> <span class=n>xs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>it</span> <span class=o>=</span> <span class=nb>map</span><span class=p>(</span><span class=n>_prepare</span><span class=p>,</span> <span class=n>ds</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>it</span> <span class=o>=</span> <span class=n>jax_utils</span><span class=o>.</span><span class=n>prefetch_to_device</span><span class=p>(</span><span class=n>it</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>it</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=utilities-for-visualizing-emnist-data>Utilities for visualizing EMNIST data<a hidden class=anchor aria-hidden=true href=#utilities-for-visualizing-emnist-data>#</a></h3><details><summary>Open section on utilities.</summary><p>We will be using these utilities to print the images generated from the diffusion models.</p><p>A part of this code is adapted from the <a href=https://colab.research.google.com/github/google-research/vdm/blob/main/colab/SimpleDiffusionColab.ipynb target=_blank>vdm - simple diffusion example colab notebook</a>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-1-1><a class=lnlinks href=#hl-1-1> 1</a>
</span><span class=lnt id=hl-1-2><a class=lnlinks href=#hl-1-2> 2</a>
</span><span class=lnt id=hl-1-3><a class=lnlinks href=#hl-1-3> 3</a>
</span><span class=lnt id=hl-1-4><a class=lnlinks href=#hl-1-4> 4</a>
</span><span class=lnt id=hl-1-5><a class=lnlinks href=#hl-1-5> 5</a>
</span><span class=lnt id=hl-1-6><a class=lnlinks href=#hl-1-6> 6</a>
</span><span class=lnt id=hl-1-7><a class=lnlinks href=#hl-1-7> 7</a>
</span><span class=lnt id=hl-1-8><a class=lnlinks href=#hl-1-8> 8</a>
</span><span class=lnt id=hl-1-9><a class=lnlinks href=#hl-1-9> 9</a>
</span><span class=lnt id=hl-1-10><a class=lnlinks href=#hl-1-10>10</a>
</span><span class=lnt id=hl-1-11><a class=lnlinks href=#hl-1-11>11</a>
</span><span class=lnt id=hl-1-12><a class=lnlinks href=#hl-1-12>12</a>
</span><span class=lnt id=hl-1-13><a class=lnlinks href=#hl-1-13>13</a>
</span><span class=lnt id=hl-1-14><a class=lnlinks href=#hl-1-14>14</a>
</span><span class=lnt id=hl-1-15><a class=lnlinks href=#hl-1-15>15</a>
</span><span class=lnt id=hl-1-16><a class=lnlinks href=#hl-1-16>16</a>
</span><span class=lnt id=hl-1-17><a class=lnlinks href=#hl-1-17>17</a>
</span><span class=lnt id=hl-1-18><a class=lnlinks href=#hl-1-18>18</a>
</span><span class=lnt id=hl-1-19><a class=lnlinks href=#hl-1-19>19</a>
</span><span class=lnt id=hl-1-20><a class=lnlinks href=#hl-1-20>20</a>
</span><span class=lnt id=hl-1-21><a class=lnlinks href=#hl-1-21>21</a>
</span><span class=lnt id=hl-1-22><a class=lnlinks href=#hl-1-22>22</a>
</span><span class=lnt id=hl-1-23><a class=lnlinks href=#hl-1-23>23</a>
</span><span class=lnt id=hl-1-24><a class=lnlinks href=#hl-1-24>24</a>
</span><span class=lnt id=hl-1-25><a class=lnlinks href=#hl-1-25>25</a>
</span><span class=lnt id=hl-1-26><a class=lnlinks href=#hl-1-26>26</a>
</span><span class=lnt id=hl-1-27><a class=lnlinks href=#hl-1-27>27</a>
</span><span class=lnt id=hl-1-28><a class=lnlinks href=#hl-1-28>28</a>
</span><span class=lnt id=hl-1-29><a class=lnlinks href=#hl-1-29>29</a>
</span><span class=lnt id=hl-1-30><a class=lnlinks href=#hl-1-30>30</a>
</span><span class=lnt id=hl-1-31><a class=lnlinks href=#hl-1-31>31</a>
</span><span class=lnt id=hl-1-32><a class=lnlinks href=#hl-1-32>32</a>
</span><span class=lnt id=hl-1-33><a class=lnlinks href=#hl-1-33>33</a>
</span><span class=lnt id=hl-1-34><a class=lnlinks href=#hl-1-34>34</a>
</span><span class=lnt id=hl-1-35><a class=lnlinks href=#hl-1-35>35</a>
</span><span class=lnt id=hl-1-36><a class=lnlinks href=#hl-1-36>36</a>
</span><span class=lnt id=hl-1-37><a class=lnlinks href=#hl-1-37>37</a>
</span><span class=lnt id=hl-1-38><a class=lnlinks href=#hl-1-38>38</a>
</span><span class=lnt id=hl-1-39><a class=lnlinks href=#hl-1-39>39</a>
</span><span class=lnt id=hl-1-40><a class=lnlinks href=#hl-1-40>40</a>
</span><span class=lnt id=hl-1-41><a class=lnlinks href=#hl-1-41>41</a>
</span><span class=lnt id=hl-1-42><a class=lnlinks href=#hl-1-42>42</a>
</span><span class=lnt id=hl-1-43><a class=lnlinks href=#hl-1-43>43</a>
</span><span class=lnt id=hl-1-44><a class=lnlinks href=#hl-1-44>44</a>
</span><span class=lnt id=hl-1-45><a class=lnlinks href=#hl-1-45>45</a>
</span><span class=lnt id=hl-1-46><a class=lnlinks href=#hl-1-46>46</a>
</span><span class=lnt id=hl-1-47><a class=lnlinks href=#hl-1-47>47</a>
</span><span class=lnt id=hl-1-48><a class=lnlinks href=#hl-1-48>48</a>
</span><span class=lnt id=hl-1-49><a class=lnlinks href=#hl-1-49>49</a>
</span><span class=lnt id=hl-1-50><a class=lnlinks href=#hl-1-50>50</a>
</span><span class=lnt id=hl-1-51><a class=lnlinks href=#hl-1-51>51</a>
</span><span class=lnt id=hl-1-52><a class=lnlinks href=#hl-1-52>52</a>
</span><span class=lnt id=hl-1-53><a class=lnlinks href=#hl-1-53>53</a>
</span><span class=lnt id=hl-1-54><a class=lnlinks href=#hl-1-54>54</a>
</span><span class=lnt id=hl-1-55><a class=lnlinks href=#hl-1-55>55</a>
</span><span class=lnt id=hl-1-56><a class=lnlinks href=#hl-1-56>56</a>
</span><span class=lnt id=hl-1-57><a class=lnlinks href=#hl-1-57>57</a>
</span><span class=lnt id=hl-1-58><a class=lnlinks href=#hl-1-58>58</a>
</span><span class=lnt id=hl-1-59><a class=lnlinks href=#hl-1-59>59</a>
</span><span class=lnt id=hl-1-60><a class=lnlinks href=#hl-1-60>60</a>
</span><span class=lnt id=hl-1-61><a class=lnlinks href=#hl-1-61>61</a>
</span><span class=lnt id=hl-1-62><a class=lnlinks href=#hl-1-62>62</a>
</span><span class=lnt id=hl-1-63><a class=lnlinks href=#hl-1-63>63</a>
</span><span class=lnt id=hl-1-64><a class=lnlinks href=#hl-1-64>64</a>
</span><span class=lnt id=hl-1-65><a class=lnlinks href=#hl-1-65>65</a>
</span><span class=lnt id=hl-1-66><a class=lnlinks href=#hl-1-66>66</a>
</span><span class=lnt id=hl-1-67><a class=lnlinks href=#hl-1-67>67</a>
</span><span class=lnt id=hl-1-68><a class=lnlinks href=#hl-1-68>68</a>
</span><span class=lnt id=hl-1-69><a class=lnlinks href=#hl-1-69>69</a>
</span><span class=lnt id=hl-1-70><a class=lnlinks href=#hl-1-70>70</a>
</span><span class=lnt id=hl-1-71><a class=lnlinks href=#hl-1-71>71</a>
</span><span class=lnt id=hl-1-72><a class=lnlinks href=#hl-1-72>72</a>
</span><span class=lnt id=hl-1-73><a class=lnlinks href=#hl-1-73>73</a>
</span><span class=lnt id=hl-1-74><a class=lnlinks href=#hl-1-74>74</a>
</span><span class=lnt id=hl-1-75><a class=lnlinks href=#hl-1-75>75</a>
</span><span class=lnt id=hl-1-76><a class=lnlinks href=#hl-1-76>76</a>
</span><span class=lnt id=hl-1-77><a class=lnlinks href=#hl-1-77>77</a>
</span><span class=lnt id=hl-1-78><a class=lnlinks href=#hl-1-78>78</a>
</span><span class=lnt id=hl-1-79><a class=lnlinks href=#hl-1-79>79</a>
</span><span class=lnt id=hl-1-80><a class=lnlinks href=#hl-1-80>80</a>
</span><span class=lnt id=hl-1-81><a class=lnlinks href=#hl-1-81>81</a>
</span><span class=lnt id=hl-1-82><a class=lnlinks href=#hl-1-82>82</a>
</span><span class=lnt id=hl-1-83><a class=lnlinks href=#hl-1-83>83</a>
</span><span class=lnt id=hl-1-84><a class=lnlinks href=#hl-1-84>84</a>
</span><span class=lnt id=hl-1-85><a class=lnlinks href=#hl-1-85>85</a>
</span><span class=lnt id=hl-1-86><a class=lnlinks href=#hl-1-86>86</a>
</span><span class=lnt id=hl-1-87><a class=lnlinks href=#hl-1-87>87</a>
</span><span class=lnt id=hl-1-88><a class=lnlinks href=#hl-1-88>88</a>
</span><span class=lnt id=hl-1-89><a class=lnlinks href=#hl-1-89>89</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>io</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>IPython.display</span> <span class=kn>import</span> <span class=n>display_png</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib</span> <span class=k>as</span> <span class=nn>mpl</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.cm</span> <span class=k>as</span> <span class=nn>cm</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>imify</span><span class=p>(</span><span class=n>arr</span><span class=p>,</span> <span class=n>vmin</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>vmax</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>origin</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;&#34;&#34;Convert an array to an image.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>  Arguments:
</span></span></span><span class=line><span class=cl><span class=s2>    arr : array-like The image data. The shape can be one of MxN (luminance),
</span></span></span><span class=line><span class=cl><span class=s2>      MxNx3 (RGB) or MxNx4 (RGBA).
</span></span></span><span class=line><span class=cl><span class=s2>    vmin : scalar, optional lower value.
</span></span></span><span class=line><span class=cl><span class=s2>    vmax : scalar, optional *vmin* and *vmax* set the color scaling for the
</span></span></span><span class=line><span class=cl><span class=s2>      image by fixing the values that map to the colormap color limits. If
</span></span></span><span class=line><span class=cl><span class=s2>      either *vmin* or *vmax* is None, that limit is determined from the *arr*
</span></span></span><span class=line><span class=cl><span class=s2>      min/max value.
</span></span></span><span class=line><span class=cl><span class=s2>    cmap : str or `~matplotlib.colors.Colormap`, optional A Colormap instance or
</span></span></span><span class=line><span class=cl><span class=s2>      registered colormap name. The colormap maps scalar data to colors. It is
</span></span></span><span class=line><span class=cl><span class=s2>      ignored for RGB(A) data.
</span></span></span><span class=line><span class=cl><span class=s2>        Defaults to :rc:`image.cmap` (&#39;viridis&#39;).
</span></span></span><span class=line><span class=cl><span class=s2>    origin : {&#39;upper&#39;, &#39;lower&#39;}, optional Indicates whether the ``(0, 0)`` index
</span></span></span><span class=line><span class=cl><span class=s2>      of the array is in the upper
</span></span></span><span class=line><span class=cl><span class=s2>        left or lower left corner of the axes.  Defaults to :rc:`image.origin`
</span></span></span><span class=line><span class=cl><span class=s2>          (&#39;upper&#39;).
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>  Returns:
</span></span></span><span class=line><span class=cl><span class=s2>    A uint8 image array.
</span></span></span><span class=line><span class=cl><span class=s2>  &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>  <span class=n>sm</span> <span class=o>=</span> <span class=n>cm</span><span class=o>.</span><span class=n>ScalarMappable</span><span class=p>(</span><span class=n>cmap</span><span class=o>=</span><span class=n>cmap</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>sm</span><span class=o>.</span><span class=n>set_clim</span><span class=p>(</span><span class=n>vmin</span><span class=p>,</span> <span class=n>vmax</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>origin</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>origin</span> <span class=o>=</span> <span class=n>mpl</span><span class=o>.</span><span class=n>rcParams</span><span class=p>[</span><span class=s2>&#34;image.origin&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>origin</span> <span class=o>==</span> <span class=s2>&#34;lower&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>arr</span> <span class=o>=</span> <span class=n>arr</span><span class=p>[::</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=n>rgba</span> <span class=o>=</span> <span class=n>sm</span><span class=o>.</span><span class=n>to_rgba</span><span class=p>(</span><span class=n>arr</span><span class=p>,</span> <span class=nb>bytes</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>rgba</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>rawarrview</span><span class=p>(</span><span class=n>array</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;&#34;&#34;Visualize an array as if it was an image in colab notebooks.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>  Arguments:
</span></span></span><span class=line><span class=cl><span class=s2>    array: an array which will be turned into an image.
</span></span></span><span class=line><span class=cl><span class=s2>    **kwargs: Additional keyword arguments passed to imify.
</span></span></span><span class=line><span class=cl><span class=s2>  &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>  <span class=n>f</span> <span class=o>=</span> <span class=n>io</span><span class=o>.</span><span class=n>BytesIO</span><span class=p>()</span>
</span></span><span class=line><span class=cl>  <span class=n>imarray</span> <span class=o>=</span> <span class=n>imify</span><span class=p>(</span><span class=n>array</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>plt</span><span class=o>.</span><span class=n>imsave</span><span class=p>(</span><span class=n>f</span><span class=p>,</span> <span class=n>imarray</span><span class=p>,</span> <span class=nb>format</span><span class=o>=</span><span class=s2>&#34;png&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>f</span><span class=o>.</span><span class=n>seek</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>dat</span> <span class=o>=</span> <span class=n>f</span><span class=o>.</span><span class=n>read</span><span class=p>()</span>
</span></span><span class=line><span class=cl>  <span class=n>f</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span><span class=line><span class=cl>  <span class=n>display_png</span><span class=p>(</span><span class=n>dat</span><span class=p>,</span> <span class=n>raw</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>reshape_image_batch</span><span class=p>(</span><span class=n>array</span><span class=p>,</span> <span class=n>cut</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>rows</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;&#34;&#34;Given an array of shape [n, x, y, ...] reshape it to create an image field.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>  Arguments:
</span></span></span><span class=line><span class=cl><span class=s2>    array: The array to reshape.
</span></span></span><span class=line><span class=cl><span class=s2>    cut: Optional cut on the number of images to view. Will default to whole
</span></span></span><span class=line><span class=cl><span class=s2>      array.
</span></span></span><span class=line><span class=cl><span class=s2>    rows: Number of rows to use.  Will default to the integer less than the
</span></span></span><span class=line><span class=cl><span class=s2>      sqrt.
</span></span></span><span class=line><span class=cl><span class=s2>    axis: Axis to interpretate at the batch dimension.  By default the image
</span></span></span><span class=line><span class=cl><span class=s2>      dimensions immediately follow.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>  Returns:
</span></span></span><span class=line><span class=cl><span class=s2>    reshaped_array: An array of shape [rows * x, cut / rows * y, ...]
</span></span></span><span class=line><span class=cl><span class=s2>  &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>  <span class=n>original_shape</span> <span class=o>=</span> <span class=n>array</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>  <span class=k>assert</span> <span class=nb>len</span><span class=p>(</span><span class=n>original_shape</span><span class=p>)</span> <span class=o>&gt;=</span> <span class=mi>2</span><span class=p>,</span> <span class=s2>&#34;array must be at least 3 Dimensional.&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>cut</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>cut</span> <span class=o>=</span> <span class=n>original_shape</span><span class=p>[</span><span class=n>axis</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>rows</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>rows</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>cut</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>cols</span> <span class=o>=</span> <span class=n>cut</span> <span class=o>//</span> <span class=n>rows</span>
</span></span><span class=line><span class=cl>  <span class=n>cut</span> <span class=o>=</span> <span class=n>cols</span> <span class=o>*</span> <span class=n>rows</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>leading</span> <span class=o>=</span> <span class=n>original_shape</span><span class=p>[:</span><span class=n>axis</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=n>x_width</span> <span class=o>=</span> <span class=n>original_shape</span><span class=p>[</span><span class=n>axis</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=n>y_width</span> <span class=o>=</span> <span class=n>original_shape</span><span class=p>[</span><span class=n>axis</span> <span class=o>+</span> <span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=n>remaining</span> <span class=o>=</span> <span class=n>original_shape</span><span class=p>[</span><span class=n>axis</span> <span class=o>+</span> <span class=mi>3</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>array</span> <span class=o>=</span> <span class=n>array</span><span class=p>[:</span><span class=n>cut</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=n>array</span> <span class=o>=</span> <span class=n>array</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>leading</span> <span class=o>+</span> <span class=p>(</span><span class=n>rows</span><span class=p>,</span> <span class=n>cols</span><span class=p>,</span> <span class=n>x_width</span><span class=p>,</span> <span class=n>y_width</span><span class=p>)</span> <span class=o>+</span> <span class=n>remaining</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>array</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>moveaxis</span><span class=p>(</span><span class=n>array</span><span class=p>,</span> <span class=n>axis</span> <span class=o>+</span> <span class=mi>2</span><span class=p>,</span> <span class=n>axis</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>array</span> <span class=o>=</span> <span class=n>array</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>leading</span> <span class=o>+</span> <span class=p>(</span><span class=n>rows</span> <span class=o>*</span> <span class=n>x_width</span><span class=p>,</span> <span class=n>cols</span> <span class=o>*</span> <span class=n>y_width</span><span class=p>)</span> <span class=o>+</span> <span class=n>remaining</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>array</span>
</span></span></code></pre></td></tr></table></div></div></details><h2 id=setting-up-diffusion-step>Setting up Diffusion Step<a hidden class=anchor aria-hidden=true href=#setting-up-diffusion-step>#</a></h2><p>Setting up a diffusion step involves:</p><ol><li>Defining a schedule for diffusion: $\alpha$</li><li>Defining variables that we will be using during denoising and diffusion steps.</li><li>Defining utilities to add noise to an image.</li></ol><p>You can find more details about diffusion in the <a href=/posts/diffusion-models/denoising-diffusion-models-1/#1-diffusion-step>earlier post.</a></p><p>We are going to use the cosine schedule for diffusion in this example.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-2-1><a class=lnlinks href=#hl-2-1> 1</a>
</span><span class=lnt id=hl-2-2><a class=lnlinks href=#hl-2-2> 2</a>
</span><span class=lnt id=hl-2-3><a class=lnlinks href=#hl-2-3> 3</a>
</span><span class=lnt id=hl-2-4><a class=lnlinks href=#hl-2-4> 4</a>
</span><span class=lnt id=hl-2-5><a class=lnlinks href=#hl-2-5> 5</a>
</span><span class=lnt id=hl-2-6><a class=lnlinks href=#hl-2-6> 6</a>
</span><span class=lnt id=hl-2-7><a class=lnlinks href=#hl-2-7> 7</a>
</span><span class=lnt id=hl-2-8><a class=lnlinks href=#hl-2-8> 8</a>
</span><span class=lnt id=hl-2-9><a class=lnlinks href=#hl-2-9> 9</a>
</span><span class=lnt id=hl-2-10><a class=lnlinks href=#hl-2-10>10</a>
</span><span class=lnt id=hl-2-11><a class=lnlinks href=#hl-2-11>11</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>cosine_beta_schedule</span><span class=p>(</span><span class=n>timesteps</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mf>0.008</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    cosine schedule as proposed in https://arxiv.org/abs/2102.09672
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>steps</span> <span class=o>=</span> <span class=n>timesteps</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>timesteps</span><span class=p>,</span> <span class=n>steps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>alphas_cumprod</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>cos</span><span class=p>(((</span><span class=n>x</span> <span class=o>/</span> <span class=n>timesteps</span><span class=p>)</span> <span class=o>+</span> <span class=n>s</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>s</span><span class=p>)</span> <span class=o>*</span> <span class=n>jnp</span><span class=o>.</span><span class=n>pi</span> <span class=o>*</span> <span class=mf>0.5</span><span class=p>)</span> <span class=o>**</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>    <span class=n>alphas_cumprod</span> <span class=o>=</span> <span class=n>alphas_cumprod</span> <span class=o>/</span> <span class=n>alphas_cumprod</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>betas</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>-</span> <span class=p>(</span><span class=n>alphas_cumprod</span><span class=p>[</span><span class=mi>1</span><span class=p>:]</span> <span class=o>/</span> <span class=n>alphas_cumprod</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>	<span class=c1># clipping this at 0.1 as I have found it difficult to work with higher values of beta.</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>jnp</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=n>betas</span><span class=p>,</span> <span class=mf>0.0001</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Setting up the variables:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-3-1><a class=lnlinks href=#hl-3-1> 1</a>
</span><span class=lnt id=hl-3-2><a class=lnlinks href=#hl-3-2> 2</a>
</span><span class=lnt id=hl-3-3><a class=lnlinks href=#hl-3-3> 3</a>
</span><span class=lnt id=hl-3-4><a class=lnlinks href=#hl-3-4> 4</a>
</span><span class=lnt id=hl-3-5><a class=lnlinks href=#hl-3-5> 5</a>
</span><span class=lnt id=hl-3-6><a class=lnlinks href=#hl-3-6> 6</a>
</span><span class=lnt id=hl-3-7><a class=lnlinks href=#hl-3-7> 7</a>
</span><span class=lnt id=hl-3-8><a class=lnlinks href=#hl-3-8> 8</a>
</span><span class=lnt id=hl-3-9><a class=lnlinks href=#hl-3-9> 9</a>
</span><span class=lnt id=hl-3-10><a class=lnlinks href=#hl-3-10>10</a>
</span><span class=lnt id=hl-3-11><a class=lnlinks href=#hl-3-11>11</a>
</span><span class=lnt id=hl-3-12><a class=lnlinks href=#hl-3-12>12</a>
</span><span class=lnt id=hl-3-13><a class=lnlinks href=#hl-3-13>13</a>
</span><span class=lnt id=hl-3-14><a class=lnlinks href=#hl-3-14>14</a>
</span><span class=lnt id=hl-3-15><a class=lnlinks href=#hl-3-15>15</a>
</span><span class=lnt id=hl-3-16><a class=lnlinks href=#hl-3-16>16</a>
</span><span class=lnt id=hl-3-17><a class=lnlinks href=#hl-3-17>17</a>
</span><span class=lnt id=hl-3-18><a class=lnlinks href=#hl-3-18>18</a>
</span><span class=lnt id=hl-3-19><a class=lnlinks href=#hl-3-19>19</a>
</span><span class=lnt id=hl-3-20><a class=lnlinks href=#hl-3-20>20</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>timesteps</span> <span class=o>=</span> <span class=mi>250</span>
</span></span><span class=line><span class=cl><span class=n>betas</span> <span class=o>=</span> <span class=n>cosine_beta_schedule</span><span class=p>(</span><span class=n>timesteps</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>alphas</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>betas</span>
</span></span><span class=line><span class=cl><span class=n>alphas_</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>cumprod</span><span class=p>(</span><span class=n>alphas</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>variance</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>alphas_</span>
</span></span><span class=line><span class=cl><span class=n>sd</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>variance</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># these variables are used during diffusion and denoising step</span>
</span></span><span class=line><span class=cl><span class=n>alphas_prev_</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>pad</span><span class=p>(</span><span class=n>alphas_</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=s2>&#34;constant&#34;</span><span class=p>,</span> <span class=n>constant_values</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>sigma_squared_q_t</span> <span class=o>=</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>alphas</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>alphas_</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>alphas_prev_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>log_sigma_squared_q_t</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>alphas</span><span class=p>)</span> <span class=o>+</span> <span class=n>jnp</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>alphas_</span><span class=p>)</span> <span class=o>-</span> <span class=n>jnp</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>alphas_prev_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>sigma_squared_q_t_corrected</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>log_sigma_squared_q_t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## following code here -- we are computing the posterior variance</span>
</span></span><span class=line><span class=cl><span class=c1>## https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L196</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L78 </span>
</span></span><span class=line><span class=cl><span class=n>log_posterior_variance</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>hstack</span><span class=p>([</span><span class=n>posterior_variance</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>posterior_variance</span><span class=p>[</span><span class=mi>1</span><span class=p>:]]))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>posterior_variance_corrected</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>log_posterior_variance</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Let&rsquo;s define the utilities as well, adding noise using the re-parameterization trick. The diffusion step takes in data, time step value and adds noise to the data according to the equation below. It uses the schedule $\alpha$ defined above.</p>$$
\begin{align}
q_t(x_t|x_0) &= \sqrt{\bar\alpha_t}x_0 + \sqrt{(1 - \bar\alpha_t )}\ast\epsilon_0^\ast ; \space where \space \epsilon_0^\ast \in N(0, I) \cr
&= N(\sqrt{\bar\alpha_t}x_0, (1 - \bar\alpha_t)I) \cr
\end{align}
$$<div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-4-1><a class=lnlinks href=#hl-4-1> 1</a>
</span><span class=lnt id=hl-4-2><a class=lnlinks href=#hl-4-2> 2</a>
</span><span class=lnt id=hl-4-3><a class=lnlinks href=#hl-4-3> 3</a>
</span><span class=lnt id=hl-4-4><a class=lnlinks href=#hl-4-4> 4</a>
</span><span class=lnt id=hl-4-5><a class=lnlinks href=#hl-4-5> 5</a>
</span><span class=lnt id=hl-4-6><a class=lnlinks href=#hl-4-6> 6</a>
</span><span class=lnt id=hl-4-7><a class=lnlinks href=#hl-4-7> 7</a>
</span><span class=lnt id=hl-4-8><a class=lnlinks href=#hl-4-8> 8</a>
</span><span class=lnt id=hl-4-9><a class=lnlinks href=#hl-4-9> 9</a>
</span><span class=lnt id=hl-4-10><a class=lnlinks href=#hl-4-10>10</a>
</span><span class=lnt id=hl-4-11><a class=lnlinks href=#hl-4-11>11</a>
</span><span class=lnt id=hl-4-12><a class=lnlinks href=#hl-4-12>12</a>
</span><span class=lnt id=hl-4-13><a class=lnlinks href=#hl-4-13>13</a>
</span><span class=lnt id=hl-4-14><a class=lnlinks href=#hl-4-14>14</a>
</span><span class=lnt id=hl-4-15><a class=lnlinks href=#hl-4-15>15</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># how to add noise to the data</span>
</span></span><span class=line><span class=cl><span class=nd>@jax.jit</span> <span class=p>:</span> <span class=n>jit</span> <span class=n>compilazation</span> <span class=n>to</span> <span class=n>significantly</span> <span class=n>speed</span> <span class=n>up</span> <span class=n>code</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_noisy</span><span class=p>(</span><span class=n>rng</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>timestep</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>timestep</span> <span class=o>=</span> <span class=n>einops</span><span class=o>.</span><span class=n>repeat</span><span class=p>(</span><span class=n>timestep</span><span class=p>,</span> <span class=s1>&#39;b -&gt; b 28 28 1&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># we will use the reparameterization trick</span>
</span></span><span class=line><span class=cl>    <span class=c1># need to generate new keys everytime</span>
</span></span><span class=line><span class=cl>    <span class=n>_</span><span class=p>,</span> <span class=n>noise_key</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>rng</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>noise_at_t</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>noise_key</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=n>batch</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>added_noise_at_t</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>batch</span> <span class=o>*</span> <span class=n>jnp</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>alphas_</span><span class=p>[</span><span class=n>timestep</span><span class=p>]),</span> <span class=n>noise_at_t</span> <span class=o>*</span> <span class=n>sd</span><span class=p>[</span><span class=n>timestep</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>added_noise_at_t</span><span class=p>,</span> <span class=n>noise_at_t</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># recovering original data by removing noise</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>recover_original</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>timestep</span><span class=p>,</span> <span class=n>noise</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>true_data</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>subtract</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>noise</span><span class=o>*</span><span class=n>sd</span><span class=p>[</span><span class=n>timestep</span><span class=p>])</span><span class=o>/</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>alphas_</span><span class=p>[</span><span class=n>timestep</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>true_data</span>
</span></span></code></pre></td></tr></table></div></div><details><summary>Take a random data point and add noise over multiple steps. Click to see the code.</summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-5-1><a class=lnlinks href=#hl-5-1> 1</a>
</span><span class=lnt id=hl-5-2><a class=lnlinks href=#hl-5-2> 2</a>
</span><span class=lnt id=hl-5-3><a class=lnlinks href=#hl-5-3> 3</a>
</span><span class=lnt id=hl-5-4><a class=lnlinks href=#hl-5-4> 4</a>
</span><span class=lnt id=hl-5-5><a class=lnlinks href=#hl-5-5> 5</a>
</span><span class=lnt id=hl-5-6><a class=lnlinks href=#hl-5-6> 6</a>
</span><span class=lnt id=hl-5-7><a class=lnlinks href=#hl-5-7> 7</a>
</span><span class=lnt id=hl-5-8><a class=lnlinks href=#hl-5-8> 8</a>
</span><span class=lnt id=hl-5-9><a class=lnlinks href=#hl-5-9> 9</a>
</span><span class=lnt id=hl-5-10><a class=lnlinks href=#hl-5-10>10</a>
</span><span class=lnt id=hl-5-11><a class=lnlinks href=#hl-5-11>11</a>
</span><span class=lnt id=hl-5-12><a class=lnlinks href=#hl-5-12>12</a>
</span><span class=lnt id=hl-5-13><a class=lnlinks href=#hl-5-13>13</a>
</span><span class=lnt id=hl-5-14><a class=lnlinks href=#hl-5-14>14</a>
</span><span class=lnt id=hl-5-15><a class=lnlinks href=#hl-5-15>15</a>
</span><span class=lnt id=hl-5-16><a class=lnlinks href=#hl-5-16>16</a>
</span><span class=lnt id=hl-5-17><a class=lnlinks href=#hl-5-17>17</a>
</span><span class=lnt id=hl-5-18><a class=lnlinks href=#hl-5-18>18</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>random_index</span> <span class=o>=</span> <span class=mi>22</span>
</span></span><span class=line><span class=cl><span class=n>image</span> <span class=o>=</span> <span class=nb>next</span><span class=p>(</span><span class=n>create_input_iter</span><span class=p>(</span><span class=n>train_ds</span><span class=p>))[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>][</span><span class=n>random_index</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>fig</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>ims</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=n>noisy_images</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>get_noisy</span><span class=p>(</span><span class=n>key</span><span class=p>,</span> <span class=n>einops</span><span class=o>.</span><span class=n>repeat</span><span class=p>(</span><span class=n>image</span><span class=p>,</span> <span class=s1>&#39;h w c -&gt; b h w c&#39;</span><span class=p>,</span> <span class=n>b</span><span class=o>=</span><span class=n>timesteps</span><span class=o>//</span><span class=mi>5</span><span class=p>),</span> <span class=n>jnp</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>timesteps</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>colab</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>noisy_images</span> <span class=o>=</span> <span class=n>einops</span><span class=o>.</span><span class=n>rearrange</span><span class=p>(</span><span class=n>noisy_images</span><span class=p>,</span> <span class=s1>&#39;b h w c -&gt; b h (w c)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>noisy_images</span> <span class=o>=</span> <span class=n>unnormalize</span><span class=p>(</span><span class=n>noisy_images</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>timesteps</span><span class=o>//</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>im</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>noisy_images</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>cmap</span><span class=o>=</span><span class=s2>&#34;gray&#34;</span><span class=p>,</span> <span class=n>animated</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>ims</span><span class=o>.</span><span class=n>append</span><span class=p>([</span><span class=n>im</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>animate</span> <span class=o>=</span> <span class=n>animation</span><span class=o>.</span><span class=n>ArtistAnimation</span><span class=p>(</span><span class=n>fig</span><span class=p>,</span> <span class=n>ims</span><span class=p>,</span> <span class=n>interval</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>blit</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>repeat_delay</span><span class=o>=</span><span class=mi>3000</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>animate</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>gifs_dir</span><span class=o>+</span><span class=s1>&#39;diffusion.gif&#39;</span><span class=p>,</span> <span class=n>writer</span><span class=o>=</span><span class=s1>&#39;pillow&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span><span class=err> </span><span class=nb>open</span><span class=p>(</span><span class=n>gifs_dir</span><span class=err> </span><span class=o>+</span><span class=err> </span><span class=s1>&#39;diffusion.gif&#39;</span><span class=p>,</span><span class=s1>&#39;rb&#39;</span><span class=p>)</span><span class=err> </span><span class=k>as</span><span class=err> </span><span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=err>  </span><span class=n>display</span><span class=p>(</span><span class=n>Image</span><span class=p>(</span><span class=n>data</span><span class=o>=</span><span class=n>f</span><span class=o>.</span><span class=n>read</span><span class=p>(),</span><span class=err> </span><span class=nb>format</span><span class=o>=</span><span class=s1>&#39;png&#39;</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div></details><figure class=align-center><img loading=lazy src=/images/diffusion-z.gif#center alt="Figure 2: Using the utilities defined above to add Gaussian Noise in multiple steps to a handwritten character &amp;lsquo;z&amp;rsquo;" width=50%><figcaption><p>Figure 2: Using the utilities defined above to add Gaussian Noise in multiple steps to a handwritten character &lsquo;z&rsquo;</p></figcaption></figure><h2 id=the-denoising-model-u-net>The Denoising Model: U-Net<a hidden class=anchor aria-hidden=true href=#the-denoising-model-u-net>#</a></h2><p>In the earlier blog posts, we were working on 2-d samples. The <a href=/posts/diffusion-models/denoising-diffusion-models-1/#neural-network>denoising model</a> was a multi layer perceptron with GeLU activations. In the case of EMNIST we will have to denoise images. <a href=https://en.wikipedia.org/wiki/U-Net target=_blank>U-Nets</a> are the recommended models to do this.</p><figure class=align-center><img loading=lazy src=/images/u-net.png#center alt="Figure 3: U-Net architecture. It&amp;rsquo;s similar to the one we will build." width=100%><figcaption><p>Figure 3: U-Net architecture. It&rsquo;s similar to the one we will build.</p></figcaption></figure><p>The U-Net architecture has the following characteristics:</p><ul><li>It&rsquo;s typically represented as a U block.</li><li>The first part of the U block downsamples the image. We go from a 28*28 image to a 7*7 image. This is done using Downsampling convolution blocks.</li><li>With downsampling, we increase the number of channels (features). We go from 1 channel in the input to 192 channels at the end of the first part of the U-net.</li><li>The 2<sup>nd</sup> part of the U block does the opposite of the 1st part. We go from a 7*7 image to a 28*28 image, and also go from 192 channels to 1 channel. Upsampling is done using Upsampling convolutional blocks.</li><li>At the same time, the U-Net has residual connections, connecting layers in the downsampling part to the layers in the upsampling part, this makes training the network efficient.</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-6-1><a class=lnlinks href=#hl-6-1> 1</a>
</span><span class=lnt id=hl-6-2><a class=lnlinks href=#hl-6-2> 2</a>
</span><span class=lnt id=hl-6-3><a class=lnlinks href=#hl-6-3> 3</a>
</span><span class=lnt id=hl-6-4><a class=lnlinks href=#hl-6-4> 4</a>
</span><span class=lnt id=hl-6-5><a class=lnlinks href=#hl-6-5> 5</a>
</span><span class=lnt id=hl-6-6><a class=lnlinks href=#hl-6-6> 6</a>
</span><span class=lnt id=hl-6-7><a class=lnlinks href=#hl-6-7> 7</a>
</span><span class=lnt id=hl-6-8><a class=lnlinks href=#hl-6-8> 8</a>
</span><span class=lnt id=hl-6-9><a class=lnlinks href=#hl-6-9> 9</a>
</span><span class=lnt id=hl-6-10><a class=lnlinks href=#hl-6-10>10</a>
</span><span class=lnt id=hl-6-11><a class=lnlinks href=#hl-6-11>11</a>
</span><span class=lnt id=hl-6-12><a class=lnlinks href=#hl-6-12>12</a>
</span><span class=lnt id=hl-6-13><a class=lnlinks href=#hl-6-13>13</a>
</span><span class=lnt id=hl-6-14><a class=lnlinks href=#hl-6-14>14</a>
</span><span class=lnt id=hl-6-15><a class=lnlinks href=#hl-6-15>15</a>
</span><span class=lnt id=hl-6-16><a class=lnlinks href=#hl-6-16>16</a>
</span><span class=lnt id=hl-6-17><a class=lnlinks href=#hl-6-17>17</a>
</span><span class=lnt id=hl-6-18><a class=lnlinks href=#hl-6-18>18</a>
</span><span class=lnt id=hl-6-19><a class=lnlinks href=#hl-6-19>19</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># upsample operation in the UNET</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Downsample</span><span class=p>(</span><span class=n>hk</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>output_channels</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>conv</span> <span class=o>=</span> <span class=n>hk</span><span class=o>.</span><span class=n>Conv2D</span><span class=p>(</span><span class=n>output_channels</span><span class=o>=</span><span class=n>output_channels</span><span class=p>,</span> <span class=n>kernel_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>4</span><span class=p>),</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=fm>__call__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl><span class=c1># Downsample operation in the UNET</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Upsample</span><span class=p>(</span><span class=n>hk</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>output_channels</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>conv</span> <span class=o>=</span> <span class=n>hk</span><span class=o>.</span><span class=n>Conv2D</span><span class=p>(</span><span class=n>output_channels</span><span class=o>=</span><span class=n>output_channels</span><span class=p>,</span> <span class=n>kernel_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>),</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;SAME&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=fm>__call__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># scaling image to twice size</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>einops</span><span class=o>.</span><span class=n>repeat</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=s1>&#39;b h w c -&gt; b (a h) (aa w) c&#39;</span><span class=p>,</span> <span class=n>a</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>aa</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>Time Step Embeddings:</strong>
We will be using sinusoidal embeddings for time steps. This is following the <a href=/posts/diffusion-models/denoising-diffusion-models-2/#time-step-embedding>discussion in the previous post.</a></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-7-1><a class=lnlinks href=#hl-7-1> 1</a>
</span><span class=lnt id=hl-7-2><a class=lnlinks href=#hl-7-2> 2</a>
</span><span class=lnt id=hl-7-3><a class=lnlinks href=#hl-7-3> 3</a>
</span><span class=lnt id=hl-7-4><a class=lnlinks href=#hl-7-4> 4</a>
</span><span class=lnt id=hl-7-5><a class=lnlinks href=#hl-7-5> 5</a>
</span><span class=lnt id=hl-7-6><a class=lnlinks href=#hl-7-6> 6</a>
</span><span class=lnt id=hl-7-7><a class=lnlinks href=#hl-7-7> 7</a>
</span><span class=lnt id=hl-7-8><a class=lnlinks href=#hl-7-8> 8</a>
</span><span class=lnt id=hl-7-9><a class=lnlinks href=#hl-7-9> 9</a>
</span><span class=lnt id=hl-7-10><a class=lnlinks href=#hl-7-10>10</a>
</span><span class=lnt id=hl-7-11><a class=lnlinks href=#hl-7-11>11</a>
</span><span class=lnt id=hl-7-12><a class=lnlinks href=#hl-7-12>12</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>TimeEmbeddings</span><span class=p>(</span><span class=n>hk</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>half_dim</span> <span class=o>=</span> <span class=n>dim</span> <span class=o>//</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>    <span class=n>embeddings</span> <span class=o>=</span> <span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mi>10000</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>half_dim</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>embeddings</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>half_dim</span><span class=p>)</span> <span class=o>*</span> <span class=o>-</span><span class=n>embeddings</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=fm>__call__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>timesteps</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embeddings</span>
</span></span><span class=line><span class=cl>      <span class=n>embeddings</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>multiply</span><span class=p>(</span><span class=n>timesteps</span><span class=p>[:,</span> <span class=kc>None</span><span class=p>],</span> <span class=n>embeddings</span><span class=p>[:,</span> <span class=kc>None</span><span class=p>]</span><span class=o>.</span><span class=n>T</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=n>embeddings</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>hstack</span><span class=p>([</span><span class=n>jnp</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>embeddings</span><span class=p>),</span> <span class=n>jnp</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>embeddings</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>      <span class=k>return</span> <span class=n>embeddings</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>Conditional Labels:</strong>
EMNIST dataset has 62 labels, 26 characters in lowercase, 26 characters in uppercase, 10 numbers. We add another label to represent the masked label.
We use the Haiku Embed class to generate embedding vectors for these labels.
Labels are fused with the input as explained in the <a href=/posts/diffusion-models/denoising-diffusion-models-2/#guidance--classifier-free-guidance>previous post</a>.
Instead of fusing label and time step information only at the start, we are fusing it with the input at every Block.</p><p><strong>Network Definition:</strong>
After convolution layers, I am using a <cite>BatchNorm<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></cite> layer, using the Haiku BatchNorm module to normalize the values across the batch. This is shown to improve the convergence of deep models.
Using BatchNorm complicates the implementation a bit since, BatchNorm layers need to maintain a buffer states for mean and variance of the activations.
<a href=https://youtu.be/l_3zj6HeWUE target=_blank>This video</a> from Yannic is an excellent introduction to the different kind of Normalizations that one can apply to speed up training.</p><p>The code below follows the U-net implementation in <a href=https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py#L267 target=_blank>LucidRains Diffusion model implementation</a>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-8-1><a class=lnlinks href=#hl-8-1> 1</a>
</span><span class=lnt id=hl-8-2><a class=lnlinks href=#hl-8-2> 2</a>
</span><span class=lnt id=hl-8-3><a class=lnlinks href=#hl-8-3> 3</a>
</span><span class=lnt id=hl-8-4><a class=lnlinks href=#hl-8-4> 4</a>
</span><span class=lnt id=hl-8-5><a class=lnlinks href=#hl-8-5> 5</a>
</span><span class=lnt id=hl-8-6><a class=lnlinks href=#hl-8-6> 6</a>
</span><span class=lnt id=hl-8-7><a class=lnlinks href=#hl-8-7> 7</a>
</span><span class=lnt id=hl-8-8><a class=lnlinks href=#hl-8-8> 8</a>
</span><span class=lnt id=hl-8-9><a class=lnlinks href=#hl-8-9> 9</a>
</span><span class=lnt id=hl-8-10><a class=lnlinks href=#hl-8-10>10</a>
</span><span class=lnt id=hl-8-11><a class=lnlinks href=#hl-8-11>11</a>
</span><span class=lnt id=hl-8-12><a class=lnlinks href=#hl-8-12>12</a>
</span><span class=lnt id=hl-8-13><a class=lnlinks href=#hl-8-13>13</a>
</span><span class=lnt id=hl-8-14><a class=lnlinks href=#hl-8-14>14</a>
</span><span class=lnt id=hl-8-15><a class=lnlinks href=#hl-8-15>15</a>
</span><span class=lnt id=hl-8-16><a class=lnlinks href=#hl-8-16>16</a>
</span><span class=lnt id=hl-8-17><a class=lnlinks href=#hl-8-17>17</a>
</span><span class=lnt id=hl-8-18><a class=lnlinks href=#hl-8-18>18</a>
</span><span class=lnt id=hl-8-19><a class=lnlinks href=#hl-8-19>19</a>
</span><span class=lnt id=hl-8-20><a class=lnlinks href=#hl-8-20>20</a>
</span><span class=lnt id=hl-8-21><a class=lnlinks href=#hl-8-21>21</a>
</span><span class=lnt id=hl-8-22><a class=lnlinks href=#hl-8-22>22</a>
</span><span class=lnt id=hl-8-23><a class=lnlinks href=#hl-8-23>23</a>
</span><span class=lnt id=hl-8-24><a class=lnlinks href=#hl-8-24>24</a>
</span><span class=lnt id=hl-8-25><a class=lnlinks href=#hl-8-25>25</a>
</span><span class=lnt id=hl-8-26><a class=lnlinks href=#hl-8-26>26</a>
</span><span class=lnt id=hl-8-27><a class=lnlinks href=#hl-8-27>27</a>
</span><span class=lnt id=hl-8-28><a class=lnlinks href=#hl-8-28>28</a>
</span><span class=lnt id=hl-8-29><a class=lnlinks href=#hl-8-29>29</a>
</span><span class=lnt id=hl-8-30><a class=lnlinks href=#hl-8-30>30</a>
</span><span class=lnt id=hl-8-31><a class=lnlinks href=#hl-8-31>31</a>
</span><span class=lnt id=hl-8-32><a class=lnlinks href=#hl-8-32>32</a>
</span><span class=lnt id=hl-8-33><a class=lnlinks href=#hl-8-33>33</a>
</span><span class=lnt id=hl-8-34><a class=lnlinks href=#hl-8-34>34</a>
</span><span class=lnt id=hl-8-35><a class=lnlinks href=#hl-8-35>35</a>
</span><span class=lnt id=hl-8-36><a class=lnlinks href=#hl-8-36>36</a>
</span><span class=lnt id=hl-8-37><a class=lnlinks href=#hl-8-37>37</a>
</span><span class=lnt id=hl-8-38><a class=lnlinks href=#hl-8-38>38</a>
</span><span class=lnt id=hl-8-39><a class=lnlinks href=#hl-8-39>39</a>
</span><span class=lnt id=hl-8-40><a class=lnlinks href=#hl-8-40>40</a>
</span><span class=lnt id=hl-8-41><a class=lnlinks href=#hl-8-41>41</a>
</span><span class=lnt id=hl-8-42><a class=lnlinks href=#hl-8-42>42</a>
</span><span class=lnt id=hl-8-43><a class=lnlinks href=#hl-8-43>43</a>
</span><span class=lnt id=hl-8-44><a class=lnlinks href=#hl-8-44>44</a>
</span><span class=lnt id=hl-8-45><a class=lnlinks href=#hl-8-45>45</a>
</span><span class=lnt id=hl-8-46><a class=lnlinks href=#hl-8-46>46</a>
</span><span class=lnt id=hl-8-47><a class=lnlinks href=#hl-8-47>47</a>
</span><span class=lnt id=hl-8-48><a class=lnlinks href=#hl-8-48>48</a>
</span><span class=lnt id=hl-8-49><a class=lnlinks href=#hl-8-49>49</a>
</span><span class=lnt id=hl-8-50><a class=lnlinks href=#hl-8-50>50</a>
</span><span class=lnt id=hl-8-51><a class=lnlinks href=#hl-8-51>51</a>
</span><span class=lnt id=hl-8-52><a class=lnlinks href=#hl-8-52>52</a>
</span><span class=lnt id=hl-8-53><a class=lnlinks href=#hl-8-53>53</a>
</span><span class=lnt id=hl-8-54><a class=lnlinks href=#hl-8-54>54</a>
</span><span class=lnt id=hl-8-55><a class=lnlinks href=#hl-8-55>55</a>
</span><span class=lnt id=hl-8-56><a class=lnlinks href=#hl-8-56>56</a>
</span><span class=lnt id=hl-8-57><a class=lnlinks href=#hl-8-57>57</a>
</span><span class=lnt id=hl-8-58><a class=lnlinks href=#hl-8-58>58</a>
</span><span class=lnt id=hl-8-59><a class=lnlinks href=#hl-8-59>59</a>
</span><span class=lnt id=hl-8-60><a class=lnlinks href=#hl-8-60>60</a>
</span><span class=lnt id=hl-8-61><a class=lnlinks href=#hl-8-61>61</a>
</span><span class=lnt id=hl-8-62><a class=lnlinks href=#hl-8-62>62</a>
</span><span class=lnt id=hl-8-63><a class=lnlinks href=#hl-8-63>63</a>
</span><span class=lnt id=hl-8-64><a class=lnlinks href=#hl-8-64>64</a>
</span><span class=lnt id=hl-8-65><a class=lnlinks href=#hl-8-65>65</a>
</span><span class=lnt id=hl-8-66><a class=lnlinks href=#hl-8-66>66</a>
</span><span class=lnt id=hl-8-67><a class=lnlinks href=#hl-8-67>67</a>
</span><span class=lnt id=hl-8-68><a class=lnlinks href=#hl-8-68>68</a>
</span><span class=lnt id=hl-8-69><a class=lnlinks href=#hl-8-69>69</a>
</span><span class=lnt id=hl-8-70><a class=lnlinks href=#hl-8-70>70</a>
</span><span class=lnt id=hl-8-71><a class=lnlinks href=#hl-8-71>71</a>
</span><span class=lnt id=hl-8-72><a class=lnlinks href=#hl-8-72>72</a>
</span><span class=lnt id=hl-8-73><a class=lnlinks href=#hl-8-73>73</a>
</span><span class=lnt id=hl-8-74><a class=lnlinks href=#hl-8-74>74</a>
</span><span class=lnt id=hl-8-75><a class=lnlinks href=#hl-8-75>75</a>
</span><span class=lnt id=hl-8-76><a class=lnlinks href=#hl-8-76>76</a>
</span><span class=lnt id=hl-8-77><a class=lnlinks href=#hl-8-77>77</a>
</span><span class=lnt id=hl-8-78><a class=lnlinks href=#hl-8-78>78</a>
</span><span class=lnt id=hl-8-79><a class=lnlinks href=#hl-8-79>79</a>
</span><span class=lnt id=hl-8-80><a class=lnlinks href=#hl-8-80>80</a>
</span><span class=lnt id=hl-8-81><a class=lnlinks href=#hl-8-81>81</a>
</span><span class=lnt id=hl-8-82><a class=lnlinks href=#hl-8-82>82</a>
</span><span class=lnt id=hl-8-83><a class=lnlinks href=#hl-8-83>83</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Unet class to predict noise from a given image</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>UNet</span><span class=p>(</span><span class=n>hk</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>init_conv</span> <span class=o>=</span> <span class=n>hk</span><span class=o>.</span><span class=n>Conv2D</span><span class=p>(</span><span class=n>output_channels</span><span class=o>=</span><span class=mi>48</span><span class=p>,</span> <span class=n>kernel_shape</span> <span class=o>=</span> <span class=p>(</span><span class=mi>5</span><span class=p>,</span><span class=mi>5</span><span class=p>),</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;SAME&#39;</span><span class=p>,</span> <span class=n>with_bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>norm</span> <span class=o>=</span> <span class=n>hk</span><span class=o>.</span><span class=n>BatchNorm</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=kc>True</span><span class=p>,</span> <span class=n>decay_rate</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>silu</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>silu</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>block1</span> <span class=o>=</span> <span class=n>Block</span><span class=p>(</span><span class=n>output_channels</span><span class=o>=</span><span class=mi>48</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>downsample1</span> <span class=o>=</span> <span class=n>Downsample</span><span class=p>(</span><span class=mi>96</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>block2</span> <span class=o>=</span> <span class=n>Block</span><span class=p>(</span><span class=n>output_channels</span><span class=o>=</span><span class=mi>96</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>downsample2</span> <span class=o>=</span> <span class=n>Downsample</span><span class=p>(</span><span class=mi>192</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>middle_block</span> <span class=o>=</span> <span class=n>Block</span><span class=p>(</span><span class=n>output_channels</span><span class=o>=</span><span class=mi>192</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>upsample1</span> <span class=o>=</span> <span class=n>Upsample</span><span class=p>(</span><span class=mi>96</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>block3</span> <span class=o>=</span> <span class=n>Block</span><span class=p>(</span><span class=n>output_channels</span><span class=o>=</span><span class=mi>96</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>upsample2</span> <span class=o>=</span> <span class=n>Upsample</span><span class=p>(</span><span class=mi>48</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>block4</span> <span class=o>=</span> <span class=n>Block</span><span class=p>(</span><span class=n>output_channels</span><span class=o>=</span><span class=mi>48</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span> <span class=o>=</span> <span class=n>hk</span><span class=o>.</span><span class=n>Conv2D</span><span class=p>(</span><span class=n>output_channels</span><span class=o>=</span><span class=mi>48</span><span class=p>,</span> <span class=n>kernel_shape</span> <span class=o>=</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span><span class=mi>3</span><span class=p>),</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;SAME&#39;</span><span class=p>,</span> <span class=n>with_bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span> <span class=o>=</span> <span class=n>hk</span><span class=o>.</span><span class=n>BatchNorm</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=kc>True</span><span class=p>,</span> <span class=n>decay_rate</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>conv2</span> <span class=o>=</span> <span class=n>hk</span><span class=o>.</span><span class=n>Conv2D</span><span class=p>(</span><span class=n>output_channels</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>kernel_shape</span> <span class=o>=</span> <span class=p>(</span><span class=mi>5</span><span class=p>,</span><span class=mi>5</span><span class=p>),</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;SAME&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>time_mlp</span> <span class=o>=</span> <span class=n>hk</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
</span></span><span class=line><span class=cl>      <span class=n>hk</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>),</span>
</span></span><span class=line><span class=cl>      <span class=n>jax</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>gelu</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=n>hk</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=c1># conditional vectors encoding</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>embedding_vectors</span> <span class=o>=</span> <span class=n>hk</span><span class=o>.</span><span class=n>Embed</span><span class=p>(</span><span class=mi>10</span><span class=o>+</span><span class=mi>26</span><span class=o>+</span><span class=mi>26</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span> <span class=mi>63</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>timestep_embeddings</span> <span class=o>=</span> <span class=n>TimeEmbeddings</span><span class=p>(</span><span class=mi>96</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=fm>__call__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>timesteps</span><span class=p>,</span> <span class=n>cond</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>is_training</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>cond_embedding</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=n>conditioning</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>timesteps</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=n>timestep_embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>timestep_embeddings</span><span class=p>(</span><span class=n>timesteps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=n>conditioning</span> <span class=o>=</span> <span class=n>timestep_embeddings</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>cond</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=n>label_embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding_vectors</span><span class=p>(</span><span class=n>cond</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=n>conditioning</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>concatenate</span><span class=p>([</span><span class=n>label_embeddings</span><span class=p>,</span> <span class=n>conditioning</span><span class=p>],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>conditioning</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>  
</span></span><span class=line><span class=cl>      <span class=n>cond_embedding</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>time_mlp</span><span class=p>(</span><span class=n>conditioning</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>h</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>silu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>init_conv</span><span class=p>(</span><span class=n>x</span><span class=p>),</span> <span class=n>is_training</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>xx</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>copy</span><span class=p>(</span><span class=n>h</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>b1</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>block1</span><span class=p>(</span><span class=n>h</span><span class=p>,</span> <span class=n>cond_embedding</span><span class=p>,</span> <span class=n>is_training</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>h</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>downsample1</span><span class=p>(</span><span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>b2</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>block2</span><span class=p>(</span><span class=n>h</span><span class=p>,</span> <span class=n>cond_embedding</span><span class=p>,</span> <span class=n>is_training</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>h</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>downsample2</span><span class=p>(</span><span class=n>b2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>h</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>upsample1</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>middle_block</span><span class=p>(</span><span class=n>h</span><span class=p>,</span> <span class=n>cond_embedding</span><span class=p>,</span> <span class=n>is_training</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>b3</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>block3</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>concatenate</span><span class=p>((</span><span class=n>h</span><span class=p>,</span> <span class=n>b2</span><span class=p>),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>3</span><span class=p>),</span> <span class=n>cond_embedding</span><span class=p>,</span> <span class=n>is_training</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>h</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>upsample2</span><span class=p>(</span><span class=n>b3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>b4</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>block4</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>concatenate</span><span class=p>((</span><span class=n>h</span><span class=p>,</span> <span class=n>b1</span><span class=p>),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>3</span><span class=p>),</span> <span class=n>cond_embedding</span><span class=p>,</span> <span class=n>is_training</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>h</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv2</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>silu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>norm1</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>conv1</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>concatenate</span><span class=p>((</span><span class=n>xx</span><span class=p>,</span> <span class=n>b4</span><span class=p>),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>3</span><span class=p>)),</span> <span class=n>is_training</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>h</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Block</span><span class=p>(</span><span class=n>hk</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=c1># a basic resnet style convolutional block</span>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>output_channels</span><span class=p>,</span> <span class=n>kernel_size</span><span class=p>,</span> <span class=n>padding</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>proj</span> <span class=o>=</span> <span class=n>hk</span><span class=o>.</span><span class=n>Conv2D</span><span class=p>(</span><span class=n>output_channels</span><span class=o>=</span><span class=n>output_channels</span><span class=p>,</span> <span class=n>kernel_shape</span><span class=o>=</span><span class=p>(</span><span class=n>kernel_size</span><span class=p>,</span> <span class=n>kernel_size</span><span class=p>),</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;SAME&#39;</span><span class=p>,</span> <span class=n>with_bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># using batch norm instead of layernorm as the batch sizes are large</span>
</span></span><span class=line><span class=cl>    <span class=c1># orig: self.norm = hk.LayerNorm(axis=(-3, -2, -1), create_scale=True, create_offset=True)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>norm</span> <span class=o>=</span> <span class=n>hk</span><span class=o>.</span><span class=n>BatchNorm</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=kc>True</span><span class=p>,</span> <span class=n>decay_rate</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>silu</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>silu</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span> <span class=o>=</span> <span class=n>hk</span><span class=o>.</span><span class=n>Conv2D</span><span class=p>(</span><span class=n>output_channels</span><span class=o>=</span><span class=n>output_channels</span><span class=p>,</span> <span class=n>kernel_shape</span><span class=o>=</span><span class=p>(</span><span class=n>kernel_size</span><span class=p>,</span> <span class=n>kernel_size</span><span class=p>),</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;SAME&#39;</span><span class=p>,</span> <span class=n>with_bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span> <span class=o>=</span> <span class=n>hk</span><span class=o>.</span><span class=n>BatchNorm</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=kc>True</span><span class=p>,</span> <span class=n>decay_rate</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>out_conv</span> <span class=o>=</span> <span class=n>hk</span><span class=o>.</span><span class=n>Conv2D</span><span class=p>(</span><span class=n>output_channels</span><span class=o>=</span><span class=n>output_channels</span><span class=p>,</span> <span class=n>kernel_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;SAME&#39;</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>    <span class=c1># self.time_mlp = None</span>
</span></span><span class=line><span class=cl>    <span class=n>dims</span> <span class=o>=</span> <span class=n>output_channels</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>time_mlp</span> <span class=o>=</span> <span class=n>hk</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
</span></span><span class=line><span class=cl>      <span class=n>jax</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>silu</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=n>hk</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>dims</span><span class=o>*</span><span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>])</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=fm>__call__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>timestep_embeddings</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>is_training</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>h</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>h</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>h</span><span class=p>,</span> <span class=n>is_training</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>timestep_embeddings</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>time_mlp</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=n>time_embedding</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>time_mlp</span><span class=p>(</span><span class=n>timestep_embeddings</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=n>time_embedding</span> <span class=o>=</span> <span class=n>einops</span><span class=o>.</span><span class=n>rearrange</span><span class=p>(</span><span class=n>time_embedding</span><span class=p>,</span> <span class=s1>&#39;b c -&gt; b 1 1 c&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=n>shift</span><span class=p>,</span> <span class=n>scale</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>time_embedding</span><span class=p>,</span> <span class=n>indices_or_sections</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=n>h</span> <span class=o>=</span> <span class=n>shift</span> <span class=o>+</span> <span class=p>(</span><span class=n>scale</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span><span class=o>*</span><span class=n>h</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>h</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>silu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>norm1</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>conv1</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>silu</span><span class=p>(</span><span class=n>h</span><span class=p>)),</span> <span class=n>is_training</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>out_conv</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>+</span> <span class=n>h</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=training-code>Training Code:<a hidden class=anchor aria-hidden=true href=#training-code>#</a></h2><p>Loss function: I am using the Huber loss instead of using the L2 or the L1 loss. I would assume changing the loss function wouldn&rsquo;t impact the results significantly.</p><p>Note: the importance weight determines the importance of the sample for denoising. As noted in <cite>Improved Denoising Diffusion Probabilistic Models<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></cite>, one idea to train diffusion models is to completely ignore this term. This helps the model training.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-9-1><a class=lnlinks href=#hl-9-1> 1</a>
</span><span class=lnt id=hl-9-2><a class=lnlinks href=#hl-9-2> 2</a>
</span><span class=lnt id=hl-9-3><a class=lnlinks href=#hl-9-3> 3</a>
</span><span class=lnt id=hl-9-4><a class=lnlinks href=#hl-9-4> 4</a>
</span><span class=lnt id=hl-9-5><a class=lnlinks href=#hl-9-5> 5</a>
</span><span class=lnt id=hl-9-6><a class=lnlinks href=#hl-9-6> 6</a>
</span><span class=lnt id=hl-9-7><a class=lnlinks href=#hl-9-7> 7</a>
</span><span class=lnt id=hl-9-8><a class=lnlinks href=#hl-9-8> 8</a>
</span><span class=lnt id=hl-9-9><a class=lnlinks href=#hl-9-9> 9</a>
</span><span class=lnt id=hl-9-10><a class=lnlinks href=#hl-9-10>10</a>
</span><span class=lnt id=hl-9-11><a class=lnlinks href=#hl-9-11>11</a>
</span><span class=lnt id=hl-9-12><a class=lnlinks href=#hl-9-12>12</a>
</span><span class=lnt id=hl-9-13><a class=lnlinks href=#hl-9-13>13</a>
</span><span class=lnt id=hl-9-14><a class=lnlinks href=#hl-9-14>14</a>
</span><span class=lnt id=hl-9-15><a class=lnlinks href=#hl-9-15>15</a>
</span><span class=lnt id=hl-9-16><a class=lnlinks href=#hl-9-16>16</a>
</span><span class=lnt id=hl-9-17><a class=lnlinks href=#hl-9-17>17</a>
</span><span class=lnt id=hl-9-18><a class=lnlinks href=#hl-9-18>18</a>
</span><span class=lnt id=hl-9-19><a class=lnlinks href=#hl-9-19>19</a>
</span><span class=lnt id=hl-9-20><a class=lnlinks href=#hl-9-20>20</a>
</span><span class=lnt id=hl-9-21><a class=lnlinks href=#hl-9-21>21</a>
</span><span class=lnt id=hl-9-22><a class=lnlinks href=#hl-9-22>22</a>
</span><span class=lnt id=hl-9-23><a class=lnlinks href=#hl-9-23>23</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># using jax.jit to speed up computation of the loss function</span>
</span></span><span class=line><span class=cl><span class=n>partial</span><span class=p>(</span><span class=n>jax</span><span class=o>.</span><span class=n>jit</span><span class=p>,</span>  <span class=n>static_argnums</span><span class=o>=</span><span class=p>(</span><span class=mi>4</span><span class=p>,))</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>compute_loss</span><span class=p>(</span><span class=n>params</span><span class=p>:</span> <span class=n>hk</span><span class=o>.</span><span class=n>Params</span><span class=p>,</span> <span class=n>state</span><span class=p>:</span> <span class=n>hk</span><span class=o>.</span><span class=n>State</span><span class=p>,</span> <span class=n>batch</span><span class=p>:</span> <span class=n>Batch</span><span class=p>,</span> <span class=n>is_energy_method</span><span class=p>:</span> <span class=nb>bool</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>is_training</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>jnp</span><span class=o>.</span><span class=n>ndarray</span><span class=p>,</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>jnp</span><span class=o>.</span><span class=n>ndarray</span><span class=p>,</span> <span class=n>hk</span><span class=o>.</span><span class=n>State</span><span class=p>]]:</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;&#34;&#34;Compute the loss of the network, including L2.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>  <span class=n>x</span><span class=p>,</span> <span class=n>label</span><span class=p>,</span> <span class=n>timestep</span><span class=p>,</span> <span class=n>noise</span> <span class=o>=</span> <span class=n>batch</span>
</span></span><span class=line><span class=cl>  <span class=c1># not capturing state as it is not needed; it should be internally updated and maintaing by haiku and doesn&#39;t need gradient updates </span>
</span></span><span class=line><span class=cl>  <span class=n>pred_data</span><span class=p>,</span> <span class=n>state</span> <span class=o>=</span> <span class=n>net</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>state</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>timestep</span><span class=p>,</span> <span class=n>label</span><span class=p>,</span> <span class=n>is_training</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>error_func</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>imp_weight</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=c1># 1/2 * (1/sigma_squared_q_t_corrected[timestep]) * ((betas[timestep])**2 / (variance[timestep] * alphas[timestep]))</span>
</span></span><span class=line><span class=cl>      <span class=c1># loss on prediction</span>
</span></span><span class=line><span class=cl>    <span class=n>loss_</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>multiply</span><span class=p>(</span><span class=n>imp_weight</span><span class=p>,</span> <span class=n>huber_loss</span><span class=p>(</span><span class=n>noise</span><span class=p>,</span> <span class=n>pred_data</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>loss_</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>energy_func</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=c1>## Energy function interpretation</span>
</span></span><span class=line><span class=cl>    <span class=n>imp_weight</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=c1># 1/2 * (1/sigma_squared_q_t_corrected[timestep]) * ((betas[timestep])**2 / (alphas[timestep]))</span>
</span></span><span class=line><span class=cl>    <span class=c1># loss on prediction</span>
</span></span><span class=line><span class=cl>    <span class=n>loss_</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>multiply</span><span class=p>(</span><span class=n>imp_weight</span><span class=p>,</span> <span class=n>huber_loss</span><span class=p>(</span><span class=n>pred_data</span><span class=p>,</span> <span class=n>jnp</span><span class=o>.</span><span class=n>divide</span><span class=p>(</span><span class=n>noise</span><span class=p>,</span> <span class=o>-</span><span class=n>sd</span><span class=p>[</span><span class=n>timestep</span><span class=p>]))))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>loss_</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>  <span class=n>loss_</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>lax</span><span class=o>.</span><span class=n>cond</span><span class=p>(</span><span class=n>is_energy_method</span><span class=p>,</span> <span class=n>energy_func</span><span class=p>,</span> <span class=n>error_func</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>loss_</span><span class=p>,</span> <span class=p>(</span><span class=n>loss_</span><span class=p>,</span> <span class=n>state</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>Updating Model Weights:</strong></p><p>This is a typical code for any Neural Network training in Haiku/Optax and JAX. We are finding the gradient of the loss with respect to the parameters of the Neural Network using JAX and updating the weights using Optax.</p><p>Additionally, we are doing exponential updates to the parameters.<cite> Paper on Polyak averaging.<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></cite></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-10-1><a class=lnlinks href=#hl-10-1> 1</a>
</span><span class=lnt id=hl-10-2><a class=lnlinks href=#hl-10-2> 2</a>
</span><span class=lnt id=hl-10-3><a class=lnlinks href=#hl-10-3> 3</a>
</span><span class=lnt id=hl-10-4><a class=lnlinks href=#hl-10-4> 4</a>
</span><span class=lnt id=hl-10-5><a class=lnlinks href=#hl-10-5> 5</a>
</span><span class=lnt id=hl-10-6><a class=lnlinks href=#hl-10-6> 6</a>
</span><span class=lnt id=hl-10-7><a class=lnlinks href=#hl-10-7> 7</a>
</span><span class=lnt id=hl-10-8><a class=lnlinks href=#hl-10-8> 8</a>
</span><span class=lnt id=hl-10-9><a class=lnlinks href=#hl-10-9> 9</a>
</span><span class=lnt id=hl-10-10><a class=lnlinks href=#hl-10-10>10</a>
</span><span class=lnt id=hl-10-11><a class=lnlinks href=#hl-10-11>11</a>
</span><span class=lnt id=hl-10-12><a class=lnlinks href=#hl-10-12>12</a>
</span><span class=lnt id=hl-10-13><a class=lnlinks href=#hl-10-13>13</a>
</span><span class=lnt id=hl-10-14><a class=lnlinks href=#hl-10-14>14</a>
</span><span class=lnt id=hl-10-15><a class=lnlinks href=#hl-10-15>15</a>
</span><span class=lnt id=hl-10-16><a class=lnlinks href=#hl-10-16>16</a>
</span><span class=lnt id=hl-10-17><a class=lnlinks href=#hl-10-17>17</a>
</span><span class=lnt id=hl-10-18><a class=lnlinks href=#hl-10-18>18</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@jax.jit</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>update</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>params</span><span class=p>:</span> <span class=n>hk</span><span class=o>.</span><span class=n>Params</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>state</span><span class=p>:</span> <span class=n>hk</span><span class=o>.</span><span class=n>State</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>opt_state</span><span class=p>:</span> <span class=n>optax</span><span class=o>.</span><span class=n>OptState</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>batch</span><span class=p>:</span> <span class=n>Batch</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>is_energy_method</span><span class=p>:</span> <span class=nb>bool</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>jnp</span><span class=o>.</span><span class=n>ndarray</span><span class=p>,</span> <span class=n>hk</span><span class=o>.</span><span class=n>Params</span><span class=p>,</span> <span class=n>optax</span><span class=o>.</span><span class=n>OptState</span><span class=p>,</span> <span class=n>hk</span><span class=o>.</span><span class=n>State</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;&#34;&#34;Compute gradients and update the weights&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>  <span class=n>grads</span><span class=p>,</span> <span class=p>(</span><span class=n>loss_value</span><span class=p>,</span> <span class=n>state</span><span class=p>)</span><span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>grad</span><span class=p>(</span><span class=n>compute_loss</span><span class=p>,</span> <span class=n>has_aux</span><span class=o>=</span><span class=kc>True</span><span class=p>)(</span><span class=n>params</span><span class=p>,</span> <span class=n>state</span><span class=p>,</span> <span class=n>batch</span><span class=p>,</span> <span class=n>is_energy_method</span><span class=p>,</span> <span class=n>is_training</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>updates</span><span class=p>,</span> <span class=n>opt_state</span> <span class=o>=</span> <span class=n>opt</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=n>grads</span><span class=p>,</span> <span class=n>opt_state</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>new_params</span> <span class=o>=</span> <span class=n>optax</span><span class=o>.</span><span class=n>apply_updates</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>updates</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>loss_value</span><span class=p>,</span> <span class=n>new_params</span><span class=p>,</span> <span class=n>opt_state</span><span class=p>,</span> <span class=n>state</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@jax.jit</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>ema_update</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>avg_params</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;&#34;&#34;Incrementally update parameters via polyak averaging.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>  <span class=c1># Polyak averaging tracks an (exponential moving) average of the past parameters of a model, for use at test/evaluation time.</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>optax</span><span class=o>.</span><span class=n>incremental_update</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>avg_params</span><span class=p>,</span> <span class=n>step_size</span><span class=o>=</span><span class=mf>0.95</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=training-code-1>Training Code<a hidden class=anchor aria-hidden=true href=#training-code-1>#</a></h3><p>The training code is pretty straight-forward. Below, I have removed the code to checkpoint the different models.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-11-1><a class=lnlinks href=#hl-11-1> 1</a>
</span><span class=lnt id=hl-11-2><a class=lnlinks href=#hl-11-2> 2</a>
</span><span class=lnt id=hl-11-3><a class=lnlinks href=#hl-11-3> 3</a>
</span><span class=lnt id=hl-11-4><a class=lnlinks href=#hl-11-4> 4</a>
</span><span class=lnt id=hl-11-5><a class=lnlinks href=#hl-11-5> 5</a>
</span><span class=lnt id=hl-11-6><a class=lnlinks href=#hl-11-6> 6</a>
</span><span class=lnt id=hl-11-7><a class=lnlinks href=#hl-11-7> 7</a>
</span><span class=lnt id=hl-11-8><a class=lnlinks href=#hl-11-8> 8</a>
</span><span class=lnt id=hl-11-9><a class=lnlinks href=#hl-11-9> 9</a>
</span><span class=lnt id=hl-11-10><a class=lnlinks href=#hl-11-10>10</a>
</span><span class=lnt id=hl-11-11><a class=lnlinks href=#hl-11-11>11</a>
</span><span class=lnt id=hl-11-12><a class=lnlinks href=#hl-11-12>12</a>
</span><span class=lnt id=hl-11-13><a class=lnlinks href=#hl-11-13>13</a>
</span><span class=lnt id=hl-11-14><a class=lnlinks href=#hl-11-14>14</a>
</span><span class=lnt id=hl-11-15><a class=lnlinks href=#hl-11-15>15</a>
</span><span class=lnt id=hl-11-16><a class=lnlinks href=#hl-11-16>16</a>
</span><span class=lnt id=hl-11-17><a class=lnlinks href=#hl-11-17>17</a>
</span><span class=lnt id=hl-11-18><a class=lnlinks href=#hl-11-18>18</a>
</span><span class=lnt id=hl-11-19><a class=lnlinks href=#hl-11-19>19</a>
</span><span class=lnt id=hl-11-20><a class=lnlinks href=#hl-11-20>20</a>
</span><span class=lnt id=hl-11-21><a class=lnlinks href=#hl-11-21>21</a>
</span><span class=lnt id=hl-11-22><a class=lnlinks href=#hl-11-22>22</a>
</span><span class=lnt id=hl-11-23><a class=lnlinks href=#hl-11-23>23</a>
</span><span class=lnt id=hl-11-24><a class=lnlinks href=#hl-11-24>24</a>
</span><span class=lnt id=hl-11-25><a class=lnlinks href=#hl-11-25>25</a>
</span><span class=lnt id=hl-11-26><a class=lnlinks href=#hl-11-26>26</a>
</span><span class=lnt id=hl-11-27><a class=lnlinks href=#hl-11-27>27</a>
</span><span class=lnt id=hl-11-28><a class=lnlinks href=#hl-11-28>28</a>
</span><span class=lnt id=hl-11-29><a class=lnlinks href=#hl-11-29>29</a>
</span><span class=lnt id=hl-11-30><a class=lnlinks href=#hl-11-30>30</a>
</span><span class=lnt id=hl-11-31><a class=lnlinks href=#hl-11-31>31</a>
</span><span class=lnt id=hl-11-32><a class=lnlinks href=#hl-11-32>32</a>
</span><span class=lnt id=hl-11-33><a class=lnlinks href=#hl-11-33>33</a>
</span><span class=lnt id=hl-11-34><a class=lnlinks href=#hl-11-34>34</a>
</span><span class=lnt id=hl-11-35><a class=lnlinks href=#hl-11-35>35</a>
</span><span class=lnt id=hl-11-36><a class=lnlinks href=#hl-11-36>36</a>
</span><span class=lnt id=hl-11-37><a class=lnlinks href=#hl-11-37>37</a>
</span><span class=lnt id=hl-11-38><a class=lnlinks href=#hl-11-38>38</a>
</span><span class=lnt id=hl-11-39><a class=lnlinks href=#hl-11-39>39</a>
</span><span class=lnt id=hl-11-40><a class=lnlinks href=#hl-11-40>40</a>
</span><span class=lnt id=hl-11-41><a class=lnlinks href=#hl-11-41>41</a>
</span><span class=lnt id=hl-11-42><a class=lnlinks href=#hl-11-42>42</a>
</span><span class=lnt id=hl-11-43><a class=lnlinks href=#hl-11-43>43</a>
</span><span class=lnt id=hl-11-44><a class=lnlinks href=#hl-11-44>44</a>
</span><span class=lnt id=hl-11-45><a class=lnlinks href=#hl-11-45>45</a>
</span><span class=lnt id=hl-11-46><a class=lnlinks href=#hl-11-46>46</a>
</span><span class=lnt id=hl-11-47><a class=lnlinks href=#hl-11-47>47</a>
</span><span class=lnt id=hl-11-48><a class=lnlinks href=#hl-11-48>48</a>
</span><span class=lnt id=hl-11-49><a class=lnlinks href=#hl-11-49>49</a>
</span><span class=lnt id=hl-11-50><a class=lnlinks href=#hl-11-50>50</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># initialization</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>f</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>timesteps</span><span class=p>,</span> <span class=n>label</span><span class=p>,</span> <span class=n>is_training</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>unet</span> <span class=o>=</span> <span class=n>UNet</span><span class=p>()</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>unet</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>timesteps</span><span class=p>,</span> <span class=n>label</span><span class=p>,</span> <span class=n>is_training</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>f_t</span> <span class=o>=</span> <span class=n>hk</span><span class=o>.</span><span class=n>transform_with_state</span><span class=p>(</span><span class=n>f</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>net</span> <span class=o>=</span> <span class=n>hk</span><span class=o>.</span><span class=n>without_apply_rng</span><span class=p>(</span><span class=n>f_t</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>params</span><span class=p>,</span> <span class=n>state</span> <span class=o>=</span> <span class=n>net</span><span class=o>.</span><span class=n>init</span><span class=p>(</span><span class=n>rng</span><span class=p>,</span> <span class=n>image</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>:</span><span class=n>batch_size</span><span class=p>],</span> <span class=n>timesteps_</span><span class=p>,</span> <span class=n>label</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>:</span><span class=n>batch_size</span><span class=p>],</span> <span class=n>is_training</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>opt</span> <span class=o>=</span> <span class=n>optax</span><span class=o>.</span><span class=n>adam</span><span class=p>(</span><span class=mf>1e-3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>avg_params</span> <span class=o>=</span> <span class=n>deepcopy</span><span class=p>(</span><span class=n>params</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>opt_state</span> <span class=o>=</span> <span class=n>opt</span><span class=o>.</span><span class=n>init</span><span class=p>(</span><span class=n>params</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>batches_iter</span> <span class=o>=</span> <span class=mi>10000</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># maintaining a batch on which we will measure loss; we will save the model based on performance on this batch</span>
</span></span><span class=line><span class=cl><span class=n>one_timestep</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>mod</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>+</span><span class=mi>1</span><span class=p>),</span> <span class=n>timesteps</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>train</span> <span class=o>=</span> <span class=n>create_input_iter</span><span class=p>(</span><span class=n>train_ds</span><span class=p>)</span> 
</span></span><span class=line><span class=cl><span class=n>data_in_batch_</span><span class=p>,</span> <span class=n>label_</span> <span class=o>=</span> <span class=nb>next</span><span class=p>(</span><span class=n>train</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>data_in_batch_</span> <span class=o>=</span> <span class=n>data_in_batch_</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>label_</span> <span class=o>=</span> <span class=n>label_</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>data_noisy_temp_</span><span class=p>,</span> <span class=n>noise_temp_</span> <span class=o>=</span> <span class=n>get_noisy</span><span class=p>(</span><span class=n>key</span><span class=p>,</span> <span class=n>data_in_batch_</span><span class=p>,</span> <span class=n>one_timestep</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># main method for training</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train_model</span><span class=p>(</span><span class=n>opt_state</span><span class=p>,</span> <span class=n>params</span><span class=p>,</span> <span class=n>avg_params</span><span class=p>,</span> <span class=n>state</span><span class=p>,</span> <span class=n>energy_method</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>best_loss</span> <span class=o>=</span> <span class=n>sys</span><span class=o>.</span><span class=n>float_info</span><span class=o>.</span><span class=n>max</span> <span class=c1># initialization   </span>
</span></span><span class=line><span class=cl>	<span class=n>unique_key</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>fold_in</span><span class=p>(</span><span class=n>key</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># same subkey being used for noise sampling, as it doesn&#39;t matter :)</span>
</span></span><span class=line><span class=cl>	<span class=n>_</span><span class=p>,</span> <span class=o>*</span><span class=n>timestep_subkeys</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>unique_key</span><span class=p>,</span> <span class=n>batches_iter</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=n>losses</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>iteration</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>batches_iter</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>data_in_batch</span><span class=p>,</span> <span class=n>label</span> <span class=o>=</span> <span class=nb>next</span><span class=p>(</span><span class=n>train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>data_in_batch</span> <span class=o>=</span> <span class=n>data_in_batch</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>		<span class=n>label</span> <span class=o>=</span> <span class=n>label</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>		<span class=n>idx</span> <span class=o>=</span> <span class=p>(</span><span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=n>key</span><span class=o>=</span><span class=n>timestep_subkeys</span><span class=p>[</span><span class=n>iteration</span><span class=p>],</span> <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span> <span class=o>*</span> <span class=p>(</span><span class=n>timesteps</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>idx</span> <span class=o>=</span> <span class=n>einops</span><span class=o>.</span><span class=n>rearrange</span><span class=p>(</span><span class=n>idx</span><span class=p>,</span> <span class=s1>&#39;a b -&gt; (a b)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>timestep</span> <span class=o>=</span> <span class=n>idx</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>		<span class=n>data_noisy</span><span class=p>,</span> <span class=n>noise</span> <span class=o>=</span> <span class=n>get_noisy</span><span class=p>(</span><span class=n>timestep_subkeys</span><span class=p>[</span><span class=n>iteration</span><span class=p>],</span> <span class=n>data_in_batch</span><span class=p>,</span> <span class=n>timestep</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=c1># todo: call gradient update function here</span>
</span></span><span class=line><span class=cl>		<span class=n>loss_value</span><span class=p>,</span> <span class=n>params</span><span class=p>,</span> <span class=n>opt_state</span><span class=p>,</span> <span class=n>state</span> <span class=o>=</span> <span class=n>update</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>state</span><span class=p>,</span> <span class=n>opt_state</span><span class=p>,</span> <span class=p>[</span><span class=n>data_noisy</span><span class=p>,</span> <span class=n>label</span><span class=p>,</span> <span class=n>timestep</span><span class=p>,</span> <span class=n>noise</span><span class=p>],</span> <span class=n>energy_method</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>avg_params</span> <span class=o>=</span> <span class=n>ema_update</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>avg_params</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		                
</span></span><span class=line><span class=cl>		<span class=c1>## evaluating noise on a fixed timestep to calculate best model</span>
</span></span><span class=line><span class=cl>		<span class=n>loss_temp</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>device_get</span><span class=p>(</span><span class=n>compute_loss</span><span class=p>(</span><span class=n>avg_params</span><span class=p>,</span> <span class=n>state</span><span class=p>,</span> <span class=p>[</span><span class=n>data_noisy_temp_</span><span class=p>,</span> <span class=n>label_</span><span class=p>,</span> <span class=n>one_timestep</span><span class=p>,</span> <span class=n>noise_temp_</span><span class=p>],</span> <span class=n>energy_method</span><span class=p>,</span> <span class=n>is_training</span><span class=o>=</span><span class=kc>False</span><span class=p>))</span>
</span></span><span class=line><span class=cl>		<span class=n>losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>loss_temp</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>		<span class=k>if</span> <span class=n>loss_temp</span> <span class=o>&lt;</span> <span class=n>best_loss</span><span class=p>:</span>
</span></span><span class=line><span class=cl>			<span class=n>best_loss</span> <span class=o>=</span> <span class=n>loss_temp</span>
</span></span><span class=line><span class=cl>			<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;saving iteration: </span><span class=si>{</span><span class=n>iteration</span><span class=si>}</span><span class=s2> loss: </span><span class=si>{</span><span class=n>best_loss</span><span class=si>:</span><span class=s2>&gt;7f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>return</span> <span class=n>data_noisy</span><span class=p>,</span> <span class=n>data_in_batch</span><span class=p>,</span> <span class=n>timestep</span><span class=p>,</span> <span class=n>losses</span><span class=p>,</span> <span class=n>avg_params</span><span class=p>,</span> <span class=n>state</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=generating-conditional-samples>Generating Conditional Samples<a hidden class=anchor aria-hidden=true href=#generating-conditional-samples>#</a></h2><h3 id=using-all-time-steps-for-generation>Using all time steps for generation<a hidden class=anchor aria-hidden=true href=#using-all-time-steps-for-generation>#</a></h3><p>Code for generating samples is below. We start off with random samples from a Standard Gaussian Distribution and follow steps as described <a href=/posts/diffusion-models/denoising-diffusion-models-1/#2-denoising-step>here</a>.
We will be using the naïve version of classifier guidance described <a href=/posts/diffusion-models/denoising-diffusion-models-2/#guidance--classifier-free-guidance>here</a>.</p><ul><li>Start by sampling a random image from a Sandard Gaussian distribution at time step $T$.</li><li>Get an estimate of the error added to the input image from the U-net.</li><li>Use the equation 5, in the link above, to calculate the image at the previous time step $T-1$.</li><li>Repeat until time step 1.</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-12-1><a class=lnlinks href=#hl-12-1>1</a>
</span><span class=lnt id=hl-12-2><a class=lnlinks href=#hl-12-2>2</a>
</span><span class=lnt id=hl-12-3><a class=lnlinks href=#hl-12-3>3</a>
</span><span class=lnt id=hl-12-4><a class=lnlinks href=#hl-12-4>4</a>
</span><span class=lnt id=hl-12-5><a class=lnlinks href=#hl-12-5>5</a>
</span><span class=lnt id=hl-12-6><a class=lnlinks href=#hl-12-6>6</a>
</span><span class=lnt id=hl-12-7><a class=lnlinks href=#hl-12-7>7</a>
</span><span class=lnt id=hl-12-8><a class=lnlinks href=#hl-12-8>8</a>
</span><span class=lnt id=hl-12-9><a class=lnlinks href=#hl-12-9>9</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># defining useful variables</span>
</span></span><span class=line><span class=cl><span class=n>alphas_prev_</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>pad</span><span class=p>(</span><span class=n>alphas_</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=s2>&#34;constant&#34;</span><span class=p>,</span> <span class=n>constant_values</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>sigma_squared_q_t</span> <span class=o>=</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>alphas</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>alphas_</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>alphas_prev_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>log_sigma_squared_q_t</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>alphas</span><span class=p>)</span> <span class=o>+</span> <span class=n>jnp</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>alphas_</span><span class=p>)</span> <span class=o>-</span> <span class=n>jnp</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>alphas_prev_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>sigma_squared_q_t_corrected</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>log_sigma_squared_q_t</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>key</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>PRNGKey</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>mean_coeff_1</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>alphas</span><span class=p>)</span><span class=o>*</span><span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>alphas_prev_</span><span class=p>)</span> <span class=o>/</span> <span class=n>variance</span>
</span></span><span class=line><span class=cl><span class=n>mean_coeff_2</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>alphas_prev_</span><span class=p>)</span> <span class=o>*</span> <span class=n>betas</span> <span class=o>/</span> <span class=n>variance</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>Generate Samples:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-13-1><a class=lnlinks href=#hl-13-1> 1</a>
</span><span class=lnt id=hl-13-2><a class=lnlinks href=#hl-13-2> 2</a>
</span><span class=lnt id=hl-13-3><a class=lnlinks href=#hl-13-3> 3</a>
</span><span class=lnt id=hl-13-4><a class=lnlinks href=#hl-13-4> 4</a>
</span><span class=lnt id=hl-13-5><a class=lnlinks href=#hl-13-5> 5</a>
</span><span class=lnt id=hl-13-6><a class=lnlinks href=#hl-13-6> 6</a>
</span><span class=lnt id=hl-13-7><a class=lnlinks href=#hl-13-7> 7</a>
</span><span class=lnt id=hl-13-8><a class=lnlinks href=#hl-13-8> 8</a>
</span><span class=lnt id=hl-13-9><a class=lnlinks href=#hl-13-9> 9</a>
</span><span class=lnt id=hl-13-10><a class=lnlinks href=#hl-13-10>10</a>
</span><span class=lnt id=hl-13-11><a class=lnlinks href=#hl-13-11>11</a>
</span><span class=lnt id=hl-13-12><a class=lnlinks href=#hl-13-12>12</a>
</span><span class=lnt id=hl-13-13><a class=lnlinks href=#hl-13-13>13</a>
</span><span class=lnt id=hl-13-14><a class=lnlinks href=#hl-13-14>14</a>
</span><span class=lnt id=hl-13-15><a class=lnlinks href=#hl-13-15>15</a>
</span><span class=lnt id=hl-13-16><a class=lnlinks href=#hl-13-16>16</a>
</span><span class=lnt id=hl-13-17><a class=lnlinks href=#hl-13-17>17</a>
</span><span class=lnt id=hl-13-18><a class=lnlinks href=#hl-13-18>18</a>
</span><span class=lnt id=hl-13-19><a class=lnlinks href=#hl-13-19>19</a>
</span><span class=lnt id=hl-13-20><a class=lnlinks href=#hl-13-20>20</a>
</span><span class=lnt id=hl-13-21><a class=lnlinks href=#hl-13-21>21</a>
</span><span class=lnt id=hl-13-22><a class=lnlinks href=#hl-13-22>22</a>
</span><span class=lnt id=hl-13-23><a class=lnlinks href=#hl-13-23>23</a>
</span><span class=lnt id=hl-13-24><a class=lnlinks href=#hl-13-24>24</a>
</span><span class=lnt id=hl-13-25><a class=lnlinks href=#hl-13-25>25</a>
</span><span class=lnt id=hl-13-26><a class=lnlinks href=#hl-13-26>26</a>
</span><span class=lnt id=hl-13-27><a class=lnlinks href=#hl-13-27>27</a>
</span><span class=lnt id=hl-13-28><a class=lnlinks href=#hl-13-28>28</a>
</span><span class=lnt id=hl-13-29><a class=lnlinks href=#hl-13-29>29</a>
</span><span class=lnt id=hl-13-30><a class=lnlinks href=#hl-13-30>30</a>
</span><span class=lnt id=hl-13-31><a class=lnlinks href=#hl-13-31>31</a>
</span><span class=lnt id=hl-13-32><a class=lnlinks href=#hl-13-32>32</a>
</span><span class=lnt id=hl-13-33><a class=lnlinks href=#hl-13-33>33</a>
</span><span class=lnt id=hl-13-34><a class=lnlinks href=#hl-13-34>34</a>
</span><span class=lnt id=hl-13-35><a class=lnlinks href=#hl-13-35>35</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>random</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>generate_data</span><span class=p>(</span><span class=n>avg_params</span><span class=p>,</span> <span class=n>state</span><span class=p>,</span> <span class=n>label</span><span class=p>,</span> <span class=n>energy_method</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>clipped_version</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size_generation</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>label</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>unique_key</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>fold_in</span><span class=p>(</span><span class=n>key</span><span class=p>,</span> <span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>100</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>_</span><span class=p>,</span> <span class=n>subkey</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>unique_key</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>_</span><span class=p>,</span> <span class=o>*</span><span class=n>subkeys</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>unique_key</span><span class=p>,</span> <span class=n>timesteps</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># need to generate new keys everytime</span>
</span></span><span class=line><span class=cl>    <span class=n>data_noisy</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>subkey</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=n>batch_size_generation</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>    <span class=n>data_in_batch</span> <span class=o>=</span> <span class=n>data_noisy</span>
</span></span><span class=line><span class=cl>    <span class=n>datas</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=n>datas</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>jax</span><span class=o>.</span><span class=n>device_get</span><span class=p>(</span><span class=n>data_noisy</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>timesteps</span><span class=o>+</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>timestep</span> <span class=o>=</span> <span class=n>timesteps</span><span class=o>-</span><span class=n>t</span>
</span></span><span class=line><span class=cl>        <span class=n>t_repeated</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>repeat</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=n>timestep</span><span class=p>]),</span> <span class=n>batch_size_generation</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># data_stacked = torch.vstack([data_in_batch, labelled_values])</span>
</span></span><span class=line><span class=cl>        <span class=n>pred_data</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>net</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=n>avg_params</span><span class=p>,</span> <span class=n>state</span><span class=p>,</span> <span class=n>data_in_batch</span><span class=p>,</span> <span class=n>t_repeated</span><span class=p>,</span> <span class=n>label</span><span class=p>,</span> <span class=n>is_training</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=c1># clipping an improvement as recommended in https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils.py#L171</span>
</span></span><span class=line><span class=cl>		<span class=c1># this helps in improving the samples generated as it keeps the random variables in the range of 0 to +1</span>
</span></span><span class=line><span class=cl>		<span class=n>x_reconstructed</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>subtract</span><span class=p>(</span><span class=n>data_in_batch</span><span class=p>,</span> <span class=n>pred_data</span> <span class=o>*</span> <span class=n>sd</span><span class=p>[</span><span class=n>timestep</span><span class=p>])</span><span class=o>/</span> <span class=n>jnp</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>alphas_</span><span class=p>[</span><span class=n>timestep</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>		<span class=k>if</span> <span class=n>timestep</span> <span class=o>&gt;=</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>			<span class=n>x_reconstructed</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=n>x_reconstructed</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>		<span class=n>mean_data_1</span> <span class=o>=</span> <span class=n>data_in_batch</span> <span class=o>*</span> <span class=n>mean_coeff_1</span><span class=p>[</span><span class=n>timestep</span><span class=p>]</span>
</span></span><span class=line><span class=cl>		<span class=n>mean_data_2</span> <span class=o>=</span> <span class=n>x_reconstructed</span> <span class=o>*</span> <span class=n>mean_coeff_2</span><span class=p>[</span><span class=n>timestep</span><span class=p>]</span>
</span></span><span class=line><span class=cl>		<span class=n>mean_data</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>mean_data_1</span><span class=p>,</span> <span class=n>mean_data_2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>posterior_data</span> <span class=o>=</span> <span class=n>posterior_variance_corrected</span><span class=p>[</span><span class=n>timestep</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>data_noisy</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>subkeys</span><span class=p>[</span><span class=n>t</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=n>batch_size_generation</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>data_in_batch</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>mean_data</span><span class=p>,</span>  <span class=n>jnp</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>posterior_data</span><span class=p>)</span> <span class=o>*</span> <span class=n>data_noisy</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>datas</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>jax</span><span class=o>.</span><span class=n>device_get</span><span class=p>(</span><span class=n>data_in_batch</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>datas</span><span class=p>,</span> <span class=n>data_in_batch</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=striding-reducing-steps-needed-for-generation>Striding: Reducing steps needed for Generation<a hidden class=anchor aria-hidden=true href=#striding-reducing-steps-needed-for-generation>#</a></h3><p>We could reduce the number of steps needed for generation. One of the popular approaches to do this is by using <em>Time Step Striding</em>.</p><p>Instead of moving stepsone at a time from $T$ to 1, <code>range(T, 1, -1)</code>, we will move $s$ steps at a time, <code>range(T, 1, -s)</code>. Doing this will speed up generation of new samples from the model by s times. I have been able to produce good quality samples with $s$ set to 5.</p><p>We just need to make minor adjustments to the variables we created so that the maths still works.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-14-1><a class=lnlinks href=#hl-14-1> 1</a>
</span><span class=lnt id=hl-14-2><a class=lnlinks href=#hl-14-2> 2</a>
</span><span class=lnt id=hl-14-3><a class=lnlinks href=#hl-14-3> 3</a>
</span><span class=lnt id=hl-14-4><a class=lnlinks href=#hl-14-4> 4</a>
</span><span class=lnt id=hl-14-5><a class=lnlinks href=#hl-14-5> 5</a>
</span><span class=lnt id=hl-14-6><a class=lnlinks href=#hl-14-6> 6</a>
</span><span class=lnt id=hl-14-7><a class=lnlinks href=#hl-14-7> 7</a>
</span><span class=lnt id=hl-14-8><a class=lnlinks href=#hl-14-8> 8</a>
</span><span class=lnt id=hl-14-9><a class=lnlinks href=#hl-14-9> 9</a>
</span><span class=lnt id=hl-14-10><a class=lnlinks href=#hl-14-10>10</a>
</span><span class=lnt id=hl-14-11><a class=lnlinks href=#hl-14-11>11</a>
</span><span class=lnt id=hl-14-12><a class=lnlinks href=#hl-14-12>12</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>strided_schedule</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>timesteps</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span> <span class=o>+</span> <span class=p>[</span><span class=n>timesteps</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>alphas_strided_</span> <span class=o>=</span> <span class=n>alphas_</span><span class=p>[</span><span class=n>strided_schedule</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>alphas_prev_strided_</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>pad</span><span class=p>(</span><span class=n>alphas_strided_</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=s2>&#34;constant&#34;</span><span class=p>,</span> <span class=n>constant_values</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>betas_strided</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>-</span> <span class=p>(</span><span class=n>alphas_strided_</span><span class=o>/</span><span class=n>alphas_prev_strided_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>posterior_variance_new_schedule</span> <span class=o>=</span> <span class=n>betas_strided</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>alphas_prev_strided_</span><span class=p>)</span><span class=o>/</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>alphas_strided_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>log_posterior_variance</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>hstack</span><span class=p>([</span><span class=n>posterior_variance_new_schedule</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>posterior_variance_new_schedule</span><span class=p>[</span><span class=mi>1</span><span class=p>:]]))</span>
</span></span><span class=line><span class=cl><span class=n>posterior_variance_new_schedule_corrected</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>log_posterior_variance</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>mean_coeff_1_strided</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>betas_strided</span><span class=p>)</span><span class=o>*</span><span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>alphas_prev_strided_</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>alphas_strided_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>mean_coeff_2_strided</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>alphas_prev_strided_</span><span class=p>)</span> <span class=o>*</span> <span class=n>betas_strided</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>alphas_strided_</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>Generate Samples:</strong></p><p>The code to generate samples is pretty similar, now we need to use the strided variables defined above.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-15-1><a class=lnlinks href=#hl-15-1> 1</a>
</span><span class=lnt id=hl-15-2><a class=lnlinks href=#hl-15-2> 2</a>
</span><span class=lnt id=hl-15-3><a class=lnlinks href=#hl-15-3> 3</a>
</span><span class=lnt id=hl-15-4><a class=lnlinks href=#hl-15-4> 4</a>
</span><span class=lnt id=hl-15-5><a class=lnlinks href=#hl-15-5> 5</a>
</span><span class=lnt id=hl-15-6><a class=lnlinks href=#hl-15-6> 6</a>
</span><span class=lnt id=hl-15-7><a class=lnlinks href=#hl-15-7> 7</a>
</span><span class=lnt id=hl-15-8><a class=lnlinks href=#hl-15-8> 8</a>
</span><span class=lnt id=hl-15-9><a class=lnlinks href=#hl-15-9> 9</a>
</span><span class=lnt id=hl-15-10><a class=lnlinks href=#hl-15-10>10</a>
</span><span class=lnt id=hl-15-11><a class=lnlinks href=#hl-15-11>11</a>
</span><span class=lnt id=hl-15-12><a class=lnlinks href=#hl-15-12>12</a>
</span><span class=lnt id=hl-15-13><a class=lnlinks href=#hl-15-13>13</a>
</span><span class=lnt id=hl-15-14><a class=lnlinks href=#hl-15-14>14</a>
</span><span class=lnt id=hl-15-15><a class=lnlinks href=#hl-15-15>15</a>
</span><span class=lnt id=hl-15-16><a class=lnlinks href=#hl-15-16>16</a>
</span><span class=lnt id=hl-15-17><a class=lnlinks href=#hl-15-17>17</a>
</span><span class=lnt id=hl-15-18><a class=lnlinks href=#hl-15-18>18</a>
</span><span class=lnt id=hl-15-19><a class=lnlinks href=#hl-15-19>19</a>
</span><span class=lnt id=hl-15-20><a class=lnlinks href=#hl-15-20>20</a>
</span><span class=lnt id=hl-15-21><a class=lnlinks href=#hl-15-21>21</a>
</span><span class=lnt id=hl-15-22><a class=lnlinks href=#hl-15-22>22</a>
</span><span class=lnt id=hl-15-23><a class=lnlinks href=#hl-15-23>23</a>
</span><span class=lnt id=hl-15-24><a class=lnlinks href=#hl-15-24>24</a>
</span><span class=lnt id=hl-15-25><a class=lnlinks href=#hl-15-25>25</a>
</span><span class=lnt id=hl-15-26><a class=lnlinks href=#hl-15-26>26</a>
</span><span class=lnt id=hl-15-27><a class=lnlinks href=#hl-15-27>27</a>
</span><span class=lnt id=hl-15-28><a class=lnlinks href=#hl-15-28>28</a>
</span><span class=lnt id=hl-15-29><a class=lnlinks href=#hl-15-29>29</a>
</span><span class=lnt id=hl-15-30><a class=lnlinks href=#hl-15-30>30</a>
</span><span class=lnt id=hl-15-31><a class=lnlinks href=#hl-15-31>31</a>
</span><span class=lnt id=hl-15-32><a class=lnlinks href=#hl-15-32>32</a>
</span><span class=lnt id=hl-15-33><a class=lnlinks href=#hl-15-33>33</a>
</span><span class=lnt id=hl-15-34><a class=lnlinks href=#hl-15-34>34</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>random</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>generate_data_strided</span><span class=p>(</span><span class=n>avg_params</span><span class=p>,</span> <span class=n>state</span><span class=p>,</span> <span class=n>label</span><span class=p>,</span> <span class=n>energy_method</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>clipped_version</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size_generation</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>label</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>unique_key</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>fold_in</span><span class=p>(</span><span class=n>key</span><span class=p>,</span> <span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>100</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>_</span><span class=p>,</span> <span class=n>subkey</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>unique_key</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>_</span><span class=p>,</span> <span class=o>*</span><span class=n>subkeys</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>unique_key</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>strided_schedule</span><span class=p>)</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>data_noisy</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>subkey</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=n>batch_size_generation</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>          
</span></span><span class=line><span class=cl>    <span class=n>data_in_batch</span> <span class=o>=</span> <span class=n>data_noisy</span>
</span></span><span class=line><span class=cl>	<span class=n>datas</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=n>datas</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>jax</span><span class=o>.</span><span class=n>device_get</span><span class=p>(</span><span class=n>data_noisy</span><span class=p>))</span>                   
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>strided_schedule</span><span class=p>)</span><span class=o>+</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>stride_timestep</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>strided_schedule</span><span class=p>)</span><span class=o>-</span><span class=n>t</span>
</span></span><span class=line><span class=cl>        <span class=n>timestep</span> <span class=o>=</span> <span class=n>strided_schedule</span><span class=p>[</span><span class=n>stride_timestep</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>t_repeated</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>repeat</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=n>timestep</span><span class=p>]),</span> <span class=n>batch_size_generation</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># data_stacked = torch.vstack([data_in_batch, labelled_values])</span>
</span></span><span class=line><span class=cl>        <span class=n>pred_data</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>net</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=n>avg_params</span><span class=p>,</span> <span class=n>state</span><span class=p>,</span> <span class=n>data_in_batch</span><span class=p>,</span> <span class=n>t_repeated</span><span class=p>,</span> <span class=n>label</span><span class=p>,</span> <span class=n>is_training</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=c1># Clipping helps in improving the samples generated as it keeps the random variables in the range of 0 to +1</span>
</span></span><span class=line><span class=cl>		<span class=n>x_reconstructed</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>subtract</span><span class=p>(</span><span class=n>data_in_batch</span><span class=p>,</span> <span class=n>pred_data</span> <span class=o>*</span> <span class=n>sd</span><span class=p>[</span><span class=n>timestep</span><span class=p>])</span><span class=o>/</span> <span class=n>jnp</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>alphas_</span><span class=p>[</span><span class=n>timestep</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=k>if</span> <span class=n>timestep</span> <span class=o>&gt;=</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>			<span class=n>x_reconstructed</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=n>x_reconstructed</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>		<span class=n>mean_data_1</span> <span class=o>=</span> <span class=n>data_in_batch</span> <span class=o>*</span> <span class=n>mean_coeff_1_strided</span><span class=p>[</span><span class=n>stride_timestep</span><span class=p>]</span>
</span></span><span class=line><span class=cl>		<span class=n>mean_data_2</span> <span class=o>=</span> <span class=n>x_reconstructed</span> <span class=o>*</span> <span class=n>mean_coeff_2_strided</span><span class=p>[</span><span class=n>stride_timestep</span><span class=p>]</span>
</span></span><span class=line><span class=cl>		<span class=n>mean_data</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>mean_data_1</span><span class=p>,</span> <span class=n>mean_data_2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>posterior_data</span> <span class=o>=</span> <span class=n>posterior_variance_new_schedule_corrected</span><span class=p>[</span><span class=n>stride_timestep</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>data_noisy</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>subkeys</span><span class=p>[</span><span class=n>t</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=n>batch_size_generation</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>data_in_batch</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>mean_data</span><span class=p>,</span>  <span class=n>jnp</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>posterior_data</span><span class=p>)</span> <span class=o>*</span> <span class=n>data_noisy</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=err>        </span><span class=n>datas</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>jax</span><span class=o>.</span><span class=n>device_get</span><span class=p>(</span><span class=n>data_in_batch</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=err>    </span><span class=k>return</span><span class=err> </span><span class=n>datas</span><span class=p>,</span><span class=err> </span><span class=n>data_in_batch</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=generating-samples--outputs>Generating Samples & Outputs<a hidden class=anchor aria-hidden=true href=#generating-samples--outputs>#</a></h2><p>Let&rsquo;s first create a map between the label values and the characters.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-16-1><a class=lnlinks href=#hl-16-1> 1</a>
</span><span class=lnt id=hl-16-2><a class=lnlinks href=#hl-16-2> 2</a>
</span><span class=lnt id=hl-16-3><a class=lnlinks href=#hl-16-3> 3</a>
</span><span class=lnt id=hl-16-4><a class=lnlinks href=#hl-16-4> 4</a>
</span><span class=lnt id=hl-16-5><a class=lnlinks href=#hl-16-5> 5</a>
</span><span class=lnt id=hl-16-6><a class=lnlinks href=#hl-16-6> 6</a>
</span><span class=lnt id=hl-16-7><a class=lnlinks href=#hl-16-7> 7</a>
</span><span class=lnt id=hl-16-8><a class=lnlinks href=#hl-16-8> 8</a>
</span><span class=lnt id=hl-16-9><a class=lnlinks href=#hl-16-9> 9</a>
</span><span class=lnt id=hl-16-10><a class=lnlinks href=#hl-16-10>10</a>
</span><span class=lnt id=hl-16-11><a class=lnlinks href=#hl-16-11>11</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>string</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>63</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=p>[</span><span class=nb>str</span><span class=p>(</span><span class=n>i</span><span class=p>)</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>10</span><span class=p>)))]</span> <span class=o>+</span> <span class=nb>list</span><span class=p>(</span><span class=n>string</span><span class=o>.</span><span class=n>ascii_uppercase</span> <span class=o>+</span> <span class=n>string</span><span class=o>.</span><span class=n>ascii_lowercase</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>dict_</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>x</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>dict_</span><span class=p>[</span><span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=o>-</span><span class=mi>1</span><span class=p>]]</span> <span class=o>=</span> <span class=n>i</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_label</span><span class=p>(</span><span class=n>ans</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>jnp</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=n>dict_</span><span class=p>[</span><span class=nb>str</span><span class=o>.</span><span class=n>upper</span><span class=p>(</span><span class=n>char</span><span class=p>)]</span> <span class=k>for</span> <span class=n>char</span> <span class=ow>in</span> <span class=n>ans</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>Generating Samples:</strong></p><p>Generating &lsquo;<em><strong>varun</strong></em>&rsquo; using the trained diffusion model.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-17-1><a class=lnlinks href=#hl-17-1>1</a>
</span><span class=lnt id=hl-17-2><a class=lnlinks href=#hl-17-2>2</a>
</span><span class=lnt id=hl-17-3><a class=lnlinks href=#hl-17-3>3</a>
</span><span class=lnt id=hl-17-4><a class=lnlinks href=#hl-17-4>4</a>
</span><span class=lnt id=hl-17-5><a class=lnlinks href=#hl-17-5>5</a>
</span><span class=lnt id=hl-17-6><a class=lnlinks href=#hl-17-6>6</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>datas</span><span class=p>,</span> <span class=n>d</span> <span class=o>=</span> <span class=n>generate_data_strided</span><span class=p>(</span><span class=n>avg_params</span><span class=p>,</span> <span class=n>state</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span> <span class=s2>&#34;varun&#34;</span><span class=p>,</span> <span class=n>energy_method</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>clipped_version</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>datas_</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>datas</span><span class=p>[</span><span class=mi>0</span><span class=p>:</span> <span class=o>-</span><span class=mi>1</span> <span class=p>:</span><span class=mi>2</span><span class=p>]),</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>d_</span> <span class=o>=</span> <span class=n>einops</span><span class=o>.</span><span class=n>rearrange</span><span class=p>(</span><span class=n>datas_</span><span class=p>,</span> <span class=s1>&#39;a b c d e -&gt; (b a) c d e&#39;</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>rawarrview</span><span class=p>(</span><span class=n>reshape_image_batch</span><span class=p>(</span><span class=n>datas</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(),</span> <span class=n>rows</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;bone_r&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>rawarrview</span><span class=p>(</span><span class=n>reshape_image_batch</span><span class=p>(</span><span class=n>d_</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(),</span><span class=err> </span><span class=n>rows</span><span class=o>=</span><span class=mi>5</span><span class=p>),</span><span class=err> </span><span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;bone_r&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p><figure class=align-center><img loading=lazy src=/images/varun.png#center alt="Figure 4: &amp;lsquo;varun&amp;rsquo; generated using strided sampling technique." width=100%><figcaption><p>Figure 4: &lsquo;varun&rsquo; generated using strided sampling technique.</p></figcaption></figure><figure class=align-center><img loading=lazy src=/images/denoising_varun.png#center alt="Figure 5: Visualizing the steps in the diffusion process. If you notice carefully, not much is happening at the early stages." width=100%><figcaption><p>Figure 5: Visualizing the steps in the diffusion process. If you notice carefully, not much is happening at the early stages.</p></figcaption></figure></p><p>Generating &lsquo;<em><strong>tulsian</strong></em>&rsquo; using the trained diffusion model.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-18-1><a class=lnlinks href=#hl-18-1>1</a>
</span><span class=lnt id=hl-18-2><a class=lnlinks href=#hl-18-2>2</a>
</span><span class=lnt id=hl-18-3><a class=lnlinks href=#hl-18-3>3</a>
</span><span class=lnt id=hl-18-4><a class=lnlinks href=#hl-18-4>4</a>
</span><span class=lnt id=hl-18-5><a class=lnlinks href=#hl-18-5>5</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>datas</span><span class=p>,</span> <span class=n>d</span> <span class=o>=</span> <span class=n>generate_data_strided</span><span class=p>(</span><span class=n>avg_params</span><span class=p>,</span> <span class=n>state</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span> <span class=s2>&#34;tulsian&#34;</span><span class=p>,</span> <span class=n>energy_method</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>clipped_version</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>datas_</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>datas</span><span class=p>[</span><span class=mi>0</span><span class=p>:</span> <span class=o>-</span><span class=mi>1</span> <span class=p>:</span><span class=mi>2</span><span class=p>]),</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>d_</span> <span class=o>=</span> <span class=n>einops</span><span class=o>.</span><span class=n>rearrange</span><span class=p>(</span><span class=n>datas_</span><span class=p>,</span> <span class=s1>&#39;a b c d e -&gt; (b a) c d e&#39;</span><span class=p>)</span> 
</span></span><span class=line><span class=cl><span class=n>rawarrview</span><span class=p>(</span><span class=n>reshape_image_batch</span><span class=p>(</span><span class=n>datas</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(),</span> <span class=n>rows</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;bone_r&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>rawarrview</span><span class=p>(</span><span class=n>reshape_image_batch</span><span class=p>(</span><span class=n>d_</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(),</span><span class=err> </span><span class=n>rows</span><span class=o>=</span><span class=mi>7</span><span class=p>),</span><span class=err> </span><span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;bone_r&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p><figure class=align-center><img loading=lazy src=/images/tulsian.png#center alt="Figure 6: &amp;rsquo;tulsian&amp;rsquo; generated using strided sampling technique." width=100%><figcaption><p>Figure 6: &rsquo;tulsian&rsquo; generated using strided sampling technique.</p></figcaption></figure><figure class=align-center><img loading=lazy src=/images/denoising_tulsian.png#center alt="Figure 7: Visualizing the steps in the diffusion process. Similar to earlier, not much is happening at the early stages." width=100%><figcaption><p>Figure 7: Visualizing the steps in the diffusion process. Similar to earlier, not much is happening at the early stages.</p></figcaption></figure></p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Denoising Diffusion models are a powerful algorithmic tool for Generative AI. Although much of the work done so far focusses on images, we could generate any distribution using these techniques.</p><p>I sincerely hope this introduction was useful to you. Please explore additional resources <a href=/posts/diffusion-models/bonus-denoising-diffusion-models-resources/>here.</a></p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Ioffe et al. 2015 <a href=https://arxiv.org/abs/1502.03167 target=_blank>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Nichol et al. 2021 <a href=http://arxiv.org/abs/2102.09672 target=_blank>Improved Denoising Diffusion Probabilistic Models</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Polyak et al. 1991<a href=https://epubs.siam.org/doi/10.1137/0330046 target=_blank>Acceleration of Stochastic Approximation by Averaging</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://varun-ml.github.io/tags/diffusion-model/>diffusion model</a></li><li><a href=https://varun-ml.github.io/tags/generative-ai/>generative ai</a></li><li><a href=https://varun-ml.github.io/tags/jax/>jax</a></li><li><a href=https://varun-ml.github.io/tags/emnist/>emnist</a></li><li><a href=https://varun-ml.github.io/tags/u-net/>u-net</a></li><li><a href=https://varun-ml.github.io/tags/haiku/>haiku</a></li><li><a href=https://varun-ml.github.io/tags/optax/>optax</a></li><li><a href=https://varun-ml.github.io/tags/diffusion-model-series/>diffusion model series</a></li><li><a href=https://varun-ml.github.io/tags/diffusion-model-blog/>diffusion model blog</a></li><li><a href=https://varun-ml.github.io/tags/diffusion-model-tutorial/>diffusion model tutorial</a></li><li><a href=https://varun-ml.github.io/tags/google-colab/>google colab</a></li></ul><nav class=paginav><a class=next href=https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-2/><span class=title>Next »</span><br><span>Denoising Diffusion Models Part 2: Improving Diffusion Models</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Denoising Diffusion Models Part 3: Generating Characters and numbers with Diffusion Models on twitter" href="https://twitter.com/intent/tweet/?text=Denoising%20Diffusion%20Models%20Part%203%3a%20Generating%20Characters%20and%20numbers%20with%20Diffusion%20Models&url=https%3a%2f%2fvarun-ml.github.io%2fposts%2fdiffusion-models%2fdenoising-diffusion-models-3%2f&hashtags=diffusionmodel%2cemnist%2cgenerativeai%2cjax%2cemnist%2cu-net%2chaiku%2coptax%2cdiffusionmodelseries%2cdiffusionmodelblog%2cdiffusionmodeltutorial%2cgooglecolab"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Denoising Diffusion Models Part 3: Generating Characters and numbers with Diffusion Models on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fvarun-ml.github.io%2fposts%2fdiffusion-models%2fdenoising-diffusion-models-3%2f&title=Denoising%20Diffusion%20Models%20Part%203%3a%20Generating%20Characters%20and%20numbers%20with%20Diffusion%20Models&summary=Denoising%20Diffusion%20Models%20Part%203%3a%20Generating%20Characters%20and%20numbers%20with%20Diffusion%20Models&source=https%3a%2f%2fvarun-ml.github.io%2fposts%2fdiffusion-models%2fdenoising-diffusion-models-3%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Denoising Diffusion Models Part 3: Generating Characters and numbers with Diffusion Models on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fvarun-ml.github.io%2fposts%2fdiffusion-models%2fdenoising-diffusion-models-3%2f&title=Denoising%20Diffusion%20Models%20Part%203%3a%20Generating%20Characters%20and%20numbers%20with%20Diffusion%20Models"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Denoising Diffusion Models Part 3: Generating Characters and numbers with Diffusion Models on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fvarun-ml.github.io%2fposts%2fdiffusion-models%2fdenoising-diffusion-models-3%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Denoising Diffusion Models Part 3: Generating Characters and numbers with Diffusion Models on whatsapp" href="https://api.whatsapp.com/send?text=Denoising%20Diffusion%20Models%20Part%203%3a%20Generating%20Characters%20and%20numbers%20with%20Diffusion%20Models%20-%20https%3a%2f%2fvarun-ml.github.io%2fposts%2fdiffusion-models%2fdenoising-diffusion-models-3%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Denoising Diffusion Models Part 3: Generating Characters and numbers with Diffusion Models on telegram" href="https://telegram.me/share/url?text=Denoising%20Diffusion%20Models%20Part%203%3a%20Generating%20Characters%20and%20numbers%20with%20Diffusion%20Models&url=https%3a%2f%2fvarun-ml.github.io%2fposts%2fdiffusion-models%2fdenoising-diffusion-models-3%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://varun-ml.github.io/>wity'ai</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>