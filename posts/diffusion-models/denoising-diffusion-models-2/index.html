<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Denoising Diffusion Models Part 2: Improving Diffusion Models | wity'ai</title><meta name=keywords content="conditioned generation,classifier free guidance,timestep embedding,diffusion model,emnist,generative ai,torch,error estimation,score estimation,tutorial,diffusion model series,diffusion model blog,diffusion model tutorial,google colab"><meta name=description content="Code for this blog post: Notebook Github Link Colab Predicting Error and Score Function Error / Score Prediction Classifier free Guidance and other improvements Advanced concepts Topics to cover We have done most of the heavy-lifting in Part 1 of this series on Diffusion Models. To be able to use them well in practice, we may need to make some more improvements. That&rsquo;s what we will do.
Time step embedding and concatenation/fusion to the input data."><meta name=author content="Varun Tulsian"><link rel=canonical href=https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-2/><meta name=google-site-verification content="G-1VE9P31T88"><link crossorigin=anonymous href=/assets/css/stylesheet.5b8287ef08b591d10eaf101babd7987c2c16890af54b921a596538e838629b23.css integrity="sha256-W4KH7wi1kdEOrxAbq9eYfCwWiQr1S5IaWWU46DhimyM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://varun-ml.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://varun-ml.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://varun-ml.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://varun-ml.github.io/apple-touch-icon.png><link rel=mask-icon href=https://varun-ml.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-2/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1VE9P31T88"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1VE9P31T88",{anonymize_ip:!1})}</script><meta property="og:title" content="Denoising Diffusion Models Part 2: Improving Diffusion Models"><meta property="og:description" content="Code for this blog post: Notebook Github Link Colab Predicting Error and Score Function Error / Score Prediction Classifier free Guidance and other improvements Advanced concepts Topics to cover We have done most of the heavy-lifting in Part 1 of this series on Diffusion Models. To be able to use them well in practice, we may need to make some more improvements. That&rsquo;s what we will do.
Time step embedding and concatenation/fusion to the input data."><meta property="og:type" content="article"><meta property="og:url" content="https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-2/"><meta property="og:image" content="https://varun-ml.github.io/images/denoising-class-conditioned.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-09T17:38:49+05:30"><meta property="article:modified_time" content="2022-12-09T17:38:49+05:30"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://varun-ml.github.io/images/denoising-class-conditioned.png"><meta name=twitter:title content="Denoising Diffusion Models Part 2: Improving Diffusion Models"><meta name=twitter:description content="Code for this blog post: Notebook Github Link Colab Predicting Error and Score Function Error / Score Prediction Classifier free Guidance and other improvements Advanced concepts Topics to cover We have done most of the heavy-lifting in Part 1 of this series on Diffusion Models. To be able to use them well in practice, we may need to make some more improvements. That&rsquo;s what we will do.
Time step embedding and concatenation/fusion to the input data."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://varun-ml.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Denoising Diffusion Models Part 2: Improving Diffusion Models","item":"https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Denoising Diffusion Models Part 2: Improving Diffusion Models","name":"Denoising Diffusion Models Part 2: Improving Diffusion Models","description":"Code for this blog post: Notebook Github Link Colab Predicting Error and Score Function Error / Score Prediction Classifier free Guidance and other improvements Advanced concepts Topics to cover We have done most of the heavy-lifting in Part 1 of this series on Diffusion Models. To be able to use them well in practice, we may need to make some more improvements. That\u0026rsquo;s what we will do.\nTime step embedding and concatenation/fusion to the input data.","keywords":["conditioned generation","classifier free guidance","timestep embedding","diffusion model","emnist","generative ai","torch","error estimation","score estimation","tutorial","diffusion model series","diffusion model blog","diffusion model tutorial","google colab"],"articleBody":"Code for this blog post: Notebook Github Link Colab Predicting Error and Score Function Error / Score Prediction Classifier free Guidance and other improvements Advanced concepts Topics to cover We have done most of the heavy-lifting in Part 1 of this series on Diffusion Models. To be able to use them well in practice, we may need to make some more improvements. That’s what we will do.\nTime step embedding and concatenation/fusion to the input data. Error Prediction $\\hat \\epsilon_0^\\ast$ and Score Function Prediction $s$ instead of predicting the actual input $x_0$. Class conditioned generation or Classifier free guidance, where we guide the diffusion model to generate data based on class labels. The ideas are an extension to the concepts introduced earlier, but are vital parts of any practical diffusion model implementation.\nTime Step Embedding During the denoising process, the Neural Network needs to know the time step at which denoising is being done. Passing the time step $t$ as a scalar value is not ideal. Rather, it would be preferable to pass the time step as an embedding to the Neural Network. In the Attention is all you need1 paper, the authors proposed sinusoidal embeddings to encode position of the tokens (time steps in our case).\nPseudocode:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import math # adapted from HuggingFace -- https://huggingface.co/blog/annotated-diffusion class SinusoidalPositionEmbeddings(nn.Module): def __init__(self, dim): super().__init__() self.dim = dim def forward(self, time): half_dim = self.dim // 2 # embedding values need to be small embeddings = math.log(10000) / (half_dim - 1) embeddings = torch.exp(torch.arange(half_dim) * -embeddings) embeddings = time[:, None] * embeddings[None, :] embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1) return embeddings # generate 8 dimensional time step embeddings sinusoidalPositionEmbeddings = SinusoidalPositionEmbeddings(8) Sinusoidal embeddings are small in absolute value and can be fused(added) or concatenated to the input data to provide the Neural Network some information about the time step at which the denoising process is happening.\nPassing Time Step Embedding (Fusing and Concatenation):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # MLP to project time step embedding before we pass it to the input self.position_mlp = nn.Sequential( nn.GELU(), nn.Linear(8, 2) # 2 is the input data dimension ) timestep_embeddings = position_embeddings[timestep.long()] time_embeddings = self.position_mlp(timestep_embeddings) # concatenation # x = (batch_size, input data dimension); time_embeddings = (batch_size, input data dimension) concat_x = torch.cat((x, time_embeddings), dim=1) # fusing shift, scale = jnp.split(time_embedding, indices_or_sections=2, axis=-1) x = shift + (scale+1)*x Fusing the data with the data is computationally efficient as it requires significantly fewer number of weight parameters. The key idea here is that Neural Networks are powerful enough to separate any added information to the original data without the information being explicitly passed. Have a look at this video if you want to get better intuition. Positional embeddings in transformers EXPLAINED.\nIf we want to add time step information to an image, it is typically done by broadcasting the time step embedding along the channel dimension.\nAdding Time Step to Image (Fusing):\n1 2 3 4 5 6 7 8 9 # image batch data -- (batch_size, height, width, channels) # timestep embedding -- (batch_size, embedding_dimension) # ensure channels == embedding_dimension # broadcast and add timestep to image data import einops timestep_embedding = einops.rearrange(b, 'b c -\u003e b 1 1 c') fused_image_data = image_data.add(timestep_embedding) Fusing data to an input is a technique useful to add any kind of conditional information to the data. We will use this idea in EMNIST: Blog 3 where we use this concept to fuse label and time step information with the images during class conditional generation. This technique was used in the work, Improved Denoising Diffusion Probabilistic Models.\nError Prediction and Score Function Prediction Predicting the Error $\\hat \\epsilon_0^\\ast$ During the denoising step, the model ingests $\\hat x_t$ and spits out $\\hat x_t^0$, a prediction for $x_0$. Instead of outputting predictions of the input data, we want the model to output a prediction of the error $\\hat\\epsilon^\\ast_0$.\nLet’s investigate the relationship between error $\\epsilon_0^\\ast$ and input data $x_0$. $$ \\begin{align} x_t \u0026= \\sqrt{\\bar\\alpha_t}x_0 + \\sqrt{(1 - \\bar\\alpha_t )}\\ast\\epsilon_0^\\ast \\cr x_0 \u0026= \\frac{x_t-\\sqrt{(1 - \\bar\\alpha_t )}\\ast\\epsilon_0^\\ast}{\\sqrt{\\bar\\alpha_t}} \\end{align} $$ Equation 2 shows that if we have $x_t$ and $\\epsilon^\\ast_0$ than it determines $x_0$.\nThe denoising step now looks like this:\nMake a prediction of source error $\\hat\\epsilon^\\ast_0$ using the $NN(\\hat x_t, t)$. Using Equation 2, evaluate $x_0^t$, which is the prediction of the input data at time step $t$. Next step: During training: We want an equivalent version of the loss function $Error\\_Loss(\\epsilon^\\ast_0, \\hat \\epsilon_0^\\ast, t)$. $$ \\begin{align} Loss(x_0, \\hat x_t, t) = 1/2\\ast(\\frac{\\bar\\alpha_{t-1}}{1-\\bar\\alpha_{t-1}} - \\frac{\\bar\\alpha_t}{1-\\bar\\alpha_t})\\ast\\mid\\mid x_0-\\hat x_0^t\\mid\\mid_2^2 \\cr \\text{Making modifications to loss using Equation 2} \\nonumber \\cr Error\\_Loss(\\epsilon^\\ast_0, \\hat \\epsilon_0^\\ast, t) = \\frac{1}{2\\sigma^2_q(t)} \\frac{(1-\\alpha_t)^2}{(1-\\bar\\alpha_t)\\alpha_t}\\mid\\mid \\epsilon^\\ast_0-\\hat \\epsilon^\\ast_0\\mid\\mid_2^2 \\end{align} $$ During data generation: Using Equation 2, reconstruct $\\hat x_0^t$. Remember, $x_0$ is a function of the Gaussian error and the latent variable. Clip the $\\hat x_0^t$ to make sure it lies in the range of -1 to +1 (normalized range for input data). torch.clip(x_reconstructed, -1, 1) Using Equation 5 (below) get a prediction for the latent at $t-1$ time step. $$ \\begin{align} p(\\hat x_{t-1}| \\hat x_t) \\varpropto N(\\hat x_{t-1};\\frac{\\sqrt\\alpha_t(1-\\bar\\alpha_{t-1})\\hat x_t + \\sqrt{\\bar\\alpha_{t-1}}(1-\\alpha_t)\\hat x_t^0}{1-\\bar\\alpha_t}, \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}I)\t\\end{align} $$ Predicting error is empirically shown to work well, refer Denoising paper2. This is probably due to the clipping of the predicted output at every time step so that the predicted data is in the normalized range. Refer this discussion.\nHere is an interesting discussion on why predicting error works for images, but may not work well for other domains such as voice generation.\nAnother important reason is that the standard formulation everyone has ended up using for images (predict the standardised noise given noisy input) implicitly downweights high frequency components, which is an excellent match for the human visual system.\n— Sander Dieleman (@sedielem) August 11, 2022 Denoising Step during data generation:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 x_reconstructed = # data_in_batch is latent variable x at timestep data_in_batch.T.sub(pred_data.mul(sd[timestep])).div(torch.sqrt(alphas_[timestep])) if timestep \u003e= 5: x_reconstructed = torch.clip(x_reconstructed, -1., 1.) mean_data_1 = data_in_batch.T.mul(mean_coeff_1[timestep]) mean_data_2 = x_reconstructed.mul(mean_coeff_2[timestep]) mean_data = mean_data_1.add(mean_data_2) posterior_data = posterior_variance_corrected[timestep] # data_in_batch is latent variable x at previous timestep data_in_batch = torch.normal(mean_data, torch.sqrt(posterior_data)).T Score Function Prediction This is yet another variation on diffusion models. Similar to predicting the error as discussed earlier, we could also predict the score function. The score function $s$ is defined as $\\nabla p(x_t)$.\nAll we need is to define $x_t$ as $f(s)$. We could then define a $Score\\_Loss(s, \\hat s, t)$ by substituting $x_t$ for the function $f(s)$ in the training step. In the denoising step we will use the function $f(s)$ defined to compute $x_t$, perform clipping so that the output lies in the normalized range and proceed as we did in the earlier section.\nThe relation between $s$ and $x$ is defined as below: $$ x_0 = \\frac{x_t + (1 - \\bar\\alpha_t )\\ast\\nabla p(x_t)}{\\sqrt{\\bar\\alpha_t}} $$ For more details and intuition on why this is an important interpretation, please refer to the section on Three equivalent interpretations.3 Yang Song has an excellent blog post on Score based generative models.\nGuidance \u0026 Classifier-free Guidance We want to control the data we generate. For example, in the image below, we have 2 labels. During generation, we want to direct the model to generate samples either from Yellow or Purple classes. Classifier Guidance is a way to do this. We will be guiding the denoising process to generate samples that are more likely to belong to the conditioned class.\nFigure 1: Data with 2 labels, Circles in Purple and Moons in Yellow\nText-guided diffusion models like Glide use the powerful Neural Network called CLIP and classifier guidance techniques to perform text-based image generation.\nSo far, we have been trying to maximize the likelihood of the data distribution $p(x)$ with diffusion models. This allowed us to randomly sample data points from the data distribution.\nA naïve idea to do class-conditioned generation could be to fuse the conditional label information with the input data.\nPseudocode for naïve idea:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def diffusion(x_0, t): code to add noise to x_0 return x_i def training(): for loop until convergence: pick an image x_0 from X (batch of images) sample t from 1 to T x_t = diffusion_step(x_0, t) x_hat_t = NN(x_t, t, y) # y is the label loss_ = loss(x_0, x_hat_t) update(NN, grad(loss_) # generating new data points through denoising steps def generate_new_data(): sample x_T from N(0, I) for t in range(T, 1): # denoising step x_hat_t = NN(x_t, t, y) # y is the desired label # get x_{t-1} x_{t-1} = func(x_hat_t, t) # refer EQ - 12 x_hat_0 = x_0 During training and data generation, we will add the conditioned label as an input along with the noisy data and the time step embedding. The output of the model will stay the same as earlier.\nThe conditioned data can be fused with the input data, just like we fused the time step information.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # haiku code self.mlp = hk.Sequential([ hk.Linear(256), jax.nn.gelu, hk.Linear(256), ]) # conditional vectors encoding self.embedding_vectors = hk.Embed(10+26+26+1, 64) self.timestep_embeddings = TimeEmbeddings(64) # the diffusion model is given x, time step and label as input def __call__(self, x, timesteps, cond=None): cond_embedding = None conditioning = None if timesteps is not None: timestep_embeddings = self.timestep_embeddings(timesteps) conditioning = timestep_embeddings if cond is not None: label_embeddings = self.embedding_vectors(cond) conditioning = jnp.concatenate([label_embeddings, conditioning], axis=1) cond_embedding = self.mlp(conditioning) ... # fusing time step and label information with input x shift, scale = jnp.split(cond_embedding, indices_or_sections=2, axis=-1) x = shift + (scale+1)*x ... Guidance The above approach may lead to models that have low sample diversity. Researchers have proposed two other forms of guidance: classifier guidance and classifier-free guidance4.\nClassifier Guidance guides the generation of new samples with the help of a classifier. The classifier takes a noisy image ($x_t$) as input and predicts the label $y$. The gradient of the distribution $p(y|x)$ is used to make updates to the weights of the Neural Network to guide it to produce samples that are likely to be $y$. This is an adversarial loss, and this approach has similarities to GANs.\nClassifier-Free Guidance would be ideal if we did not want to build a classifier. The classifier-free guidance approach models the conditional likelihood of samples as follows: $$ \\nabla p(x|y) = \\lambda \\ast \\underbrace{\\nabla p(x|y)}_{\\text{conditional}} + (1-\\lambda) \\ast \\underbrace{\\nabla p(x)}_{\\text{unconditional}} $$ The conditional and the unconditional distributions are modelled by the same neural network. To model the conditional distribution, we fuse the label information as shown in the naïve approach above. To model the unconditional distribution, we mask the label information and pass it to the diffusion model. The lambda parameter, controls the diversity of the sample we want to generate. $\\lambda=1$ would be equivalent to the naïve approach.\nClassifier-free guidance is a cheatcode that makes these models perform as if they had 10x the parameters. At least in terms of sample quality, and at the cost of diversity. All of the recent spectacular results rely heavily on this trick.\n— Sander Dieleman (@sedielem) August 11, 2022 Adding this perspective about efficiency of diffusion models. Even though to be honest, I am not sure myself :).\nLet’s look at some outputs Figure 2: A GIF show-casing the denoising process; Generating class conditioned samples over T time steps\nSee you in the next part.\nVaswani et al. 2017 “Attention is all you need” ↩︎\nHo et al. 2020 “Denoising diffusion probabilistic models” ↩︎\nCalvin Luo; 2019 “Understanding Diffusion Models: A Unified Perspective” ↩︎\nHo et al. 2021 “Classifier-free diffusion guidance” ↩︎\n","wordCount":"1987","inLanguage":"en","image":"https://varun-ml.github.io/images/denoising-class-conditioned.png","datePublished":"2022-12-09T17:38:49+05:30","dateModified":"2022-12-09T17:38:49+05:30","author":{"@type":"Person","name":"Varun Tulsian"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-2/"},"publisher":{"@type":"Organization","name":"wity'ai","logo":{"@type":"ImageObject","url":"https://varun-ml.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://varun-ml.github.io/ accesskey=h title="wity'ai (Alt + H)">wity'ai</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://varun-ml.github.io/posts title=Posts><span>Posts</span></a></li><li><a href=https://varun-ml.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://varun-ml.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://varun-ml.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://varun-ml.github.io/resume.pdf title=Resume><span>Resume</span></a></li><li><a href=https://varun-ml.github.io/faq title=Questions><span>Questions</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://varun-ml.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://varun-ml.github.io/posts/>Posts</a></div><h1 class=post-title>Denoising Diffusion Models Part 2: Improving Diffusion Models</h1><div class=post-meta><span title='2022-12-09 17:38:49 +0530 IST'>December 9, 2022</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;1987 words&nbsp;·&nbsp;Varun Tulsian</div></header><figure class=entry-cover><img loading=lazy src=https://varun-ml.github.io/images/denoising-class-conditioned.png alt="Colab tutorial for class conditioned diffusion models"><p>class conditional diffusion models</p></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#code-for-this-blog-post>Code for this blog post:</a></li><li><a href=#topics-to-cover>Topics to cover</a></li><li><a href=#time-step-embedding>Time Step Embedding</a></li><li><a href=#error-prediction-and-score-function-prediction>Error Prediction and Score Function Prediction</a><ul><li><a href=#predicting-the-error-hat-epsilon_0ast>Predicting the Error $\hat \epsilon_0^\ast$</a></li><li><a href=#score-function-prediction>Score Function Prediction</a></li></ul></li><li><a href=#guidance--classifier-free-guidance>Guidance & Classifier-free Guidance</a><ul><li><a href=#guidance>Guidance</a></li></ul></li><li><a href=#lets-look-at-some-outputs>Let&rsquo;s look at some outputs</a></li></ul></nav></div></details></div><div class=post-content><h2 id=code-for-this-blog-post>Code for this blog post:<a hidden class=anchor aria-hidden=true href=#code-for-this-blog-post>#</a></h2><table><thead><tr><th style=text-align:left>Notebook</th><th style=text-align:left>Github Link</th><th style=text-align:left>Colab</th></tr></thead><tbody><tr><td style=text-align:left>Predicting Error and Score Function</td><td style=text-align:left><a href=https://github.com/varun-ml/diffusion-models-tutorial/blob/master/toy-density-estimation/errors.diffusion_model_interpretations.ipynb target=_blank>Error / Score Prediction</a></td><td style=text-align:left><a href=https://colab.research.google.com/github/varun-ml/diffusion-models-tutorial/blob/master/toy-density-estimation/colab_errors.diffusion_model_interpretations.ipynb target=_blank><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Colab (Large)"></a></td></tr><tr><td style=text-align:left>Classifier free Guidance and other improvements</td><td style=text-align:left><a href=https://github.com/varun-ml/diffusion-models-tutorial/blob/master/toy-density-estimation/guidance_free_classifiers.ipynb target=_blank>Advanced concepts</a></td><td style=text-align:left><a href=https://colab.research.google.com/github/varun-ml/diffusion-models-tutorial/blob/master/toy-density-estimation/colab_guidance_free_classifiers.ipynb target=_blank><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Colab (Large)"></a></td></tr></tbody></table><h2 id=topics-to-cover>Topics to cover<a hidden class=anchor aria-hidden=true href=#topics-to-cover>#</a></h2><p>We have done most of the heavy-lifting in <a href=/posts/diffusion-models/denoising-diffusion-models-1/>Part 1</a> of this series on Diffusion Models. To be able to use them well in practice, we may need to make some more improvements. That&rsquo;s what we will do.</p><ol><li><strong>Time step embedding</strong> and concatenation/fusion to the input data.</li><li><strong>Error Prediction $\hat \epsilon_0^\ast$ and Score Function Prediction $s$</strong> instead of predicting the actual input $x_0$.</li><li><strong>Class conditioned generation or Classifier free guidance</strong>, where we guide the diffusion model to generate data based on class labels.</li></ol><p>The ideas are an extension to the concepts introduced earlier, but are vital parts of any practical diffusion model implementation.</p><h2 id=time-step-embedding>Time Step Embedding<a hidden class=anchor aria-hidden=true href=#time-step-embedding>#</a></h2><p>During the denoising process, the Neural Network needs to know the time step at which denoising is being done. Passing the time step $t$ as a scalar value is not ideal. Rather, it would be preferable to
pass the time step as an embedding to the Neural Network. In the <cite>Attention is all you need<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></cite> paper, the authors proposed sinusoidal embeddings to encode position of the tokens (time steps in our case).</p><p><strong>Pseudocode:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-0-1><a class=lnlinks href=#hl-0-1> 1</a>
</span><span class=lnt id=hl-0-2><a class=lnlinks href=#hl-0-2> 2</a>
</span><span class=lnt id=hl-0-3><a class=lnlinks href=#hl-0-3> 3</a>
</span><span class=lnt id=hl-0-4><a class=lnlinks href=#hl-0-4> 4</a>
</span><span class=lnt id=hl-0-5><a class=lnlinks href=#hl-0-5> 5</a>
</span><span class=lnt id=hl-0-6><a class=lnlinks href=#hl-0-6> 6</a>
</span><span class=lnt id=hl-0-7><a class=lnlinks href=#hl-0-7> 7</a>
</span><span class=lnt id=hl-0-8><a class=lnlinks href=#hl-0-8> 8</a>
</span><span class=lnt id=hl-0-9><a class=lnlinks href=#hl-0-9> 9</a>
</span><span class=lnt id=hl-0-10><a class=lnlinks href=#hl-0-10>10</a>
</span><span class=lnt id=hl-0-11><a class=lnlinks href=#hl-0-11>11</a>
</span><span class=lnt id=hl-0-12><a class=lnlinks href=#hl-0-12>12</a>
</span><span class=lnt id=hl-0-13><a class=lnlinks href=#hl-0-13>13</a>
</span><span class=lnt id=hl-0-14><a class=lnlinks href=#hl-0-14>14</a>
</span><span class=lnt id=hl-0-15><a class=lnlinks href=#hl-0-15>15</a>
</span><span class=lnt id=hl-0-16><a class=lnlinks href=#hl-0-16>16</a>
</span><span class=lnt id=hl-0-17><a class=lnlinks href=#hl-0-17>17</a>
</span><span class=lnt id=hl-0-18><a class=lnlinks href=#hl-0-18>18</a>
</span><span class=lnt id=hl-0-19><a class=lnlinks href=#hl-0-19>19</a>
</span><span class=lnt id=hl-0-20><a class=lnlinks href=#hl-0-20>20</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl><span class=c1># adapted from HuggingFace -- https://huggingface.co/blog/annotated-diffusion</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SinusoidalPositionEmbeddings</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dim</span> <span class=o>=</span> <span class=n>dim</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>time</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>half_dim</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dim</span> <span class=o>//</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>        <span class=c1># embedding values need to be small</span>
</span></span><span class=line><span class=cl>        <span class=n>embeddings</span> <span class=o>=</span> <span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mi>10000</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>half_dim</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>embeddings</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>half_dim</span><span class=p>)</span> <span class=o>*</span> <span class=o>-</span><span class=n>embeddings</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>embeddings</span> <span class=o>=</span> <span class=n>time</span><span class=p>[:,</span> <span class=kc>None</span><span class=p>]</span> <span class=o>*</span> <span class=n>embeddings</span><span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>        <span class=n>embeddings</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>embeddings</span><span class=o>.</span><span class=n>sin</span><span class=p>(),</span> <span class=n>embeddings</span><span class=o>.</span><span class=n>cos</span><span class=p>()),</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>embeddings</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl><span class=c1># generate 8 dimensional time step embeddings</span>
</span></span><span class=line><span class=cl><span class=n>sinusoidalPositionEmbeddings</span> <span class=o>=</span> <span class=n>SinusoidalPositionEmbeddings</span><span class=p>(</span><span class=mi>8</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Sinusoidal embeddings are small in absolute value and can be fused(added) or concatenated to the input data to provide the Neural Network some information about the time step at which the denoising process is happening.</p><p><strong>Passing Time Step Embedding (Fusing and Concatenation):</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-1-1><a class=lnlinks href=#hl-1-1> 1</a>
</span><span class=lnt id=hl-1-2><a class=lnlinks href=#hl-1-2> 2</a>
</span><span class=lnt id=hl-1-3><a class=lnlinks href=#hl-1-3> 3</a>
</span><span class=lnt id=hl-1-4><a class=lnlinks href=#hl-1-4> 4</a>
</span><span class=lnt id=hl-1-5><a class=lnlinks href=#hl-1-5> 5</a>
</span><span class=lnt id=hl-1-6><a class=lnlinks href=#hl-1-6> 6</a>
</span><span class=lnt id=hl-1-7><a class=lnlinks href=#hl-1-7> 7</a>
</span><span class=lnt id=hl-1-8><a class=lnlinks href=#hl-1-8> 8</a>
</span><span class=lnt id=hl-1-9><a class=lnlinks href=#hl-1-9> 9</a>
</span><span class=lnt id=hl-1-10><a class=lnlinks href=#hl-1-10>10</a>
</span><span class=lnt id=hl-1-11><a class=lnlinks href=#hl-1-11>11</a>
</span><span class=lnt id=hl-1-12><a class=lnlinks href=#hl-1-12>12</a>
</span><span class=lnt id=hl-1-13><a class=lnlinks href=#hl-1-13>13</a>
</span><span class=lnt id=hl-1-14><a class=lnlinks href=#hl-1-14>14</a>
</span><span class=lnt id=hl-1-15><a class=lnlinks href=#hl-1-15>15</a>
</span><span class=lnt id=hl-1-16><a class=lnlinks href=#hl-1-16>16</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># MLP to project time step embedding before we pass it to the input</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>position_mlp</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>          <span class=n>nn</span><span class=o>.</span><span class=n>GELU</span><span class=p>(),</span> 
</span></span><span class=line><span class=cl>          <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> <span class=c1># 2 is the input data dimension</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>timestep_embeddings</span> <span class=o>=</span> <span class=n>position_embeddings</span><span class=p>[</span><span class=n>timestep</span><span class=o>.</span><span class=n>long</span><span class=p>()]</span>
</span></span><span class=line><span class=cl><span class=n>time_embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>position_mlp</span><span class=p>(</span><span class=n>timestep_embeddings</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># concatenation</span>
</span></span><span class=line><span class=cl><span class=c1># x = (batch_size, input data dimension); time_embeddings = (batch_size, input data dimension)</span>
</span></span><span class=line><span class=cl><span class=n>concat_x</span><span class=err> </span><span class=o>=</span><span class=err> </span><span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>x</span><span class=p>,</span> <span class=n>time_embeddings</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># fusing</span>
</span></span><span class=line><span class=cl><span class=n>shift</span><span class=p>,</span> <span class=n>scale</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>time_embedding</span><span class=p>,</span> <span class=n>indices_or_sections</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>shift</span> <span class=o>+</span> <span class=p>(</span><span class=n>scale</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span><span class=o>*</span><span class=n>x</span>
</span></span></code></pre></td></tr></table></div></div><p>Fusing the data with the data is computationally efficient as it requires significantly fewer number of weight parameters. The key idea here is that Neural Networks are powerful enough to separate any added information to the original data without the information being explicitly passed. Have a look at this video if you want to get better intuition. <a href="https://www.youtube.com/watch?v=1biZfFLPRSY" target=_blank>Positional embeddings in transformers EXPLAINED</a>.</p><p>If we want to add time step information to an image, it is typically done by broadcasting the time step embedding along the channel dimension.</p><p><strong>Adding Time Step to Image (Fusing):</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-2-1><a class=lnlinks href=#hl-2-1>1</a>
</span><span class=lnt id=hl-2-2><a class=lnlinks href=#hl-2-2>2</a>
</span><span class=lnt id=hl-2-3><a class=lnlinks href=#hl-2-3>3</a>
</span><span class=lnt id=hl-2-4><a class=lnlinks href=#hl-2-4>4</a>
</span><span class=lnt id=hl-2-5><a class=lnlinks href=#hl-2-5>5</a>
</span><span class=lnt id=hl-2-6><a class=lnlinks href=#hl-2-6>6</a>
</span><span class=lnt id=hl-2-7><a class=lnlinks href=#hl-2-7>7</a>
</span><span class=lnt id=hl-2-8><a class=lnlinks href=#hl-2-8>8</a>
</span><span class=lnt id=hl-2-9><a class=lnlinks href=#hl-2-9>9</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># image batch data -- (batch_size, height, width, channels)</span>
</span></span><span class=line><span class=cl><span class=c1># timestep embedding -- (batch_size, embedding_dimension)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ensure channels == embedding_dimension</span>
</span></span><span class=line><span class=cl><span class=c1># broadcast and add timestep to image data</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>einops</span>
</span></span><span class=line><span class=cl><span class=n>timestep_embedding</span><span class=err> </span><span class=o>=</span><span class=err> </span><span class=n>einops</span><span class=o>.</span><span class=n>rearrange</span><span class=p>(</span><span class=n>b</span><span class=p>,</span><span class=err> </span><span class=s1>&#39;b c -&gt; b 1 1 c&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>fused_image_data</span> <span class=o>=</span> <span class=n>image_data</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>timestep_embedding</span><span class=p>)</span> 
</span></span></code></pre></td></tr></table></div></div><p>Fusing data to an input is a technique useful to add any kind of conditional information to the data. We will use this idea in <a href=/posts/diffusion-models/denoising-diffusion-models-3/>EMNIST: Blog 3</a> where we use this concept to fuse label and time step information with the images during class conditional generation. This technique was used in the work, <a href=https://github.com/openai/improved-diffusion target=_blank>Improved Denoising Diffusion Probabilistic Models</a>.</p><h2 id=error-prediction-and-score-function-prediction>Error Prediction and Score Function Prediction<a hidden class=anchor aria-hidden=true href=#error-prediction-and-score-function-prediction>#</a></h2><h3 id=predicting-the-error-hat-epsilon_0ast>Predicting the Error $\hat \epsilon_0^\ast$<a hidden class=anchor aria-hidden=true href=#predicting-the-error-hat-epsilon_0ast>#</a></h3><p><strong>During the denoising step</strong>, the model ingests $\hat x_t$ and spits out $\hat x_t^0$, a prediction for $x_0$. Instead of outputting predictions of the input data, we want the model to output a prediction of the error $\hat\epsilon^\ast_0$.</p><p>Let&rsquo;s investigate the relationship between error $\epsilon_0^\ast$ and input data $x_0$.
$$
\begin{align}
x_t &= \sqrt{\bar\alpha_t}x_0 + \sqrt{(1 - \bar\alpha_t )}\ast\epsilon_0^\ast \cr
x_0 &= \frac{x_t-\sqrt{(1 - \bar\alpha_t )}\ast\epsilon_0^\ast}{\sqrt{\bar\alpha_t}}
\end{align}
$$
Equation 2 shows that if we have $x_t$ and $\epsilon^\ast_0$ than it determines $x_0$.</p><p>The denoising step now looks like this:</p><ol><li>Make a prediction of source error $\hat\epsilon^\ast_0$ using the $NN(\hat x_t, t)$.</li><li>Using Equation 2, evaluate $x_0^t$, which is the prediction of the input data at time step $t$.</li><li>Next step:<ul><li>During training: We want an equivalent version of the loss function $Error\_Loss(\epsilon^\ast_0, \hat \epsilon_0^\ast, t)$.
$$
\begin{align}
Loss(x_0, \hat x_t, t) = 1/2\ast(\frac{\bar\alpha_{t-1}}{1-\bar\alpha_{t-1}} - \frac{\bar\alpha_t}{1-\bar\alpha_t})\ast\mid\mid x_0-\hat x_0^t\mid\mid_2^2 \cr
\text{Making modifications to loss using Equation 2} \nonumber \cr
Error\_Loss(\epsilon^\ast_0, \hat \epsilon_0^\ast, t) = \frac{1}{2\sigma^2_q(t)} \frac{(1-\alpha_t)^2}{(1-\bar\alpha_t)\alpha_t}\mid\mid \epsilon^\ast_0-\hat \epsilon^\ast_0\mid\mid_2^2
\end{align}
$$</li><li>During data generation:<ol><li>Using Equation 2, reconstruct $\hat x_0^t$. Remember, $x_0$ is a function of the Gaussian error and the latent variable.</li><li><strong>Clip the $\hat x_0^t$ to make sure it lies in the range of -1 to +1 (normalized range for input data)</strong>. <code>torch.clip(x_reconstructed, -1, 1)</code></li><li>Using Equation 5 (below) get a prediction for the latent at $t-1$ time step.</li></ol></li></ul></li></ol>$$
\begin{align}
p(\hat x_{t-1}| \hat x_t) \varpropto N(\hat x_{t-1};\frac{\sqrt\alpha_t(1-\bar\alpha_{t-1})\hat x_t + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)\hat x_t^0}{1-\bar\alpha_t}, \frac{(1-\alpha_t)(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}I)
\end{align}
$$<p>Predicting error is empirically shown to work well, refer <cite>Denoising paper<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></cite>. This is probably due to the clipping of the predicted output at every time step so that the predicted data is in the normalized range. <a href=https://github.com/hojonathanho/diffusion/issues/5#issuecomment-1283946094 target=_blank>Refer this discussion</a>.</p><p>Here is an interesting discussion on why predicting error works for images, but may not work well for other domains such as voice generation.</p><blockquote class=twitter-tweet data-dnt=true><p lang=en dir=ltr>Another important reason is that the standard formulation everyone has ended up using for images (predict the standardised noise given noisy input) implicitly downweights high frequency components, which is an excellent match for the human visual system.</p>&mdash; Sander Dieleman (@sedielem) <a href="https://twitter.com/sedielem/status/1557692621199400960?ref_src=twsrc%5Etfw">August 11, 2022</a></blockquote><p><strong>Denoising Step during data generation:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-3-1><a class=lnlinks href=#hl-3-1> 1</a>
</span><span class=lnt id=hl-3-2><a class=lnlinks href=#hl-3-2> 2</a>
</span><span class=lnt id=hl-3-3><a class=lnlinks href=#hl-3-3> 3</a>
</span><span class=lnt id=hl-3-4><a class=lnlinks href=#hl-3-4> 4</a>
</span><span class=lnt id=hl-3-5><a class=lnlinks href=#hl-3-5> 5</a>
</span><span class=lnt id=hl-3-6><a class=lnlinks href=#hl-3-6> 6</a>
</span><span class=lnt id=hl-3-7><a class=lnlinks href=#hl-3-7> 7</a>
</span><span class=lnt id=hl-3-8><a class=lnlinks href=#hl-3-8> 8</a>
</span><span class=lnt id=hl-3-9><a class=lnlinks href=#hl-3-9> 9</a>
</span><span class=lnt id=hl-3-10><a class=lnlinks href=#hl-3-10>10</a>
</span><span class=lnt id=hl-3-11><a class=lnlinks href=#hl-3-11>11</a>
</span><span class=lnt id=hl-3-12><a class=lnlinks href=#hl-3-12>12</a>
</span><span class=lnt id=hl-3-13><a class=lnlinks href=#hl-3-13>13</a>
</span><span class=lnt id=hl-3-14><a class=lnlinks href=#hl-3-14>14</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=n>x_reconstructed</span> <span class=o>=</span> 
</span></span><span class=line><span class=cl><span class=c1># data_in_batch is latent variable x at timestep</span>
</span></span><span class=line><span class=cl><span class=n>data_in_batch</span><span class=o>.</span><span class=n>T</span><span class=o>.</span><span class=n>sub</span><span class=p>(</span><span class=n>pred_data</span><span class=o>.</span><span class=n>mul</span><span class=p>(</span><span class=n>sd</span><span class=p>[</span><span class=n>timestep</span><span class=p>]))</span><span class=o>.</span><span class=n>div</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>alphas_</span><span class=p>[</span><span class=n>timestep</span><span class=p>]))</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>timestep</span> <span class=o>&gt;=</span> <span class=mi>5</span><span class=p>:</span>
</span></span><span class=line><span class=cl>	<span class=n>x_reconstructed</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=n>x_reconstructed</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>mean_data_1</span> <span class=o>=</span> <span class=n>data_in_batch</span><span class=o>.</span><span class=n>T</span><span class=o>.</span><span class=n>mul</span><span class=p>(</span><span class=n>mean_coeff_1</span><span class=p>[</span><span class=n>timestep</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>mean_data_2</span> <span class=o>=</span> <span class=n>x_reconstructed</span><span class=o>.</span><span class=n>mul</span><span class=p>(</span><span class=n>mean_coeff_2</span><span class=p>[</span><span class=n>timestep</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>mean_data</span> <span class=o>=</span> <span class=n>mean_data_1</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>mean_data_2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl><span class=n>posterior_data</span> <span class=o>=</span> <span class=n>posterior_variance_corrected</span><span class=p>[</span><span class=n>timestep</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># data_in_batch is latent variable x at previous timestep</span>
</span></span><span class=line><span class=cl><span class=n>data_in_batch</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>mean_data</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>posterior_data</span><span class=p>))</span><span class=o>.</span><span class=n>T</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=score-function-prediction>Score Function Prediction<a hidden class=anchor aria-hidden=true href=#score-function-prediction>#</a></h3><p>This is yet another variation on diffusion models. Similar to predicting the error as discussed earlier, we could also predict the score function. The score function $s$ is defined as $\nabla p(x_t)$.</p><p>All we need is to define $x_t$ as $f(s)$. We could then define a $Score\_Loss(s, \hat s, t)$ by substituting $x_t$ for the function $f(s)$ in the training step. In the denoising step we will use the function $f(s)$ defined to compute $x_t$, perform clipping so that the output lies in the normalized range and proceed as we did in the earlier section.</p><p>The relation between $s$ and $x$ is defined as below:
$$
x_0 = \frac{x_t + (1 - \bar\alpha_t )\ast\nabla p(x_t)}{\sqrt{\bar\alpha_t}}
$$</p><p>For more details and intuition on why this is an important interpretation, please refer to the section on <cite>Three equivalent interpretations.<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></cite>
Yang Song has an excellent blog post on <a href=https://yang-song.net/blog/2021/score/ target=_blank>Score based generative models.</a></p><h2 id=guidance--classifier-free-guidance>Guidance & Classifier-free Guidance<a hidden class=anchor aria-hidden=true href=#guidance--classifier-free-guidance>#</a></h2><p>We want to control the data we generate. For example, in the image below, we have 2 labels. During generation, we want to direct the model to generate samples either from Yellow or Purple classes.
Classifier Guidance is a way to do this. We will be guiding the denoising process to generate samples that are more likely to belong to the conditioned class.</p><figure class=align-center><img loading=lazy src=/images/data-labels.png#center alt="Figure 1: Data with 2 labels, Circles in Purple and Moons in Yellow" width=80%><figcaption><p>Figure 1: Data with 2 labels, Circles in Purple and Moons in Yellow</p></figcaption></figure><p>Text-guided diffusion models like <a href="https://www.youtube.com/watch?v=gwI6g1pBD84" target=_blank>Glide</a> use the powerful Neural Network called <a href=https://openai.com/blog/clip/ target=_blank>CLIP</a> and classifier guidance techniques to perform text-based image generation.</p><p>So far, we have been trying to maximize the likelihood of the data distribution $p(x)$ with diffusion models. This allowed us to randomly sample data points from the data distribution.</p><p><strong>A naïve idea</strong> to do class-conditioned generation could be to fuse the conditional label information with the input data.</p><p><strong>Pseudocode for naïve idea:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-4-1><a class=lnlinks href=#hl-4-1> 1</a>
</span><span class=lnt id=hl-4-2><a class=lnlinks href=#hl-4-2> 2</a>
</span><span class=lnt id=hl-4-3><a class=lnlinks href=#hl-4-3> 3</a>
</span><span class=lnt id=hl-4-4><a class=lnlinks href=#hl-4-4> 4</a>
</span><span class=lnt id=hl-4-5><a class=lnlinks href=#hl-4-5> 5</a>
</span><span class=lnt id=hl-4-6><a class=lnlinks href=#hl-4-6> 6</a>
</span><span class=lnt id=hl-4-7><a class=lnlinks href=#hl-4-7> 7</a>
</span><span class=lnt id=hl-4-8><a class=lnlinks href=#hl-4-8> 8</a>
</span><span class=lnt id=hl-4-9><a class=lnlinks href=#hl-4-9> 9</a>
</span><span class=lnt id=hl-4-10><a class=lnlinks href=#hl-4-10>10</a>
</span><span class=lnt id=hl-4-11><a class=lnlinks href=#hl-4-11>11</a>
</span><span class=lnt id=hl-4-12><a class=lnlinks href=#hl-4-12>12</a>
</span><span class=lnt id=hl-4-13><a class=lnlinks href=#hl-4-13>13</a>
</span><span class=lnt id=hl-4-14><a class=lnlinks href=#hl-4-14>14</a>
</span><span class=lnt id=hl-4-15><a class=lnlinks href=#hl-4-15>15</a>
</span><span class=lnt id=hl-4-16><a class=lnlinks href=#hl-4-16>16</a>
</span><span class=lnt id=hl-4-17><a class=lnlinks href=#hl-4-17>17</a>
</span><span class=lnt id=hl-4-18><a class=lnlinks href=#hl-4-18>18</a>
</span><span class=lnt id=hl-4-19><a class=lnlinks href=#hl-4-19>19</a>
</span><span class=lnt id=hl-4-20><a class=lnlinks href=#hl-4-20>20</a>
</span><span class=lnt id=hl-4-21><a class=lnlinks href=#hl-4-21>21</a>
</span><span class=lnt id=hl-4-22><a class=lnlinks href=#hl-4-22>22</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>diffusion</span><span class=p>(</span><span class=n>x_0</span><span class=p>,</span> <span class=n>t</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>code</span> <span class=n>to</span> <span class=n>add</span> <span class=n>noise</span> <span class=n>to</span> <span class=n>x_0</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>x_i</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>training</span><span class=p>():</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>loop</span> <span class=n>until</span> <span class=n>convergence</span><span class=p>:</span>
</span></span><span class=line><span class=cl>		<span class=n>pick</span> <span class=n>an</span> <span class=n>image</span> <span class=n>x_0</span> <span class=kn>from</span> <span class=nn>X</span> <span class=p>(</span><span class=n>batch</span> <span class=n>of</span> <span class=n>images</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>sample</span> <span class=n>t</span> <span class=kn>from</span> <span class=mi>1</span> <span class=n>to</span> <span class=n>T</span>
</span></span><span class=line><span class=cl>		<span class=n>x_t</span> <span class=o>=</span> <span class=n>diffusion_step</span><span class=p>(</span><span class=n>x_0</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>x_hat_t</span> <span class=o>=</span> <span class=n>NN</span><span class=p>(</span><span class=n>x_t</span><span class=p>,</span> <span class=n>t</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>  <span class=c1># y is the label</span>
</span></span><span class=line><span class=cl>		<span class=n>loss_</span> <span class=o>=</span> <span class=n>loss</span><span class=p>(</span><span class=n>x_0</span><span class=p>,</span> <span class=n>x_hat_t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>update</span><span class=p>(</span><span class=n>NN</span><span class=p>,</span> <span class=n>grad</span><span class=p>(</span><span class=n>loss_</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># generating new data points through denoising steps</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>generate_new_data</span><span class=p>():</span>
</span></span><span class=line><span class=cl>	<span class=n>sample</span> <span class=n>x_T</span> <span class=kn>from</span> <span class=nn>N</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>I</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>T</span><span class=p>,</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=c1># denoising step</span>
</span></span><span class=line><span class=cl>		<span class=n>x_hat_t</span> <span class=o>=</span> <span class=n>NN</span><span class=p>(</span><span class=n>x_t</span><span class=p>,</span> <span class=n>t</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>  <span class=c1># y is the desired label </span>
</span></span><span class=line><span class=cl>		<span class=c1># get x_{t-1}</span>
</span></span><span class=line><span class=cl>		<span class=n>x_</span><span class=p>{</span><span class=n>t</span><span class=o>-</span><span class=mi>1</span><span class=p>}</span> <span class=o>=</span> <span class=n>func</span><span class=p>(</span><span class=n>x_hat_t</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span> <span class=c1># refer EQ - 12</span>
</span></span><span class=line><span class=cl>	<span class=n>x_hat_0</span> <span class=o>=</span> <span class=n>x_0</span>  
</span></span></code></pre></td></tr></table></div></div><p>During training and data generation, we will add the conditioned label as an input along with the noisy data and the time step embedding. The output of the model will stay the same as earlier.</p><p>The conditioned data can be fused with the input data, just like we fused the time step information.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-5-1><a class=lnlinks href=#hl-5-1> 1</a>
</span><span class=lnt id=hl-5-2><a class=lnlinks href=#hl-5-2> 2</a>
</span><span class=lnt id=hl-5-3><a class=lnlinks href=#hl-5-3> 3</a>
</span><span class=lnt id=hl-5-4><a class=lnlinks href=#hl-5-4> 4</a>
</span><span class=lnt id=hl-5-5><a class=lnlinks href=#hl-5-5> 5</a>
</span><span class=lnt id=hl-5-6><a class=lnlinks href=#hl-5-6> 6</a>
</span><span class=lnt id=hl-5-7><a class=lnlinks href=#hl-5-7> 7</a>
</span><span class=lnt id=hl-5-8><a class=lnlinks href=#hl-5-8> 8</a>
</span><span class=lnt id=hl-5-9><a class=lnlinks href=#hl-5-9> 9</a>
</span><span class=lnt id=hl-5-10><a class=lnlinks href=#hl-5-10>10</a>
</span><span class=lnt id=hl-5-11><a class=lnlinks href=#hl-5-11>11</a>
</span><span class=lnt id=hl-5-12><a class=lnlinks href=#hl-5-12>12</a>
</span><span class=lnt id=hl-5-13><a class=lnlinks href=#hl-5-13>13</a>
</span><span class=lnt id=hl-5-14><a class=lnlinks href=#hl-5-14>14</a>
</span><span class=lnt id=hl-5-15><a class=lnlinks href=#hl-5-15>15</a>
</span><span class=lnt id=hl-5-16><a class=lnlinks href=#hl-5-16>16</a>
</span><span class=lnt id=hl-5-17><a class=lnlinks href=#hl-5-17>17</a>
</span><span class=lnt id=hl-5-18><a class=lnlinks href=#hl-5-18>18</a>
</span><span class=lnt id=hl-5-19><a class=lnlinks href=#hl-5-19>19</a>
</span><span class=lnt id=hl-5-20><a class=lnlinks href=#hl-5-20>20</a>
</span><span class=lnt id=hl-5-21><a class=lnlinks href=#hl-5-21>21</a>
</span><span class=lnt id=hl-5-22><a class=lnlinks href=#hl-5-22>22</a>
</span><span class=lnt id=hl-5-23><a class=lnlinks href=#hl-5-23>23</a>
</span><span class=lnt id=hl-5-24><a class=lnlinks href=#hl-5-24>24</a>
</span><span class=lnt id=hl-5-25><a class=lnlinks href=#hl-5-25>25</a>
</span><span class=lnt id=hl-5-26><a class=lnlinks href=#hl-5-26>26</a>
</span><span class=lnt id=hl-5-27><a class=lnlinks href=#hl-5-27>27</a>
</span><span class=lnt id=hl-5-28><a class=lnlinks href=#hl-5-28>28</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># haiku code</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>mlp</span> <span class=o>=</span> <span class=n>hk</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
</span></span><span class=line><span class=cl>  <span class=n>hk</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>),</span>
</span></span><span class=line><span class=cl>  <span class=n>jax</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>gelu</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>hk</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=c1># conditional vectors encoding</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>embedding_vectors</span> <span class=o>=</span> <span class=n>hk</span><span class=o>.</span><span class=n>Embed</span><span class=p>(</span><span class=mi>10</span><span class=o>+</span><span class=mi>26</span><span class=o>+</span><span class=mi>26</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span> <span class=mi>64</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>timestep_embeddings</span> <span class=o>=</span> <span class=n>TimeEmbeddings</span><span class=p>(</span><span class=mi>64</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># the diffusion model is given x, time step and label as input</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=fm>__call__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>timesteps</span><span class=p>,</span> <span class=n>cond</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>cond_embedding</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=n>conditioning</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>timesteps</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=n>timestep_embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>timestep_embeddings</span><span class=p>(</span><span class=n>timesteps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=n>conditioning</span> <span class=o>=</span> <span class=n>timestep_embeddings</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>cond</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=n>label_embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding_vectors</span><span class=p>(</span><span class=n>cond</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=n>conditioning</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>concatenate</span><span class=p>([</span><span class=n>label_embeddings</span><span class=p>,</span> <span class=n>conditioning</span><span class=p>],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>cond_embedding</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>mlp</span><span class=p>(</span><span class=n>conditioning</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=o>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=c1># fusing time step and label information with input x</span>
</span></span><span class=line><span class=cl>	<span class=n>shift</span><span class=p>,</span> <span class=n>scale</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>cond_embedding</span><span class=p>,</span> <span class=n>indices_or_sections</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>x</span> <span class=o>=</span> <span class=n>shift</span> <span class=o>+</span> <span class=p>(</span><span class=n>scale</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span><span class=o>*</span><span class=n>x</span>
</span></span><span class=line><span class=cl>	<span class=o>...</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=guidance>Guidance<a hidden class=anchor aria-hidden=true href=#guidance>#</a></h3><p>The above approach may lead to models that have low sample diversity. Researchers have proposed two other forms of guidance: <em>classifier guidance</em> and <cite>classifier-free guidance<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup></cite>.</p><p><strong>Classifier Guidance</strong> guides the generation of new samples with the help of a classifier. The classifier takes a noisy image ($x_t$) as input and predicts the label $y$. The gradient of the distribution $p(y|x)$ is used to make updates to the weights of the Neural Network to guide it to produce samples that are likely to be $y$. This is an adversarial loss, and this approach has similarities to GANs.</p><p><strong>Classifier-Free Guidance</strong> would be ideal if we did not want to build a classifier. The classifier-free guidance approach models the conditional likelihood of samples as follows:
$$
\nabla p(x|y) = \lambda \ast \underbrace{\nabla p(x|y)}_{\text{conditional}} + (1-\lambda) \ast \underbrace{\nabla p(x)}_{\text{unconditional}}
$$
The conditional and the unconditional distributions are modelled by the same neural network. To model the conditional distribution, we fuse the label information as shown in the naïve approach above. To model the unconditional distribution, we mask the label information and pass it to the diffusion model.
The lambda parameter, controls the diversity of the sample we want to generate. $\lambda=1$ would be equivalent to the naïve approach.</p><p><blockquote class=twitter-tweet data-dnt=true><p lang=en dir=ltr>Classifier-free guidance is a cheatcode that makes these models perform as if they had 10x the parameters. At least in terms of sample quality, and at the cost of diversity. All of the recent spectacular results rely heavily on this trick.</p>&mdash; Sander Dieleman (@sedielem) <a href="https://twitter.com/sedielem/status/1557691965076021252?ref_src=twsrc%5Etfw">August 11, 2022</a></blockquote>Adding this perspective about efficiency of diffusion models. Even though to be honest, I am not sure myself :).</p><h2 id=lets-look-at-some-outputs>Let&rsquo;s look at some outputs<a hidden class=anchor aria-hidden=true href=#lets-look-at-some-outputs>#</a></h2><figure class=align-center><img loading=lazy src=/images/class-conditioned-generation.gif#center alt="Figure 2: A GIF show-casing the denoising process; Generating class conditioned samples over T time steps" width=80%><figcaption><p>Figure 2: A GIF show-casing the denoising process; Generating class conditioned samples over T time steps</p></figcaption></figure><p>See you in the <a href=/posts/diffusion-models/denoising-diffusion-models-3>next part</a>.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Vaswani et al. 2017 <a href=https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf target=_blank>&ldquo;Attention is all you need&rdquo;</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Ho et al. 2020 <a href=https://arxiv.org/abs/2006.11239 target=_blank>&ldquo;Denoising diffusion probabilistic models&rdquo;</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Calvin Luo; 2019 <a href=https://arxiv.org/abs/2208.11970 target=_blank>&ldquo;Understanding Diffusion Models: A Unified Perspective&rdquo;</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Ho et al. 2021 <a href=https://arxiv.org/abs/2207.12598 target=_blank>&ldquo;Classifier-free diffusion guidance&rdquo;</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://varun-ml.github.io/tags/conditioned-generation/>conditioned generation</a></li><li><a href=https://varun-ml.github.io/tags/classifier-free-guidance/>classifier free guidance</a></li><li><a href=https://varun-ml.github.io/tags/timestep-embedding/>timestep embedding</a></li><li><a href=https://varun-ml.github.io/tags/diffusion-model/>diffusion model</a></li><li><a href=https://varun-ml.github.io/tags/emnist/>emnist</a></li><li><a href=https://varun-ml.github.io/tags/generative-ai/>generative ai</a></li><li><a href=https://varun-ml.github.io/tags/torch/>torch</a></li><li><a href=https://varun-ml.github.io/tags/error-estimation/>error estimation</a></li><li><a href=https://varun-ml.github.io/tags/score-estimation/>score estimation</a></li><li><a href=https://varun-ml.github.io/tags/tutorial/>tutorial</a></li><li><a href=https://varun-ml.github.io/tags/diffusion-model-series/>diffusion model series</a></li><li><a href=https://varun-ml.github.io/tags/diffusion-model-blog/>diffusion model blog</a></li><li><a href=https://varun-ml.github.io/tags/diffusion-model-tutorial/>diffusion model tutorial</a></li><li><a href=https://varun-ml.github.io/tags/google-colab/>google colab</a></li></ul><nav class=paginav><a class=prev href=https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-3/><span class=title>« Prev</span><br><span>Denoising Diffusion Models Part 3: Generating Characters and numbers with Diffusion Models</span></a>
<a class=next href=https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-1/><span class=title>Next »</span><br><span>Denoising Diffusion Models Part 1: Estimating True Distribution</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Denoising Diffusion Models Part 2: Improving Diffusion Models on twitter" href="https://twitter.com/intent/tweet/?text=Denoising%20Diffusion%20Models%20Part%202%3a%20Improving%20Diffusion%20Models&url=https%3a%2f%2fvarun-ml.github.io%2fposts%2fdiffusion-models%2fdenoising-diffusion-models-2%2f&hashtags=conditionedgeneration%2cclassifierfreeguidance%2ctimestepembedding%2cdiffusionmodel%2cemnist%2cgenerativeai%2ctorch%2cerrorestimation%2cscoreestimation%2ctutorial%2cdiffusionmodelseries%2cdiffusionmodelblog%2cdiffusionmodeltutorial%2cgooglecolab"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Denoising Diffusion Models Part 2: Improving Diffusion Models on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fvarun-ml.github.io%2fposts%2fdiffusion-models%2fdenoising-diffusion-models-2%2f&title=Denoising%20Diffusion%20Models%20Part%202%3a%20Improving%20Diffusion%20Models&summary=Denoising%20Diffusion%20Models%20Part%202%3a%20Improving%20Diffusion%20Models&source=https%3a%2f%2fvarun-ml.github.io%2fposts%2fdiffusion-models%2fdenoising-diffusion-models-2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Denoising Diffusion Models Part 2: Improving Diffusion Models on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fvarun-ml.github.io%2fposts%2fdiffusion-models%2fdenoising-diffusion-models-2%2f&title=Denoising%20Diffusion%20Models%20Part%202%3a%20Improving%20Diffusion%20Models"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Denoising Diffusion Models Part 2: Improving Diffusion Models on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fvarun-ml.github.io%2fposts%2fdiffusion-models%2fdenoising-diffusion-models-2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Denoising Diffusion Models Part 2: Improving Diffusion Models on whatsapp" href="https://api.whatsapp.com/send?text=Denoising%20Diffusion%20Models%20Part%202%3a%20Improving%20Diffusion%20Models%20-%20https%3a%2f%2fvarun-ml.github.io%2fposts%2fdiffusion-models%2fdenoising-diffusion-models-2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Denoising Diffusion Models Part 2: Improving Diffusion Models on telegram" href="https://telegram.me/share/url?text=Denoising%20Diffusion%20Models%20Part%202%3a%20Improving%20Diffusion%20Models&url=https%3a%2f%2fvarun-ml.github.io%2fposts%2fdiffusion-models%2fdenoising-diffusion-models-2%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://varun-ml.github.io/>wity'ai</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>