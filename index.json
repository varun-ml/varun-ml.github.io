[{"content":"What are Denoising Diffusion Models? Denoising Diffusion models, commonly referred to as Diffusion models, are a class of generative models based on the Variational Auto Encoder (VAE) architecture. These models are called likelihood-based models because they assign a high likelihood to the observed data samples. In contrast to other generative models, such as GANs, which learn the sampling process of a complex distribution and are trained adversarially.\nVAE\u0026rsquo;s Let\u0026rsquo;s take a detour and understand VAEs, as it will help with some intuition. VAEs are composed of two processes: an encoder ($q$) (also referred to as the inference model), which generates a latent representation ($z$) of the input data ($x$), and a decoder ($p$) (also referred to as the generator), which generates the input data ($\\hat x$) using the latent representation ($z$) as input. The encoder and decoder are trained together using a variational objective, often referred to as the ELBO.\nFigure 1: An architecture for a Variational Auto Encoder. (Image source: VAE tutorial, Kingma et.al; 2019)\nFigure 2: Graphical representation of a Variational Auto Encoder. The $p$ function is the decoder and the $q$ function is the encoder. (Image source: Calvin Luo; 2022)\nFigures 1 \u0026amp; 2 give a simplistic representation of a VAE model.\nDenoising Diffusion Models Analogous to VAEs, Denoising Diffusion models also consist of two processes:\nDiffusion - This is analogous to the VAE encoder. In this process, we repeatedly (over $t$ steps: $1$ to $T$) sample random noise and corrupt our input data by adding the noise. In contrast to the VAE encoder, we typically do not learn this process. At step $T$, the data would be just noise. Figure 3: Illustration of the Diffusion Process. Image of \u0026lsquo;Z\u0026rsquo; char is corrupted step by step.\nDenoising - This is analogous to the VAE decoder. The denoising is done by a learned model that takes completely noisy data and tries to generate the input data by repeatedly (over $t$ steps: $1$ to $T$) removing noise from the noisy data. Figure 4: Illustration of the Denoising Process. Image of \u0026lsquo;Z\u0026rsquo; character is generated step by step.\nDiffusion models are able to generate images from pure noise.\nHumans paint, they are able to generate images in a step-wise manner by painting on a blank canvas. Diffusion models are similar, they generate images in step-wise manner by denoising a noisy canvas.\nIntroduction to this series I have spent too much time understanding diffusion models, which started with some wild posts I saw on Twitter.\nIntroducing Imagen, a new text-to-image synthesis model that can generate high-fidelity, photorealistic images from a deep level of language understanding. Learn more and and check out some examples of #imagen at https://t.co/RhD6siY6BY pic.twitter.com/C8javVu3iW\n\u0026mdash; Google AI (@GoogleAI) May 24, 2022 Imagen Google AI Diffusion model. (Image source: Imagen Web Link)\nMy curiosity led to experiments, which enriched my ML skill-set. In this series, I will try to demystify some of the magic behind diffusion models. Finally read a tutorial on Diffusion models. Now, I understand how they work just not why they work. To me it\u0026#39;s crazy that they do. pic.twitter.com/LXm5OrD993\n\u0026mdash; Varun Tulsian (@varuntul22) October 5, 2022 Diffusion Models Series In this series, I will attempt to simplify the concepts and provide you with some code that you can easily run on Google Colab or your local Jupyter server. To be able to run things with minimal setup, we will be working on a 2D dataset (generated using scikit-learn) and the EMNIST dataset (28x28x1 images).\nThis is the first of 4 posts on diffusion models. You can check all the posts Full Diffusion Model Series. All the code for the diffusion model series is available here.\nThe first 3 parts of the series will focus on setting up the basic concepts and code. You won\u0026rsquo;t need a GPU to run the notebooks. The code is written in PyTorch. We will be working on simple 2D distributions and trying to generate them using denoising:\nPart 1: I will introduce the basics of the denoising approach for the diffusion model. We will predict the original distribution directly, following the first part of Luo, 20221\nPart 2: I will introduce some optimizations that are shown to work better. This will correspond to the second part (Three Equivalent Interpretations) of Luo, 20221 Part 3: I will introduce concepts around classifier-free guidance and steps to generate distributions faster using striding.\nIn the last part of the series, we will be using the concepts learned to implement diffusion models for character generation. This will be done by training a U-Net model over the Extended-MNIST dataset. The code is written in JAX, Haiku.\nHere are a few high-quality blogs that can help you understand diffusion models in greater depth. You can find a comprehensive list of resources here.\nWhat are Diffusion Models? Let\u0026rsquo;s go over this again, this time in a little more depth. We will start seeing some mathematical equations and pyTorch code below.\nFigure 5: Graphical representation of a Denoising Diffusion Model. The $p$ function is the decoder and the $q$ function is the encoder. (Image source: Calvin Luo; 2022)\nDenoising Diffusion models are a Markovian Hierarchical Variational Auto Encoder , unlike a vanilla VAE model the encoder process (diffusion) and the decoder process (denoising) happen over multiple steps. As shown in figure 5, the diffusion process starts with a random variable $x$ and generates random variables $x_i$ at the $t^{th}$ step. The denoising process starts at $x_T$ and attempts to generate $x_i$ and ultimately $x$.\nWe will be going over the Diffusion Process and the Denoising Process followed by the Training procedure. But first let\u0026rsquo;s build an understanding of what needs to be done to implement such a model.\nTraining a Denoising Diffusion model This is the main source of confusion when it comes to understanding diffusion models. From the description above and the analogy with the VAEs it would seem that training a diffusion model would comprise of the following steps.\nFeeding image as an input ($x$). Go through a Forward Pass (Diffusion), generating $x_t$\u0026rsquo;s in sequence. Feed the noisy image ($x_T$) to the Backward pass (Denoising), obtaining $\\hat x$. This would be done using a $ NN(x_t, t)$ and it should output $x_{t-1}$. Computing a loss based $L(x, \\hat x)$ and doing weight update of the model $NN$. But of-course this isn\u0026rsquo;t the case. Let\u0026rsquo;s investigate issues with this approach. For every input in our dataset, we will have to go through all the timesteps, apply diffusion and then go through the whole denoising steps, with weight updates only happening at the end of the pass. Training in such a way would be slow and we would not be able to meaningfully learn anything useful.\nInstead, we can show that we can effectively learn the distribution $p(X)$ while just doing the following steps.\nFeeding image as an input ($x$). 1a. Sample a time variable $t$ from $1,to,T$, uniformly at random. Compute the latent variable $x_t$, this would be done in 1 step. Refer section on diffusion-process. Feed the noisy image ($x_t$) to the $NN$ model, obtaining $\\hat x$. $NN(x_t, t)$ outputs $\\hat x_t$. We are not going over each step during training. $\\hat x_t$ is the reconstruction of the input $x$ as predicted at timestep $t$.\nComputing a loss based $L(x, \\hat x_t, t)$ and doing weight update of the model $NN$. Pseudo code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def diffusion(x_0, t): code to add noise to x_0 return x_i def training(): for loop until convergence: pick an image x_0 from X (batch of images) sample t from 1 to T x_t = diffusion(x_0, t) x_hat = NN(x_t, t) loss_ = loss(x, x_hat) update(NN, grad(loss_) # denoising def generate_new_data(): sample x_T from N(0, I) for t in range(T, 1): x_hat_t = NN(x_t, t) # get x_{t-1} x_{t-1} = func(x_hat_t, t) x_hat = x_0 The proof requires us to reduce the ELBO loss making use of the Markovian assumption in the diffusion process, and using Monte-Carlo estimates to obtain an equivalent loss. I will not go into details on this proof, please refer Section: Variational Diffusion models, equation 100 gives the reduced loss1.\nDiffusion Process As described earlier we are going to be adding noise to the input vector. Let\u0026rsquo;s see how exactly one can do that \u0026ndash;\nAt each time-step $t$ in the diffusion process we sample from the latent variable, $q_t(x_t|x_{t-1})$. $$ q_t(x_t|x_{t-1}) = N(\\sqrt\\alpha_tx_{t-1}, (1 - \\alpha_t )I) $$ $\\alpha_t$, $t\\in[1, T]$, where $\\alpha_t \u0026lt; 1$, describes the diffusion schedule. $\\alpha_{t-1} \u0026lt; \\alpha_t$, as we go along in the diffusion process we are adding more and more noise.\nThe form of the coefficients are chosen such that the variance of the latent variables stay at a similar scale. At this point, let me introduce the reparameterization trick.\n$$ N(\\sqrt\\alpha_tx_{t-1}, (1 - \\alpha_t )I) = \\sqrt\\alpha_tx_{t-1} + (1 - \\alpha_t)*\\epsilon \\quad with \\,\\, \\epsilon \\sim N(0, I) $$ The reparemetarization trick is cool, it simplifies working with complex Gaussian distribution. If you want to sample from the diffusion step at $t$ you would now just need to sample from a standard Gaussian distribution $N(0, I)$ and factor and plug in the mean and variance according.\nLet\u0026rsquo;s look at another cool result, let\u0026rsquo;s say we want to sample from the latent $x_t$ directly given $x$. ie. we want to sample from $q(x_t|x)$.\n$$ \\begin{align} q(x_t|x) \u0026= N(\\sqrt\\alpha_tx_{t-1}, (1 - \\alpha_t )I) \\cr \u0026= \\sqrt\\alpha_t x_{t-1} + \\sqrt(1-\\alpha_t)\\ast\\epsilon_t \\cr \u0026= \\sqrt\\alpha_t(\\sqrt\\alpha_t x_{t-2} + \\sqrt(1-\\alpha_{t-1})\\ast\\epsilon_{t-1}) + \\sqrt(1-\\alpha_t)\\ast\\epsilon_t \\cr \u0026= \\sqrt\\alpha_t\\sqrt\\alpha_t x_{t-2} + \\sqrt\\alpha_t\\sqrt(1-\\alpha_{t-1})\\ast\\epsilon_{t-1} + \\sqrt(1-\\alpha_t)\\ast\\epsilon_t \\cr \u0026= \\sqrt\\alpha_t\\sqrt\\alpha_tx_{t-2} + \\sqrt(1-\\alpha_t\\alpha_{t-1})\\ast\\epsilon_{t-1}^\\ast \\quad where \\thinspace \\epsilon_{t-1}^\\ast\\in N(0, I) \\cr \u0026= ... \\cr \u0026= \\sqrt(\\bar\\alpha_t)x + \\sqrt(1 - \\bar\\alpha_t )\\ast\\epsilon^\\ast ; where \\space \\bar\\alpha_t=\\Pi_{i=1}^T(\\sqrt\\alpha_i), \\space \\epsilon^\\ast \\in N(0, I) \\cr \u0026= N(\\sqrt(\\bar\\alpha_t)x, (1 - \\bar\\alpha_t )I)\\cr \\end{align} $$ In equation 5, we have utilized sum of two independent Gaussian random variables.\nNote: $x=x_0$ it\u0026rsquo;s the input data.\nWith this in place, let\u0026rsquo;s put some code together.\nThe diffusion schedule: $\\alpha$\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 timestepts = 200 ## linear schedule def linear_beta_schedule(timesteps): beta_start = 0.0001 beta_end = 0.02 return jnp.linspace(beta_start, beta_end, timesteps) ## cosine schedule as proposed in https://arxiv.org/abs/2102.09672 ## The cosine schedule is recommened if timesteps \u0026gt;\u0026gt; 200. As it results in a gradual noisification of the input data def cosine_beta_schedule(timesteps, s=0.008): steps = timesteps + 1 x = jnp.linspace(0, timesteps, steps) alphas_cumprod = jnp.cos(((x / timesteps) + s) / (1 + s) * jnp.pi * 0.5) ** 2 alphas_cumprod = alphas_cumprod / alphas_cumprod[0] betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1]) return jnp.clip(betas, 0.0001, 0.1) ## some handy variables betas = linear_beta_schedule(timesteps) alphas = 1 - betas alphas_ = torch.cumprod(alphas, axis=0) variance = 1 - alphas_ sd = torch.sqrt(variance) import torch.nn.functional as F alphas_prev_ = F.pad(alphas_[:-1], [1, 0], \u0026#34;constant\u0026#34;, 1.0) Diffusion: Given input data $x$, a timestep $t$ and a schedule the diffusion method should return the latent variable $x_t$.\n1 2 3 4 5 6 # how to add noise to the data def get_noisy(batch, timestep): # we will use the reparameterization trick noise_at_t = torch.normal(0, std=1, size=batch.size()) added_noise_at_t = batch.mul(torch.sqrt(alphas_[timestep])) + noise_at_t.mul(sd[timestep]) return added_noise_at_t, noise_at_t In Variational Diffusion Models, Kingma et.al, 20222 propose a way to learn the parameters of the schedule and provide additional insights helpful in understanding diffusion models.\nTraining Procedure Looking at the pseudo code, we have just defined the diffusion method. Let\u0026rsquo;s define the loss function and the Neural Network.\nNeural Network:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 from torch import nn class DenoisingModelSequential(nn.Module): def __init__(self, hidden_units=32): super(DenoisingModelSequential, self).__init__() # hidden_units = 32 self.mlp = nn.Sequential( nn.Linear(3, int(hidden_units), bias=True), nn.GELU(), nn.Linear(int(hidden_units), int(hidden_units/2), bias=True), nn.GELU(), nn.Linear(int(hidden_units/2), int(hidden_units/4), bias=True), nn.GELU(), nn.Linear(int(hidden_units/4), int(hidden_units/8), bias=True), nn.GELU(), nn.Linear(int(hidden_units/8), int(hidden_units/16), bias=True), nn.GELU(), nn.Linear(int(hidden_units/16), int(hidden_units/8), bias=True), nn.GELU(), nn.Linear(int(hidden_units/8), int(hidden_units/4), bias=True), nn.GELU(), nn.Linear(int(hidden_units/4), int(hidden_units/2), bias=True), nn.GELU(), nn.Linear(int(hidden_units/2), int(hidden_units), bias=True), nn.GELU(), nn.Linear(int(hidden_units), 2, bias=True) ) def forward(self, x): x = self.mlp(x) return x denoising_model = DenoisingModelSequential(64) The input to the NN is 3 dimensional. We are working with 2d dataset, so $x_t$\u0026rsquo;s is a 2d vector. In this article we are going to pass $t$ as the 3rd dimension, we will pass it as a scalar. In the subsequent articles we will see how to generate an embedding for a timestep and concatenate/fuse it with the input. The output of the NN needs to be $\\hat x_t$ which is a 2d vector. The NN architecture doesn\u0026rsquo;t really matter. Here, I have chosen to use a basic Multi Layer Perceptron with GeLU activation units3. We will use a U-net architecture for character generation using EMNIST dataset. Checkout Part 4.\nLoss Function: At timestep $t$ $$ Loss(x, \\hat x_t, t) = 1/2\\ast(\\frac{\\bar\\alpha_{t-1}}{1-\\bar\\alpha_{t-1}} - \\frac{\\bar\\alpha_t}{1-\\bar\\alpha_t})\\ast\\mid\\mid x-\\hat x_t\\mid\\mid_2^2 $$ $$ \\text{SNR}_t =\\frac{\\bar\\alpha_t}{1-\\bar\\alpha_t} $$ SNR stands for Signal to Noise. In the case of the diffusion, the schedule needs to be chosen such that $SNR_t \u0026lt; SNT_{t-1}$.\nLoss:\n1 2 3 4 5 6 # instead of the l2 loss, I use the huber loss # https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html loss_func = nn.HuberLoss() imp_weight = torch.sqrt(1/2 * ((alphas_prev_[timestep] / (1 - alphas_prev_[timestep])) - (alphas_[timestep]/(1 - alphas_[timestep])))) loss_ = loss_func(data_in_batch.mul(imp_weight), pred_data.T.mul(imp_weight)) In Variation Diffusion Models2, authors propose using a separate Neural Network to model SNR as a function of t. The Neural Network needs to be monotonically decreasing.\nDenoising Process Let\u0026rsquo;s recap, the denoising process is responsible to generate synthetic data $\\hat x$. It starts with a completely noisy data point and repeatedly uses a Neural Network to predict $\\hat x_t$, we use this to get $x_{t-1}$\nWritten with StackEdit.\nCalvin Luo, 2019 \u0026ldquo;Understanding Diffusion Models: A Unified Perspective\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKingma et al. 2022 \u0026ldquo;Variational Diffusion Models\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHendrycks et. al; 2016 Gelu\u0026rsquo;s\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-1/","summary":"What are Denoising Diffusion Models? Denoising Diffusion models, commonly referred to as Diffusion models, are a class of generative models based on the Variational Auto Encoder (VAE) architecture. These models are called likelihood-based models because they assign a high likelihood to the observed data samples. In contrast to other generative models, such as GANs, which learn the sampling process of a complex distribution and are trained adversarially.\nVAE\u0026rsquo;s Let\u0026rsquo;s take a detour and understand VAEs, as it will help with some intuition.","title":"Denoising Diffusion Models Part 1: Estimating True Distribution"},{"content":" Notebook Github Link Colab Basic: Predicting Original Distribution Vanilla Implementation Predicting Error and Score Function Error / Score Prediction Classifier free Guidance and other improvements Advanced concepts EMINST De-noising and Conditional generation Colab EMNIST ","permalink":"https://varun-ml.github.io/posts/diffusion-models/diffusion-models-notebooks/","summary":" Notebook Github Link Colab Basic: Predicting Original Distribution Vanilla Implementation Predicting Error and Score Function Error / Score Prediction Classifier free Guidance and other improvements Advanced concepts EMINST De-noising and Conditional generation Colab EMNIST ","title":"Code: Diffusion Model Notebooks"},{"content":" Notebook Github Link Colab Co;here Research Scholar Assignment Github ","permalink":"https://varun-ml.github.io/posts/cohere/cohere-research-scholar/","summary":" Notebook Github Link Colab Co;here Research Scholar Assignment Github ","title":"Code: Cohere Research Scholar Notebook"}]