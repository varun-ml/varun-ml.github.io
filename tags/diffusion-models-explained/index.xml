<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>diffusion models explained on wity&#39;ai</title>
    <link>https://varun-ml.github.io/tags/diffusion-models-explained/</link>
    <description>Recent content in diffusion models explained on wity&#39;ai</description>
    <image>
      <url>https://varun-ml.github.io/images/varun.png</url>
      <link>https://varun-ml.github.io/images/varun.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 Dec 2022 17:39:20 +0530</lastBuildDate><atom:link href="https://varun-ml.github.io/tags/diffusion-models-explained/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Denoising Diffusion Models Part 3: Generating Characters and numbers with Diffusion Models</title>
      <link>https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-3/</link>
      <pubDate>Fri, 09 Dec 2022 17:38:58 +0530</pubDate>
      
      <guid>https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-3/</guid>
      <description>Notebook Github Link Colab EMINST Denoising and Conditional generation Colab EMNIST Introduction We have introduced most of the concepts in the previous two blogs. In this blog post, we will see how the concepts translate to code. If you want to check out the earlier posts, you can find them here, diffusion model intro 1, and diffusion model intro 2.
EMNIST dataset Extended-MNIST dataset, as the name suggests, is an extension of the popular MNIST dataset.</description>
      <content:encoded><![CDATA[<table>
<thead>
<tr>
<th style="text-align:left">Notebook</th>
<th style="text-align:left">Github Link</th>
<th style="text-align:left">Colab</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">EMINST Denoising and Conditional generation</td>
<td style="text-align:left"><a href="https://github.com/varun-ml/diffusion-models-tutorial/blob/master/emnist-colab-notebooks/colab_EMNIST_conditional.diffusion_model.large.with_batch_norm.ipynb" target="_blank" >Colab EMNIST</a></td>
<td style="text-align:left"><a href="https://colab.research.google.com/github/varun-ml/diffusion-models-tutorial/blob/master/emnist-colab-notebooks/colab_EMNIST_conditional.diffusion_model.large.with_batch_norm.ipynb" target="_blank" >
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Colab (Large)">
</a><a href="https://colab.research.google.com/github/varun-ml/diffusion-models-tutorial/blob/master/emnist-colab-notebooks/colab_EMNIST_conditional.diffusion_model.ipynb" target="_blank" >
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Colab (Small)">
</a></td>
</tr>
</tbody>
</table>
<h2 id="introduction">Introduction</h2>
<p>We have introduced most of the concepts in the previous two blogs. In this blog post, we will see how the concepts translate to code. If you want to check out the earlier posts, you can find them here, <a href="/posts/diffusion-models/denoising-diffusion-models-1" >diffusion model intro 1</a>, and <a href="/posts/diffusion-models/denoising-diffusion-models-2" >diffusion model intro 2</a>.</p>
<h2 id="emnist-dataset">EMNIST dataset</h2>
<p><a href="https://www.tensorflow.org/datasets/catalog/emnist" target="_blank" >Extended-MNIST dataset</a>, as the name suggests, is an extension of the popular MNIST dataset. It contains labelled 28*28*1 images of handwritten English characters (upper and lower case) and numbers.</p>
<figure class="align-center ">
    <img loading="lazy" src="/images/emnist-sample.png#center"
         alt="Figure 1: Samples from the EMNIST dataset" width="50%"/> <figcaption>
            <p>Figure 1: Samples from the EMNIST dataset</p>
        </figcaption>
</figure>

<h3 id="loading-data">Loading data</h3>
<p>We will use the Tensorflow datasets library to load the EMNIST dataset. The data will be loaded in batches of size 4*128, we will also normalize the data in the range of 0-1.</p>
<p>A part of this code is adapted from the <a href="https://colab.research.google.com/github/google-research/vdm/blob/main/colab/SimpleDiffusionColab.ipynb" target="_blank" >vdm - simple diffusion example colab notebook</a>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-0-1"><a class="lnlinks" href="#hl-0-1"> 1</a>
</span><span class="lnt" id="hl-0-2"><a class="lnlinks" href="#hl-0-2"> 2</a>
</span><span class="lnt" id="hl-0-3"><a class="lnlinks" href="#hl-0-3"> 3</a>
</span><span class="lnt" id="hl-0-4"><a class="lnlinks" href="#hl-0-4"> 4</a>
</span><span class="lnt" id="hl-0-5"><a class="lnlinks" href="#hl-0-5"> 5</a>
</span><span class="lnt" id="hl-0-6"><a class="lnlinks" href="#hl-0-6"> 6</a>
</span><span class="lnt" id="hl-0-7"><a class="lnlinks" href="#hl-0-7"> 7</a>
</span><span class="lnt" id="hl-0-8"><a class="lnlinks" href="#hl-0-8"> 8</a>
</span><span class="lnt" id="hl-0-9"><a class="lnlinks" href="#hl-0-9"> 9</a>
</span><span class="lnt" id="hl-0-10"><a class="lnlinks" href="#hl-0-10">10</a>
</span><span class="lnt" id="hl-0-11"><a class="lnlinks" href="#hl-0-11">11</a>
</span><span class="lnt" id="hl-0-12"><a class="lnlinks" href="#hl-0-12">12</a>
</span><span class="lnt" id="hl-0-13"><a class="lnlinks" href="#hl-0-13">13</a>
</span><span class="lnt" id="hl-0-14"><a class="lnlinks" href="#hl-0-14">14</a>
</span><span class="lnt" id="hl-0-15"><a class="lnlinks" href="#hl-0-15">15</a>
</span><span class="lnt" id="hl-0-16"><a class="lnlinks" href="#hl-0-16">16</a>
</span><span class="lnt" id="hl-0-17"><a class="lnlinks" href="#hl-0-17">17</a>
</span><span class="lnt" id="hl-0-18"><a class="lnlinks" href="#hl-0-18">18</a>
</span><span class="lnt" id="hl-0-19"><a class="lnlinks" href="#hl-0-19">19</a>
</span><span class="lnt" id="hl-0-20"><a class="lnlinks" href="#hl-0-20">20</a>
</span><span class="lnt" id="hl-0-21"><a class="lnlinks" href="#hl-0-21">21</a>
</span><span class="lnt" id="hl-0-22"><a class="lnlinks" href="#hl-0-22">22</a>
</span><span class="lnt" id="hl-0-23"><a class="lnlinks" href="#hl-0-23">23</a>
</span><span class="lnt" id="hl-0-24"><a class="lnlinks" href="#hl-0-24">24</a>
</span><span class="lnt" id="hl-0-25"><a class="lnlinks" href="#hl-0-25">25</a>
</span><span class="lnt" id="hl-0-26"><a class="lnlinks" href="#hl-0-26">26</a>
</span><span class="lnt" id="hl-0-27"><a class="lnlinks" href="#hl-0-27">27</a>
</span><span class="lnt" id="hl-0-28"><a class="lnlinks" href="#hl-0-28">28</a>
</span><span class="lnt" id="hl-0-29"><a class="lnlinks" href="#hl-0-29">29</a>
</span><span class="lnt" id="hl-0-30"><a class="lnlinks" href="#hl-0-30">30</a>
</span><span class="lnt" id="hl-0-31"><a class="lnlinks" href="#hl-0-31">31</a>
</span><span class="lnt" id="hl-0-32"><a class="lnlinks" href="#hl-0-32">32</a>
</span><span class="lnt" id="hl-0-33"><a class="lnlinks" href="#hl-0-33">33</a>
</span><span class="lnt" id="hl-0-34"><a class="lnlinks" href="#hl-0-34">34</a>
</span><span class="lnt" id="hl-0-35"><a class="lnlinks" href="#hl-0-35">35</a>
</span><span class="lnt" id="hl-0-36"><a class="lnlinks" href="#hl-0-36">36</a>
</span><span class="lnt" id="hl-0-37"><a class="lnlinks" href="#hl-0-37">37</a>
</span><span class="lnt" id="hl-0-38"><a class="lnlinks" href="#hl-0-38">38</a>
</span><span class="lnt" id="hl-0-39"><a class="lnlinks" href="#hl-0-39">39</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">jax</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
</span></span><span class="line"><span class="cl"><span class="c1"># using tensorflow libs to help load the dataset</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="nn">tfds</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</span></span><span class="line"><span class="cl"><span class="n">from</span><span class="err"> </span><span class="n">clu</span><span class="err"> </span><span class="n">import</span><span class="err"> </span><span class="n">deterministic_data</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">dataset_builder</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">builder</span><span class="p">(</span><span class="s1">&#39;emnist&#39;</span><span class="p">,</span> <span class="n">data_dir</span><span class="o">=</span><span class="n">dataset_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dataset_builder</span><span class="o">.</span><span class="n">download_and_prepare</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">train_split</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">split_for_jax_process</span><span class="p">(</span><span class="s1">&#39;train+train&#39;</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">preprocess_fn</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">example</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">],</span> <span class="s1">&#39;float32&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,))</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># normalizing values to 0-1 range</span>
</span></span><span class="line"><span class="cl">  <span class="n">image</span> <span class="o">=</span> <span class="n">image</span> <span class="o">/</span> <span class="mf">255.0</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">example</span><span class="p">[</span><span class="s2">&#34;label&#34;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">128</span> <span class="k">if</span> <span class="n">colab</span> <span class="k">else</span> <span class="mi">64</span>
</span></span><span class="line"><span class="cl"><span class="n">train_ds</span> <span class="o">=</span> <span class="n">deterministic_data</span><span class="o">.</span><span class="n">create_dataset</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">dataset_builder</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">split</span><span class="o">=</span><span class="n">train_split</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">rng</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">shuffle_buffer_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_dims</span><span class="o">=</span><span class="p">[</span><span class="n">jax</span><span class="o">.</span><span class="n">local_device_count</span><span class="p">(),</span> <span class="n">batch_size</span> <span class="o">//</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_count</span><span class="p">()],</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_epochs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">preprocess_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">preprocess_fn</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_input_iter</span><span class="p">(</span><span class="n">ds</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">_prepare</span><span class="p">(</span><span class="n">xs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">_numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">      <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">_f</span><span class="p">,</span> <span class="n">xs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">it</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">_prepare</span><span class="p">,</span> <span class="n">ds</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">it</span> <span class="o">=</span> <span class="n">jax_utils</span><span class="o">.</span><span class="n">prefetch_to_device</span><span class="p">(</span><span class="n">it</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">it</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="utilities-for-visualizing-emnist-data">Utilities for visualizing EMNIST data</h3>
<details>
<summary> Open section on utilities. </summary>
<p>We will be using these utilities to print the images generated from the diffusion models.</p>
<p>A part of this code is adapted from the <a href="https://colab.research.google.com/github/google-research/vdm/blob/main/colab/SimpleDiffusionColab.ipynb" target="_blank" >vdm - simple diffusion example colab notebook</a>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-1-1"><a class="lnlinks" href="#hl-1-1"> 1</a>
</span><span class="lnt" id="hl-1-2"><a class="lnlinks" href="#hl-1-2"> 2</a>
</span><span class="lnt" id="hl-1-3"><a class="lnlinks" href="#hl-1-3"> 3</a>
</span><span class="lnt" id="hl-1-4"><a class="lnlinks" href="#hl-1-4"> 4</a>
</span><span class="lnt" id="hl-1-5"><a class="lnlinks" href="#hl-1-5"> 5</a>
</span><span class="lnt" id="hl-1-6"><a class="lnlinks" href="#hl-1-6"> 6</a>
</span><span class="lnt" id="hl-1-7"><a class="lnlinks" href="#hl-1-7"> 7</a>
</span><span class="lnt" id="hl-1-8"><a class="lnlinks" href="#hl-1-8"> 8</a>
</span><span class="lnt" id="hl-1-9"><a class="lnlinks" href="#hl-1-9"> 9</a>
</span><span class="lnt" id="hl-1-10"><a class="lnlinks" href="#hl-1-10">10</a>
</span><span class="lnt" id="hl-1-11"><a class="lnlinks" href="#hl-1-11">11</a>
</span><span class="lnt" id="hl-1-12"><a class="lnlinks" href="#hl-1-12">12</a>
</span><span class="lnt" id="hl-1-13"><a class="lnlinks" href="#hl-1-13">13</a>
</span><span class="lnt" id="hl-1-14"><a class="lnlinks" href="#hl-1-14">14</a>
</span><span class="lnt" id="hl-1-15"><a class="lnlinks" href="#hl-1-15">15</a>
</span><span class="lnt" id="hl-1-16"><a class="lnlinks" href="#hl-1-16">16</a>
</span><span class="lnt" id="hl-1-17"><a class="lnlinks" href="#hl-1-17">17</a>
</span><span class="lnt" id="hl-1-18"><a class="lnlinks" href="#hl-1-18">18</a>
</span><span class="lnt" id="hl-1-19"><a class="lnlinks" href="#hl-1-19">19</a>
</span><span class="lnt" id="hl-1-20"><a class="lnlinks" href="#hl-1-20">20</a>
</span><span class="lnt" id="hl-1-21"><a class="lnlinks" href="#hl-1-21">21</a>
</span><span class="lnt" id="hl-1-22"><a class="lnlinks" href="#hl-1-22">22</a>
</span><span class="lnt" id="hl-1-23"><a class="lnlinks" href="#hl-1-23">23</a>
</span><span class="lnt" id="hl-1-24"><a class="lnlinks" href="#hl-1-24">24</a>
</span><span class="lnt" id="hl-1-25"><a class="lnlinks" href="#hl-1-25">25</a>
</span><span class="lnt" id="hl-1-26"><a class="lnlinks" href="#hl-1-26">26</a>
</span><span class="lnt" id="hl-1-27"><a class="lnlinks" href="#hl-1-27">27</a>
</span><span class="lnt" id="hl-1-28"><a class="lnlinks" href="#hl-1-28">28</a>
</span><span class="lnt" id="hl-1-29"><a class="lnlinks" href="#hl-1-29">29</a>
</span><span class="lnt" id="hl-1-30"><a class="lnlinks" href="#hl-1-30">30</a>
</span><span class="lnt" id="hl-1-31"><a class="lnlinks" href="#hl-1-31">31</a>
</span><span class="lnt" id="hl-1-32"><a class="lnlinks" href="#hl-1-32">32</a>
</span><span class="lnt" id="hl-1-33"><a class="lnlinks" href="#hl-1-33">33</a>
</span><span class="lnt" id="hl-1-34"><a class="lnlinks" href="#hl-1-34">34</a>
</span><span class="lnt" id="hl-1-35"><a class="lnlinks" href="#hl-1-35">35</a>
</span><span class="lnt" id="hl-1-36"><a class="lnlinks" href="#hl-1-36">36</a>
</span><span class="lnt" id="hl-1-37"><a class="lnlinks" href="#hl-1-37">37</a>
</span><span class="lnt" id="hl-1-38"><a class="lnlinks" href="#hl-1-38">38</a>
</span><span class="lnt" id="hl-1-39"><a class="lnlinks" href="#hl-1-39">39</a>
</span><span class="lnt" id="hl-1-40"><a class="lnlinks" href="#hl-1-40">40</a>
</span><span class="lnt" id="hl-1-41"><a class="lnlinks" href="#hl-1-41">41</a>
</span><span class="lnt" id="hl-1-42"><a class="lnlinks" href="#hl-1-42">42</a>
</span><span class="lnt" id="hl-1-43"><a class="lnlinks" href="#hl-1-43">43</a>
</span><span class="lnt" id="hl-1-44"><a class="lnlinks" href="#hl-1-44">44</a>
</span><span class="lnt" id="hl-1-45"><a class="lnlinks" href="#hl-1-45">45</a>
</span><span class="lnt" id="hl-1-46"><a class="lnlinks" href="#hl-1-46">46</a>
</span><span class="lnt" id="hl-1-47"><a class="lnlinks" href="#hl-1-47">47</a>
</span><span class="lnt" id="hl-1-48"><a class="lnlinks" href="#hl-1-48">48</a>
</span><span class="lnt" id="hl-1-49"><a class="lnlinks" href="#hl-1-49">49</a>
</span><span class="lnt" id="hl-1-50"><a class="lnlinks" href="#hl-1-50">50</a>
</span><span class="lnt" id="hl-1-51"><a class="lnlinks" href="#hl-1-51">51</a>
</span><span class="lnt" id="hl-1-52"><a class="lnlinks" href="#hl-1-52">52</a>
</span><span class="lnt" id="hl-1-53"><a class="lnlinks" href="#hl-1-53">53</a>
</span><span class="lnt" id="hl-1-54"><a class="lnlinks" href="#hl-1-54">54</a>
</span><span class="lnt" id="hl-1-55"><a class="lnlinks" href="#hl-1-55">55</a>
</span><span class="lnt" id="hl-1-56"><a class="lnlinks" href="#hl-1-56">56</a>
</span><span class="lnt" id="hl-1-57"><a class="lnlinks" href="#hl-1-57">57</a>
</span><span class="lnt" id="hl-1-58"><a class="lnlinks" href="#hl-1-58">58</a>
</span><span class="lnt" id="hl-1-59"><a class="lnlinks" href="#hl-1-59">59</a>
</span><span class="lnt" id="hl-1-60"><a class="lnlinks" href="#hl-1-60">60</a>
</span><span class="lnt" id="hl-1-61"><a class="lnlinks" href="#hl-1-61">61</a>
</span><span class="lnt" id="hl-1-62"><a class="lnlinks" href="#hl-1-62">62</a>
</span><span class="lnt" id="hl-1-63"><a class="lnlinks" href="#hl-1-63">63</a>
</span><span class="lnt" id="hl-1-64"><a class="lnlinks" href="#hl-1-64">64</a>
</span><span class="lnt" id="hl-1-65"><a class="lnlinks" href="#hl-1-65">65</a>
</span><span class="lnt" id="hl-1-66"><a class="lnlinks" href="#hl-1-66">66</a>
</span><span class="lnt" id="hl-1-67"><a class="lnlinks" href="#hl-1-67">67</a>
</span><span class="lnt" id="hl-1-68"><a class="lnlinks" href="#hl-1-68">68</a>
</span><span class="lnt" id="hl-1-69"><a class="lnlinks" href="#hl-1-69">69</a>
</span><span class="lnt" id="hl-1-70"><a class="lnlinks" href="#hl-1-70">70</a>
</span><span class="lnt" id="hl-1-71"><a class="lnlinks" href="#hl-1-71">71</a>
</span><span class="lnt" id="hl-1-72"><a class="lnlinks" href="#hl-1-72">72</a>
</span><span class="lnt" id="hl-1-73"><a class="lnlinks" href="#hl-1-73">73</a>
</span><span class="lnt" id="hl-1-74"><a class="lnlinks" href="#hl-1-74">74</a>
</span><span class="lnt" id="hl-1-75"><a class="lnlinks" href="#hl-1-75">75</a>
</span><span class="lnt" id="hl-1-76"><a class="lnlinks" href="#hl-1-76">76</a>
</span><span class="lnt" id="hl-1-77"><a class="lnlinks" href="#hl-1-77">77</a>
</span><span class="lnt" id="hl-1-78"><a class="lnlinks" href="#hl-1-78">78</a>
</span><span class="lnt" id="hl-1-79"><a class="lnlinks" href="#hl-1-79">79</a>
</span><span class="lnt" id="hl-1-80"><a class="lnlinks" href="#hl-1-80">80</a>
</span><span class="lnt" id="hl-1-81"><a class="lnlinks" href="#hl-1-81">81</a>
</span><span class="lnt" id="hl-1-82"><a class="lnlinks" href="#hl-1-82">82</a>
</span><span class="lnt" id="hl-1-83"><a class="lnlinks" href="#hl-1-83">83</a>
</span><span class="lnt" id="hl-1-84"><a class="lnlinks" href="#hl-1-84">84</a>
</span><span class="lnt" id="hl-1-85"><a class="lnlinks" href="#hl-1-85">85</a>
</span><span class="lnt" id="hl-1-86"><a class="lnlinks" href="#hl-1-86">86</a>
</span><span class="lnt" id="hl-1-87"><a class="lnlinks" href="#hl-1-87">87</a>
</span><span class="lnt" id="hl-1-88"><a class="lnlinks" href="#hl-1-88">88</a>
</span><span class="lnt" id="hl-1-89"><a class="lnlinks" href="#hl-1-89">89</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">io</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">math</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display_png</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.cm</span> <span class="k">as</span> <span class="nn">cm</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">imify</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;&#34;&#34;Convert an array to an image.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">  Arguments:
</span></span></span><span class="line"><span class="cl"><span class="s2">    arr : array-like The image data. The shape can be one of MxN (luminance),
</span></span></span><span class="line"><span class="cl"><span class="s2">      MxNx3 (RGB) or MxNx4 (RGBA).
</span></span></span><span class="line"><span class="cl"><span class="s2">    vmin : scalar, optional lower value.
</span></span></span><span class="line"><span class="cl"><span class="s2">    vmax : scalar, optional *vmin* and *vmax* set the color scaling for the
</span></span></span><span class="line"><span class="cl"><span class="s2">      image by fixing the values that map to the colormap color limits. If
</span></span></span><span class="line"><span class="cl"><span class="s2">      either *vmin* or *vmax* is None, that limit is determined from the *arr*
</span></span></span><span class="line"><span class="cl"><span class="s2">      min/max value.
</span></span></span><span class="line"><span class="cl"><span class="s2">    cmap : str or `~matplotlib.colors.Colormap`, optional A Colormap instance or
</span></span></span><span class="line"><span class="cl"><span class="s2">      registered colormap name. The colormap maps scalar data to colors. It is
</span></span></span><span class="line"><span class="cl"><span class="s2">      ignored for RGB(A) data.
</span></span></span><span class="line"><span class="cl"><span class="s2">        Defaults to :rc:`image.cmap` (&#39;viridis&#39;).
</span></span></span><span class="line"><span class="cl"><span class="s2">    origin : {&#39;upper&#39;, &#39;lower&#39;}, optional Indicates whether the ``(0, 0)`` index
</span></span></span><span class="line"><span class="cl"><span class="s2">      of the array is in the upper
</span></span></span><span class="line"><span class="cl"><span class="s2">        left or lower left corner of the axes.  Defaults to :rc:`image.origin`
</span></span></span><span class="line"><span class="cl"><span class="s2">          (&#39;upper&#39;).
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">  Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">    A uint8 image array.
</span></span></span><span class="line"><span class="cl"><span class="s2">  &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="n">sm</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ScalarMappable</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">sm</span><span class="o">.</span><span class="n">set_clim</span><span class="p">(</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">origin</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">origin</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&#34;image.origin&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">origin</span> <span class="o">==</span> <span class="s2">&#34;lower&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">arr</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">rgba</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">to_rgba</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="nb">bytes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">rgba</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">rawarrview</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;&#34;&#34;Visualize an array as if it was an image in colab notebooks.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">  Arguments:
</span></span></span><span class="line"><span class="cl"><span class="s2">    array: an array which will be turned into an image.
</span></span></span><span class="line"><span class="cl"><span class="s2">    **kwargs: Additional keyword arguments passed to imify.
</span></span></span><span class="line"><span class="cl"><span class="s2">  &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="n">f</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="n">imarray</span> <span class="o">=</span> <span class="n">imify</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">plt</span><span class="o">.</span><span class="n">imsave</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">imarray</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&#34;png&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">f</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">dat</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="n">display_png</span><span class="p">(</span><span class="n">dat</span><span class="p">,</span> <span class="n">raw</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">reshape_image_batch</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">cut</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">rows</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;&#34;&#34;Given an array of shape [n, x, y, ...] reshape it to create an image field.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">  Arguments:
</span></span></span><span class="line"><span class="cl"><span class="s2">    array: The array to reshape.
</span></span></span><span class="line"><span class="cl"><span class="s2">    cut: Optional cut on the number of images to view. Will default to whole
</span></span></span><span class="line"><span class="cl"><span class="s2">      array.
</span></span></span><span class="line"><span class="cl"><span class="s2">    rows: Number of rows to use.  Will default to the integer less than the
</span></span></span><span class="line"><span class="cl"><span class="s2">      sqrt.
</span></span></span><span class="line"><span class="cl"><span class="s2">    axis: Axis to interpretate at the batch dimension.  By default the image
</span></span></span><span class="line"><span class="cl"><span class="s2">      dimensions immediately follow.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">  Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">    reshaped_array: An array of shape [rows * x, cut / rows * y, ...]
</span></span></span><span class="line"><span class="cl"><span class="s2">  &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="n">original_shape</span> <span class="o">=</span> <span class="n">array</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">  <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;array must be at least 3 Dimensional.&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">cut</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">cut</span> <span class="o">=</span> <span class="n">original_shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">rows</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">rows</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cut</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">cols</span> <span class="o">=</span> <span class="n">cut</span> <span class="o">//</span> <span class="n">rows</span>
</span></span><span class="line"><span class="cl">  <span class="n">cut</span> <span class="o">=</span> <span class="n">cols</span> <span class="o">*</span> <span class="n">rows</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">leading</span> <span class="o">=</span> <span class="n">original_shape</span><span class="p">[:</span><span class="n">axis</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">x_width</span> <span class="o">=</span> <span class="n">original_shape</span><span class="p">[</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">y_width</span> <span class="o">=</span> <span class="n">original_shape</span><span class="p">[</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">remaining</span> <span class="o">=</span> <span class="n">original_shape</span><span class="p">[</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">3</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">array</span> <span class="o">=</span> <span class="n">array</span><span class="p">[:</span><span class="n">cut</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">array</span> <span class="o">=</span> <span class="n">array</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">leading</span> <span class="o">+</span> <span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">x_width</span><span class="p">,</span> <span class="n">y_width</span><span class="p">)</span> <span class="o">+</span> <span class="n">remaining</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">array</span> <span class="o">=</span> <span class="n">array</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">leading</span> <span class="o">+</span> <span class="p">(</span><span class="n">rows</span> <span class="o">*</span> <span class="n">x_width</span><span class="p">,</span> <span class="n">cols</span> <span class="o">*</span> <span class="n">y_width</span><span class="p">)</span> <span class="o">+</span> <span class="n">remaining</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">array</span>
</span></span></code></pre></td></tr></table>
</div>
</div></details>
<h2 id="setting-up-diffusion-step">Setting up Diffusion Step</h2>
<p>Setting up a diffusion step involves:</p>
<ol>
<li>Defining a schedule for diffusion:  $\alpha$</li>
<li>Defining variables that we will be using during denoising and diffusion steps.</li>
<li>Defining utilities to add noise to an image.</li>
</ol>
<p>You can find more details about diffusion in the <a href="/posts/diffusion-models/denoising-diffusion-models-1/#1-diffusion-step" >earlier post.</a></p>
<p>We are going to use the cosine schedule for diffusion in this example.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-2-1"><a class="lnlinks" href="#hl-2-1"> 1</a>
</span><span class="lnt" id="hl-2-2"><a class="lnlinks" href="#hl-2-2"> 2</a>
</span><span class="lnt" id="hl-2-3"><a class="lnlinks" href="#hl-2-3"> 3</a>
</span><span class="lnt" id="hl-2-4"><a class="lnlinks" href="#hl-2-4"> 4</a>
</span><span class="lnt" id="hl-2-5"><a class="lnlinks" href="#hl-2-5"> 5</a>
</span><span class="lnt" id="hl-2-6"><a class="lnlinks" href="#hl-2-6"> 6</a>
</span><span class="lnt" id="hl-2-7"><a class="lnlinks" href="#hl-2-7"> 7</a>
</span><span class="lnt" id="hl-2-8"><a class="lnlinks" href="#hl-2-8"> 8</a>
</span><span class="lnt" id="hl-2-9"><a class="lnlinks" href="#hl-2-9"> 9</a>
</span><span class="lnt" id="hl-2-10"><a class="lnlinks" href="#hl-2-10">10</a>
</span><span class="lnt" id="hl-2-11"><a class="lnlinks" href="#hl-2-11">11</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">cosine_beta_schedule</span><span class="p">(</span><span class="n">timesteps</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">0.008</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    cosine schedule as proposed in https://arxiv.org/abs/2102.09672
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">steps</span> <span class="o">=</span> <span class="n">timesteps</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="n">steps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">alphas_cumprod</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">cos</span><span class="p">(((</span><span class="n">x</span> <span class="o">/</span> <span class="n">timesteps</span><span class="p">)</span> <span class="o">+</span> <span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">s</span><span class="p">)</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">    <span class="n">alphas_cumprod</span> <span class="o">=</span> <span class="n">alphas_cumprod</span> <span class="o">/</span> <span class="n">alphas_cumprod</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">betas</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">alphas_cumprod</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">/</span> <span class="n">alphas_cumprod</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">	<span class="c1"># clipping this at 0.1 as I have found it difficult to work with higher values of beta.</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Setting up the variables:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-3-1"><a class="lnlinks" href="#hl-3-1"> 1</a>
</span><span class="lnt" id="hl-3-2"><a class="lnlinks" href="#hl-3-2"> 2</a>
</span><span class="lnt" id="hl-3-3"><a class="lnlinks" href="#hl-3-3"> 3</a>
</span><span class="lnt" id="hl-3-4"><a class="lnlinks" href="#hl-3-4"> 4</a>
</span><span class="lnt" id="hl-3-5"><a class="lnlinks" href="#hl-3-5"> 5</a>
</span><span class="lnt" id="hl-3-6"><a class="lnlinks" href="#hl-3-6"> 6</a>
</span><span class="lnt" id="hl-3-7"><a class="lnlinks" href="#hl-3-7"> 7</a>
</span><span class="lnt" id="hl-3-8"><a class="lnlinks" href="#hl-3-8"> 8</a>
</span><span class="lnt" id="hl-3-9"><a class="lnlinks" href="#hl-3-9"> 9</a>
</span><span class="lnt" id="hl-3-10"><a class="lnlinks" href="#hl-3-10">10</a>
</span><span class="lnt" id="hl-3-11"><a class="lnlinks" href="#hl-3-11">11</a>
</span><span class="lnt" id="hl-3-12"><a class="lnlinks" href="#hl-3-12">12</a>
</span><span class="lnt" id="hl-3-13"><a class="lnlinks" href="#hl-3-13">13</a>
</span><span class="lnt" id="hl-3-14"><a class="lnlinks" href="#hl-3-14">14</a>
</span><span class="lnt" id="hl-3-15"><a class="lnlinks" href="#hl-3-15">15</a>
</span><span class="lnt" id="hl-3-16"><a class="lnlinks" href="#hl-3-16">16</a>
</span><span class="lnt" id="hl-3-17"><a class="lnlinks" href="#hl-3-17">17</a>
</span><span class="lnt" id="hl-3-18"><a class="lnlinks" href="#hl-3-18">18</a>
</span><span class="lnt" id="hl-3-19"><a class="lnlinks" href="#hl-3-19">19</a>
</span><span class="lnt" id="hl-3-20"><a class="lnlinks" href="#hl-3-20">20</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">timesteps</span> <span class="o">=</span> <span class="mi">250</span>
</span></span><span class="line"><span class="cl"><span class="n">betas</span> <span class="o">=</span> <span class="n">cosine_beta_schedule</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">alphas</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">betas</span>
</span></span><span class="line"><span class="cl"><span class="n">alphas_</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">variance</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alphas_</span>
</span></span><span class="line"><span class="cl"><span class="n">sd</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variance</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># these variables are used during diffusion and denoising step</span>
</span></span><span class="line"><span class="cl"><span class="n">alphas_prev_</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">alphas_</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&#34;constant&#34;</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sigma_squared_q_t</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas_</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas_prev_</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">log_sigma_squared_q_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alphas</span><span class="p">)</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alphas_</span><span class="p">)</span> <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alphas_prev_</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sigma_squared_q_t_corrected</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_sigma_squared_q_t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## following code here -- we are computing the posterior variance</span>
</span></span><span class="line"><span class="cl"><span class="c1">## https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L196</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L78 </span>
</span></span><span class="line"><span class="cl"><span class="n">log_posterior_variance</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">posterior_variance</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">posterior_variance</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">posterior_variance_corrected</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_posterior_variance</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Let&rsquo;s define the utilities as well, adding noise using the re-parameterization trick. The diffusion step takes in data, time step value and adds noise to the data according to the equation below. It uses the schedule $\alpha$ defined above.</p>

$$
\begin{align}
q_t(x_t|x_0) &= \sqrt{\bar\alpha_t}x_0 + \sqrt{(1 - \bar\alpha_t )}\ast\epsilon_0^\ast ; \space where \space \epsilon_0^\ast \in N(0, I) \cr  
&= N(\sqrt{\bar\alpha_t}x_0, (1 - \bar\alpha_t)I) \cr 
\end{align}
$$    

<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-4-1"><a class="lnlinks" href="#hl-4-1"> 1</a>
</span><span class="lnt" id="hl-4-2"><a class="lnlinks" href="#hl-4-2"> 2</a>
</span><span class="lnt" id="hl-4-3"><a class="lnlinks" href="#hl-4-3"> 3</a>
</span><span class="lnt" id="hl-4-4"><a class="lnlinks" href="#hl-4-4"> 4</a>
</span><span class="lnt" id="hl-4-5"><a class="lnlinks" href="#hl-4-5"> 5</a>
</span><span class="lnt" id="hl-4-6"><a class="lnlinks" href="#hl-4-6"> 6</a>
</span><span class="lnt" id="hl-4-7"><a class="lnlinks" href="#hl-4-7"> 7</a>
</span><span class="lnt" id="hl-4-8"><a class="lnlinks" href="#hl-4-8"> 8</a>
</span><span class="lnt" id="hl-4-9"><a class="lnlinks" href="#hl-4-9"> 9</a>
</span><span class="lnt" id="hl-4-10"><a class="lnlinks" href="#hl-4-10">10</a>
</span><span class="lnt" id="hl-4-11"><a class="lnlinks" href="#hl-4-11">11</a>
</span><span class="lnt" id="hl-4-12"><a class="lnlinks" href="#hl-4-12">12</a>
</span><span class="lnt" id="hl-4-13"><a class="lnlinks" href="#hl-4-13">13</a>
</span><span class="lnt" id="hl-4-14"><a class="lnlinks" href="#hl-4-14">14</a>
</span><span class="lnt" id="hl-4-15"><a class="lnlinks" href="#hl-4-15">15</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># how to add noise to the data</span>
</span></span><span class="line"><span class="cl"><span class="nd">@jax.jit</span> <span class="p">:</span> <span class="n">jit</span> <span class="n">compilazation</span> <span class="n">to</span> <span class="n">significantly</span> <span class="n">speed</span> <span class="n">up</span> <span class="n">code</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_noisy</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">timestep</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">timestep</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">timestep</span><span class="p">,</span> <span class="s1">&#39;b -&gt; b 28 28 1&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># we will use the reparameterization trick</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># need to generate new keys everytime</span>
</span></span><span class="line"><span class="cl">    <span class="n">_</span><span class="p">,</span> <span class="n">noise_key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">noise_at_t</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">noise_key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">added_noise_at_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">batch</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alphas_</span><span class="p">[</span><span class="n">timestep</span><span class="p">]),</span> <span class="n">noise_at_t</span> <span class="o">*</span> <span class="n">sd</span><span class="p">[</span><span class="n">timestep</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">added_noise_at_t</span><span class="p">,</span> <span class="n">noise_at_t</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># recovering original data by removing noise</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">recover_original</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">noise</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">true_data</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">noise</span><span class="o">*</span><span class="n">sd</span><span class="p">[</span><span class="n">timestep</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alphas_</span><span class="p">[</span><span class="n">timestep</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">true_data</span>
</span></span></code></pre></td></tr></table>
</div>
</div><details>
<summary>Take a random data point and add noise over multiple steps. Click to see the code.</summary>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-5-1"><a class="lnlinks" href="#hl-5-1"> 1</a>
</span><span class="lnt" id="hl-5-2"><a class="lnlinks" href="#hl-5-2"> 2</a>
</span><span class="lnt" id="hl-5-3"><a class="lnlinks" href="#hl-5-3"> 3</a>
</span><span class="lnt" id="hl-5-4"><a class="lnlinks" href="#hl-5-4"> 4</a>
</span><span class="lnt" id="hl-5-5"><a class="lnlinks" href="#hl-5-5"> 5</a>
</span><span class="lnt" id="hl-5-6"><a class="lnlinks" href="#hl-5-6"> 6</a>
</span><span class="lnt" id="hl-5-7"><a class="lnlinks" href="#hl-5-7"> 7</a>
</span><span class="lnt" id="hl-5-8"><a class="lnlinks" href="#hl-5-8"> 8</a>
</span><span class="lnt" id="hl-5-9"><a class="lnlinks" href="#hl-5-9"> 9</a>
</span><span class="lnt" id="hl-5-10"><a class="lnlinks" href="#hl-5-10">10</a>
</span><span class="lnt" id="hl-5-11"><a class="lnlinks" href="#hl-5-11">11</a>
</span><span class="lnt" id="hl-5-12"><a class="lnlinks" href="#hl-5-12">12</a>
</span><span class="lnt" id="hl-5-13"><a class="lnlinks" href="#hl-5-13">13</a>
</span><span class="lnt" id="hl-5-14"><a class="lnlinks" href="#hl-5-14">14</a>
</span><span class="lnt" id="hl-5-15"><a class="lnlinks" href="#hl-5-15">15</a>
</span><span class="lnt" id="hl-5-16"><a class="lnlinks" href="#hl-5-16">16</a>
</span><span class="lnt" id="hl-5-17"><a class="lnlinks" href="#hl-5-17">17</a>
</span><span class="lnt" id="hl-5-18"><a class="lnlinks" href="#hl-5-18">18</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">random_index</span> <span class="o">=</span> <span class="mi">22</span>
</span></span><span class="line"><span class="cl"><span class="n">image</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">create_input_iter</span><span class="p">(</span><span class="n">train_ds</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">random_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">ims</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="n">noisy_images</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_noisy</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">einops</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="s1">&#39;h w c -&gt; b h w c&#39;</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">timesteps</span><span class="o">//</span><span class="mi">5</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">colab</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">noisy_images</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">rearrange</span><span class="p">(</span><span class="n">noisy_images</span><span class="p">,</span> <span class="s1">&#39;b h w c -&gt; b h (w c)&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">noisy_images</span> <span class="o">=</span> <span class="n">unnormalize</span><span class="p">(</span><span class="n">noisy_images</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">timesteps</span><span class="o">//</span><span class="mi">10</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">noisy_images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&#34;gray&#34;</span><span class="p">,</span> <span class="n">animated</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">ims</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">im</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">animate</span> <span class="o">=</span> <span class="n">animation</span><span class="o">.</span><span class="n">ArtistAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">ims</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">blit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">repeat_delay</span><span class="o">=</span><span class="mi">3000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">animate</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">gifs_dir</span><span class="o">+</span><span class="s1">&#39;diffusion.gif&#39;</span><span class="p">,</span> <span class="n">writer</span><span class="o">=</span><span class="s1">&#39;pillow&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">with</span><span class="err"> </span><span class="nb">open</span><span class="p">(</span><span class="n">gifs_dir</span><span class="err"> </span><span class="o">+</span><span class="err"> </span><span class="s1">&#39;diffusion.gif&#39;</span><span class="p">,</span><span class="s1">&#39;rb&#39;</span><span class="p">)</span><span class="err"> </span><span class="k">as</span><span class="err"> </span><span class="n">f</span><span class="p">:</span>
</span></span><span class="line"><span class="cl"><span class="err">  </span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(),</span><span class="err"> </span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;png&#39;</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div></details>
<figure class="align-center ">
    <img loading="lazy" src="/images/diffusion-z.gif#center"
         alt="Figure 2: Using the utilities defined above to add Gaussian Noise in multiple steps to a handwritten character &amp;lsquo;z&amp;rsquo;" width="50%"/> <figcaption>
            <p>Figure 2: Using the utilities defined above to add Gaussian Noise in multiple steps to a handwritten character &lsquo;z&rsquo;</p>
        </figcaption>
</figure>

<h2 id="the-denoising-model-u-net">The Denoising Model: U-Net</h2>
<p>In the earlier blog posts, we were working on 2-d samples. The <a href="/posts/diffusion-models/denoising-diffusion-models-1/#neural-network" >denoising model</a> was a multi layer perceptron with GeLU activations. In the case of EMNIST we will have to denoise images. <a href="https://en.wikipedia.org/wiki/U-Net" target="_blank" >U-Nets</a> are the recommended models to do this.</p>
<figure class="align-center ">
    <img loading="lazy" src="/images/u-net.png#center"
         alt="Figure 3: U-Net architecture. It&amp;rsquo;s similar to the one we will build." width="100%"/> <figcaption>
            <p>Figure 3: U-Net architecture. It&rsquo;s similar to the one we will build.</p>
        </figcaption>
</figure>

<p>The U-Net architecture has the following characteristics:</p>
<ul>
<li>It&rsquo;s typically represented as a U block.</li>
<li>The first part of the U block downsamples the image. We go from a 28*28 image to a 7*7 image. This is done using Downsampling convolution blocks.</li>
<li>With downsampling, we increase the number of channels (features). We go from 1 channel in the input to 192 channels at the end of the first part of the U-net.</li>
<li>The 2<sup>nd</sup> part of the U block does the opposite of the 1st part. We go from a 7*7 image to a 28*28 image, and also go from 192 channels to 1 channel. Upsampling is done using Upsampling convolutional blocks.</li>
<li>At the same time, the U-Net has residual connections, connecting layers in the downsampling part to the layers in the upsampling part, this makes training the network efficient.</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-6-1"><a class="lnlinks" href="#hl-6-1"> 1</a>
</span><span class="lnt" id="hl-6-2"><a class="lnlinks" href="#hl-6-2"> 2</a>
</span><span class="lnt" id="hl-6-3"><a class="lnlinks" href="#hl-6-3"> 3</a>
</span><span class="lnt" id="hl-6-4"><a class="lnlinks" href="#hl-6-4"> 4</a>
</span><span class="lnt" id="hl-6-5"><a class="lnlinks" href="#hl-6-5"> 5</a>
</span><span class="lnt" id="hl-6-6"><a class="lnlinks" href="#hl-6-6"> 6</a>
</span><span class="lnt" id="hl-6-7"><a class="lnlinks" href="#hl-6-7"> 7</a>
</span><span class="lnt" id="hl-6-8"><a class="lnlinks" href="#hl-6-8"> 8</a>
</span><span class="lnt" id="hl-6-9"><a class="lnlinks" href="#hl-6-9"> 9</a>
</span><span class="lnt" id="hl-6-10"><a class="lnlinks" href="#hl-6-10">10</a>
</span><span class="lnt" id="hl-6-11"><a class="lnlinks" href="#hl-6-11">11</a>
</span><span class="lnt" id="hl-6-12"><a class="lnlinks" href="#hl-6-12">12</a>
</span><span class="lnt" id="hl-6-13"><a class="lnlinks" href="#hl-6-13">13</a>
</span><span class="lnt" id="hl-6-14"><a class="lnlinks" href="#hl-6-14">14</a>
</span><span class="lnt" id="hl-6-15"><a class="lnlinks" href="#hl-6-15">15</a>
</span><span class="lnt" id="hl-6-16"><a class="lnlinks" href="#hl-6-16">16</a>
</span><span class="lnt" id="hl-6-17"><a class="lnlinks" href="#hl-6-17">17</a>
</span><span class="lnt" id="hl-6-18"><a class="lnlinks" href="#hl-6-18">18</a>
</span><span class="lnt" id="hl-6-19"><a class="lnlinks" href="#hl-6-19">19</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># upsample operation in the UNET</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Downsample</span><span class="p">(</span><span class="n">hk</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">hk</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="c1"># Downsample operation in the UNET</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Upsample</span><span class="p">(</span><span class="n">hk</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">hk</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># scaling image to twice size</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;b h w c -&gt; b (a h) (aa w) c&#39;</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">aa</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>Time Step Embeddings:</strong>
We will be using sinusoidal embeddings for time steps. This is following the <a href="/posts/diffusion-models/denoising-diffusion-models-2/#time-step-embedding" >discussion in the previous post.</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-7-1"><a class="lnlinks" href="#hl-7-1"> 1</a>
</span><span class="lnt" id="hl-7-2"><a class="lnlinks" href="#hl-7-2"> 2</a>
</span><span class="lnt" id="hl-7-3"><a class="lnlinks" href="#hl-7-3"> 3</a>
</span><span class="lnt" id="hl-7-4"><a class="lnlinks" href="#hl-7-4"> 4</a>
</span><span class="lnt" id="hl-7-5"><a class="lnlinks" href="#hl-7-5"> 5</a>
</span><span class="lnt" id="hl-7-6"><a class="lnlinks" href="#hl-7-6"> 6</a>
</span><span class="lnt" id="hl-7-7"><a class="lnlinks" href="#hl-7-7"> 7</a>
</span><span class="lnt" id="hl-7-8"><a class="lnlinks" href="#hl-7-8"> 8</a>
</span><span class="lnt" id="hl-7-9"><a class="lnlinks" href="#hl-7-9"> 9</a>
</span><span class="lnt" id="hl-7-10"><a class="lnlinks" href="#hl-7-10">10</a>
</span><span class="lnt" id="hl-7-11"><a class="lnlinks" href="#hl-7-11">11</a>
</span><span class="lnt" id="hl-7-12"><a class="lnlinks" href="#hl-7-12">12</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">TimeEmbeddings</span><span class="p">(</span><span class="n">hk</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">half_dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">//</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">half_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">half_dim</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="n">embeddings</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span>
</span></span><span class="line"><span class="cl">      <span class="n">embeddings</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">timesteps</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">embeddings</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">embeddings</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">jnp</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">embeddings</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">      <span class="k">return</span> <span class="n">embeddings</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>Conditional Labels:</strong>
EMNIST dataset has 62 labels, 26 characters in lowercase, 26 characters in uppercase, 10 numbers. We add another label to represent the masked label.
We use the Haiku Embed class to generate embedding vectors for these labels.
Labels are fused with the input as explained in the <a href="/posts/diffusion-models/denoising-diffusion-models-2/#guidance--classifier-free-guidance" >previous post</a>.
Instead of fusing label and time step information only at the start, we are fusing it with the input at every Block.</p>
<p><strong>Network Definition:</strong>
After convolution layers, I am using a <cite>BatchNorm<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></cite> layer, using the Haiku BatchNorm module to normalize the values across the batch. This is shown to improve the convergence of deep models.
Using BatchNorm complicates the implementation a bit since, BatchNorm layers need to maintain a buffer states for mean and variance of the activations.
<a href="https://youtu.be/l_3zj6HeWUE" target="_blank" >This video</a> from Yannic is an excellent introduction to the different kind of Normalizations that one can apply to speed up training.</p>
<p>The code below follows the U-net implementation in <a href="https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py#L267" target="_blank" >LucidRains Diffusion model implementation</a>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-8-1"><a class="lnlinks" href="#hl-8-1"> 1</a>
</span><span class="lnt" id="hl-8-2"><a class="lnlinks" href="#hl-8-2"> 2</a>
</span><span class="lnt" id="hl-8-3"><a class="lnlinks" href="#hl-8-3"> 3</a>
</span><span class="lnt" id="hl-8-4"><a class="lnlinks" href="#hl-8-4"> 4</a>
</span><span class="lnt" id="hl-8-5"><a class="lnlinks" href="#hl-8-5"> 5</a>
</span><span class="lnt" id="hl-8-6"><a class="lnlinks" href="#hl-8-6"> 6</a>
</span><span class="lnt" id="hl-8-7"><a class="lnlinks" href="#hl-8-7"> 7</a>
</span><span class="lnt" id="hl-8-8"><a class="lnlinks" href="#hl-8-8"> 8</a>
</span><span class="lnt" id="hl-8-9"><a class="lnlinks" href="#hl-8-9"> 9</a>
</span><span class="lnt" id="hl-8-10"><a class="lnlinks" href="#hl-8-10">10</a>
</span><span class="lnt" id="hl-8-11"><a class="lnlinks" href="#hl-8-11">11</a>
</span><span class="lnt" id="hl-8-12"><a class="lnlinks" href="#hl-8-12">12</a>
</span><span class="lnt" id="hl-8-13"><a class="lnlinks" href="#hl-8-13">13</a>
</span><span class="lnt" id="hl-8-14"><a class="lnlinks" href="#hl-8-14">14</a>
</span><span class="lnt" id="hl-8-15"><a class="lnlinks" href="#hl-8-15">15</a>
</span><span class="lnt" id="hl-8-16"><a class="lnlinks" href="#hl-8-16">16</a>
</span><span class="lnt" id="hl-8-17"><a class="lnlinks" href="#hl-8-17">17</a>
</span><span class="lnt" id="hl-8-18"><a class="lnlinks" href="#hl-8-18">18</a>
</span><span class="lnt" id="hl-8-19"><a class="lnlinks" href="#hl-8-19">19</a>
</span><span class="lnt" id="hl-8-20"><a class="lnlinks" href="#hl-8-20">20</a>
</span><span class="lnt" id="hl-8-21"><a class="lnlinks" href="#hl-8-21">21</a>
</span><span class="lnt" id="hl-8-22"><a class="lnlinks" href="#hl-8-22">22</a>
</span><span class="lnt" id="hl-8-23"><a class="lnlinks" href="#hl-8-23">23</a>
</span><span class="lnt" id="hl-8-24"><a class="lnlinks" href="#hl-8-24">24</a>
</span><span class="lnt" id="hl-8-25"><a class="lnlinks" href="#hl-8-25">25</a>
</span><span class="lnt" id="hl-8-26"><a class="lnlinks" href="#hl-8-26">26</a>
</span><span class="lnt" id="hl-8-27"><a class="lnlinks" href="#hl-8-27">27</a>
</span><span class="lnt" id="hl-8-28"><a class="lnlinks" href="#hl-8-28">28</a>
</span><span class="lnt" id="hl-8-29"><a class="lnlinks" href="#hl-8-29">29</a>
</span><span class="lnt" id="hl-8-30"><a class="lnlinks" href="#hl-8-30">30</a>
</span><span class="lnt" id="hl-8-31"><a class="lnlinks" href="#hl-8-31">31</a>
</span><span class="lnt" id="hl-8-32"><a class="lnlinks" href="#hl-8-32">32</a>
</span><span class="lnt" id="hl-8-33"><a class="lnlinks" href="#hl-8-33">33</a>
</span><span class="lnt" id="hl-8-34"><a class="lnlinks" href="#hl-8-34">34</a>
</span><span class="lnt" id="hl-8-35"><a class="lnlinks" href="#hl-8-35">35</a>
</span><span class="lnt" id="hl-8-36"><a class="lnlinks" href="#hl-8-36">36</a>
</span><span class="lnt" id="hl-8-37"><a class="lnlinks" href="#hl-8-37">37</a>
</span><span class="lnt" id="hl-8-38"><a class="lnlinks" href="#hl-8-38">38</a>
</span><span class="lnt" id="hl-8-39"><a class="lnlinks" href="#hl-8-39">39</a>
</span><span class="lnt" id="hl-8-40"><a class="lnlinks" href="#hl-8-40">40</a>
</span><span class="lnt" id="hl-8-41"><a class="lnlinks" href="#hl-8-41">41</a>
</span><span class="lnt" id="hl-8-42"><a class="lnlinks" href="#hl-8-42">42</a>
</span><span class="lnt" id="hl-8-43"><a class="lnlinks" href="#hl-8-43">43</a>
</span><span class="lnt" id="hl-8-44"><a class="lnlinks" href="#hl-8-44">44</a>
</span><span class="lnt" id="hl-8-45"><a class="lnlinks" href="#hl-8-45">45</a>
</span><span class="lnt" id="hl-8-46"><a class="lnlinks" href="#hl-8-46">46</a>
</span><span class="lnt" id="hl-8-47"><a class="lnlinks" href="#hl-8-47">47</a>
</span><span class="lnt" id="hl-8-48"><a class="lnlinks" href="#hl-8-48">48</a>
</span><span class="lnt" id="hl-8-49"><a class="lnlinks" href="#hl-8-49">49</a>
</span><span class="lnt" id="hl-8-50"><a class="lnlinks" href="#hl-8-50">50</a>
</span><span class="lnt" id="hl-8-51"><a class="lnlinks" href="#hl-8-51">51</a>
</span><span class="lnt" id="hl-8-52"><a class="lnlinks" href="#hl-8-52">52</a>
</span><span class="lnt" id="hl-8-53"><a class="lnlinks" href="#hl-8-53">53</a>
</span><span class="lnt" id="hl-8-54"><a class="lnlinks" href="#hl-8-54">54</a>
</span><span class="lnt" id="hl-8-55"><a class="lnlinks" href="#hl-8-55">55</a>
</span><span class="lnt" id="hl-8-56"><a class="lnlinks" href="#hl-8-56">56</a>
</span><span class="lnt" id="hl-8-57"><a class="lnlinks" href="#hl-8-57">57</a>
</span><span class="lnt" id="hl-8-58"><a class="lnlinks" href="#hl-8-58">58</a>
</span><span class="lnt" id="hl-8-59"><a class="lnlinks" href="#hl-8-59">59</a>
</span><span class="lnt" id="hl-8-60"><a class="lnlinks" href="#hl-8-60">60</a>
</span><span class="lnt" id="hl-8-61"><a class="lnlinks" href="#hl-8-61">61</a>
</span><span class="lnt" id="hl-8-62"><a class="lnlinks" href="#hl-8-62">62</a>
</span><span class="lnt" id="hl-8-63"><a class="lnlinks" href="#hl-8-63">63</a>
</span><span class="lnt" id="hl-8-64"><a class="lnlinks" href="#hl-8-64">64</a>
</span><span class="lnt" id="hl-8-65"><a class="lnlinks" href="#hl-8-65">65</a>
</span><span class="lnt" id="hl-8-66"><a class="lnlinks" href="#hl-8-66">66</a>
</span><span class="lnt" id="hl-8-67"><a class="lnlinks" href="#hl-8-67">67</a>
</span><span class="lnt" id="hl-8-68"><a class="lnlinks" href="#hl-8-68">68</a>
</span><span class="lnt" id="hl-8-69"><a class="lnlinks" href="#hl-8-69">69</a>
</span><span class="lnt" id="hl-8-70"><a class="lnlinks" href="#hl-8-70">70</a>
</span><span class="lnt" id="hl-8-71"><a class="lnlinks" href="#hl-8-71">71</a>
</span><span class="lnt" id="hl-8-72"><a class="lnlinks" href="#hl-8-72">72</a>
</span><span class="lnt" id="hl-8-73"><a class="lnlinks" href="#hl-8-73">73</a>
</span><span class="lnt" id="hl-8-74"><a class="lnlinks" href="#hl-8-74">74</a>
</span><span class="lnt" id="hl-8-75"><a class="lnlinks" href="#hl-8-75">75</a>
</span><span class="lnt" id="hl-8-76"><a class="lnlinks" href="#hl-8-76">76</a>
</span><span class="lnt" id="hl-8-77"><a class="lnlinks" href="#hl-8-77">77</a>
</span><span class="lnt" id="hl-8-78"><a class="lnlinks" href="#hl-8-78">78</a>
</span><span class="lnt" id="hl-8-79"><a class="lnlinks" href="#hl-8-79">79</a>
</span><span class="lnt" id="hl-8-80"><a class="lnlinks" href="#hl-8-80">80</a>
</span><span class="lnt" id="hl-8-81"><a class="lnlinks" href="#hl-8-81">81</a>
</span><span class="lnt" id="hl-8-82"><a class="lnlinks" href="#hl-8-82">82</a>
</span><span class="lnt" id="hl-8-83"><a class="lnlinks" href="#hl-8-83">83</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Unet class to predict noise from a given image</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">UNet</span><span class="p">(</span><span class="n">hk</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">init_conv</span> <span class="o">=</span> <span class="n">hk</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="mi">48</span><span class="p">,</span> <span class="n">kernel_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">hk</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">silu</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">silu</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">block1</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="mi">48</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">downsample1</span> <span class="o">=</span> <span class="n">Downsample</span><span class="p">(</span><span class="mi">96</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">block2</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="mi">96</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">downsample2</span> <span class="o">=</span> <span class="n">Downsample</span><span class="p">(</span><span class="mi">192</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">middle_block</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="mi">192</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">upsample1</span> <span class="o">=</span> <span class="n">Upsample</span><span class="p">(</span><span class="mi">96</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">block3</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="mi">96</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">upsample2</span> <span class="o">=</span> <span class="n">Upsample</span><span class="p">(</span><span class="mi">48</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">block4</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="mi">48</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">hk</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="mi">48</span><span class="p">,</span> <span class="n">kernel_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">hk</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">hk</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">time_mlp</span> <span class="o">=</span> <span class="n">hk</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">      <span class="n">hk</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">      <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">gelu</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">hk</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># conditional vectors encoding</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">embedding_vectors</span> <span class="o">=</span> <span class="n">hk</span><span class="o">.</span><span class="n">Embed</span><span class="p">(</span><span class="mi">10</span><span class="o">+</span><span class="mi">26</span><span class="o">+</span><span class="mi">26</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">63</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">timestep_embeddings</span> <span class="o">=</span> <span class="n">TimeEmbeddings</span><span class="p">(</span><span class="mi">96</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="n">cond</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">cond_embedding</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">conditioning</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">timesteps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">timestep_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">timestep_embeddings</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">conditioning</span> <span class="o">=</span> <span class="n">timestep_embeddings</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">cond</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">label_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_vectors</span><span class="p">(</span><span class="n">cond</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">conditioning</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">label_embeddings</span><span class="p">,</span> <span class="n">conditioning</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">conditioning</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  
</span></span><span class="line"><span class="cl">      <span class="n">cond_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_mlp</span><span class="p">(</span><span class="n">conditioning</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_conv</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">is_training</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">xx</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">cond_embedding</span><span class="p">,</span> <span class="n">is_training</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample1</span><span class="p">(</span><span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">b2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block2</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">cond_embedding</span><span class="p">,</span> <span class="n">is_training</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample2</span><span class="p">(</span><span class="n">b2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">middle_block</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">cond_embedding</span><span class="p">,</span> <span class="n">is_training</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">b3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block3</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">h</span><span class="p">,</span> <span class="n">b2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span> <span class="n">cond_embedding</span><span class="p">,</span> <span class="n">is_training</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample2</span><span class="p">(</span><span class="n">b3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">b4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block4</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">h</span><span class="p">,</span> <span class="n">b1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span> <span class="n">cond_embedding</span><span class="p">,</span> <span class="n">is_training</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">xx</span><span class="p">,</span> <span class="n">b4</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)),</span> <span class="n">is_training</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">h</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">hk</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># a basic resnet style convolutional block</span>
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">hk</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_shape</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># using batch norm instead of layernorm as the batch sizes are large</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># orig: self.norm = hk.LayerNorm(axis=(-3, -2, -1), create_scale=True, create_offset=True)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">hk</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">silu</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">silu</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">hk</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_shape</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">hk</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">out_conv</span> <span class="o">=</span> <span class="n">hk</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">    <span class="c1"># self.time_mlp = None</span>
</span></span><span class="line"><span class="cl">    <span class="n">dims</span> <span class="o">=</span> <span class="n">output_channels</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">time_mlp</span> <span class="o">=</span> <span class="n">hk</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">      <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">silu</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">hk</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dims</span><span class="o">*</span><span class="mi">2</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">])</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">timestep_embeddings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">is_training</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">timestep_embeddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_mlp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">time_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_mlp</span><span class="p">(</span><span class="n">timestep_embeddings</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">time_embedding</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">rearrange</span><span class="p">(</span><span class="n">time_embedding</span><span class="p">,</span> <span class="s1">&#39;b c -&gt; b 1 1 c&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">shift</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">time_embedding</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">h</span> <span class="o">=</span> <span class="n">shift</span> <span class="o">+</span> <span class="p">(</span><span class="n">scale</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">h</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="n">h</span><span class="p">)),</span> <span class="n">is_training</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">h</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="training-code">Training Code:</h2>
<p>Loss function: I am using the Huber loss instead of using the L2 or the L1 loss. I would assume changing the loss function wouldn&rsquo;t impact the results significantly.</p>
<p>Note: the importance weight determines the importance of the sample for denoising. As noted in <cite>Improved Denoising Diffusion Probabilistic Models<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></cite>, one idea to train diffusion models is to completely ignore this term. This helps the model training.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-9-1"><a class="lnlinks" href="#hl-9-1"> 1</a>
</span><span class="lnt" id="hl-9-2"><a class="lnlinks" href="#hl-9-2"> 2</a>
</span><span class="lnt" id="hl-9-3"><a class="lnlinks" href="#hl-9-3"> 3</a>
</span><span class="lnt" id="hl-9-4"><a class="lnlinks" href="#hl-9-4"> 4</a>
</span><span class="lnt" id="hl-9-5"><a class="lnlinks" href="#hl-9-5"> 5</a>
</span><span class="lnt" id="hl-9-6"><a class="lnlinks" href="#hl-9-6"> 6</a>
</span><span class="lnt" id="hl-9-7"><a class="lnlinks" href="#hl-9-7"> 7</a>
</span><span class="lnt" id="hl-9-8"><a class="lnlinks" href="#hl-9-8"> 8</a>
</span><span class="lnt" id="hl-9-9"><a class="lnlinks" href="#hl-9-9"> 9</a>
</span><span class="lnt" id="hl-9-10"><a class="lnlinks" href="#hl-9-10">10</a>
</span><span class="lnt" id="hl-9-11"><a class="lnlinks" href="#hl-9-11">11</a>
</span><span class="lnt" id="hl-9-12"><a class="lnlinks" href="#hl-9-12">12</a>
</span><span class="lnt" id="hl-9-13"><a class="lnlinks" href="#hl-9-13">13</a>
</span><span class="lnt" id="hl-9-14"><a class="lnlinks" href="#hl-9-14">14</a>
</span><span class="lnt" id="hl-9-15"><a class="lnlinks" href="#hl-9-15">15</a>
</span><span class="lnt" id="hl-9-16"><a class="lnlinks" href="#hl-9-16">16</a>
</span><span class="lnt" id="hl-9-17"><a class="lnlinks" href="#hl-9-17">17</a>
</span><span class="lnt" id="hl-9-18"><a class="lnlinks" href="#hl-9-18">18</a>
</span><span class="lnt" id="hl-9-19"><a class="lnlinks" href="#hl-9-19">19</a>
</span><span class="lnt" id="hl-9-20"><a class="lnlinks" href="#hl-9-20">20</a>
</span><span class="lnt" id="hl-9-21"><a class="lnlinks" href="#hl-9-21">21</a>
</span><span class="lnt" id="hl-9-22"><a class="lnlinks" href="#hl-9-22">22</a>
</span><span class="lnt" id="hl-9-23"><a class="lnlinks" href="#hl-9-23">23</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># using jax.jit to speed up computation of the loss function</span>
</span></span><span class="line"><span class="cl"><span class="n">partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span>  <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,))</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="n">hk</span><span class="o">.</span><span class="n">Params</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">hk</span><span class="o">.</span><span class="n">State</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Batch</span><span class="p">,</span> <span class="n">is_energy_method</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">hk</span><span class="o">.</span><span class="n">State</span><span class="p">]]:</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;&#34;&#34;Compute the loss of the network, including L2.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="n">x</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">noise</span> <span class="o">=</span> <span class="n">batch</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># not capturing state as it is not needed; it should be internally updated and maintaing by haiku and doesn&#39;t need gradient updates </span>
</span></span><span class="line"><span class="cl">  <span class="n">pred_data</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">is_training</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">error_func</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">imp_weight</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="c1"># 1/2 * (1/sigma_squared_q_t_corrected[timestep]) * ((betas[timestep])**2 / (variance[timestep] * alphas[timestep]))</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># loss on prediction</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss_</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">imp_weight</span><span class="p">,</span> <span class="n">huber_loss</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="n">pred_data</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">loss_</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">energy_func</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="c1">## Energy function interpretation</span>
</span></span><span class="line"><span class="cl">    <span class="n">imp_weight</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="c1"># 1/2 * (1/sigma_squared_q_t_corrected[timestep]) * ((betas[timestep])**2 / (alphas[timestep]))</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># loss on prediction</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss_</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">imp_weight</span><span class="p">,</span> <span class="n">huber_loss</span><span class="p">(</span><span class="n">pred_data</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="o">-</span><span class="n">sd</span><span class="p">[</span><span class="n">timestep</span><span class="p">]))))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">loss_</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">  <span class="n">loss_</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">is_energy_method</span><span class="p">,</span> <span class="n">energy_func</span><span class="p">,</span> <span class="n">error_func</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">loss_</span><span class="p">,</span> <span class="p">(</span><span class="n">loss_</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>Updating Model Weights:</strong></p>
<p>This is a typical code for any Neural Network training in Haiku/Optax and JAX. We are finding the gradient of the loss with respect to the parameters of the Neural Network using JAX and updating the weights using Optax.</p>
<p>Additionally, we are doing exponential updates to the parameters.<cite> Paper on Polyak averaging.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></cite></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-10-1"><a class="lnlinks" href="#hl-10-1"> 1</a>
</span><span class="lnt" id="hl-10-2"><a class="lnlinks" href="#hl-10-2"> 2</a>
</span><span class="lnt" id="hl-10-3"><a class="lnlinks" href="#hl-10-3"> 3</a>
</span><span class="lnt" id="hl-10-4"><a class="lnlinks" href="#hl-10-4"> 4</a>
</span><span class="lnt" id="hl-10-5"><a class="lnlinks" href="#hl-10-5"> 5</a>
</span><span class="lnt" id="hl-10-6"><a class="lnlinks" href="#hl-10-6"> 6</a>
</span><span class="lnt" id="hl-10-7"><a class="lnlinks" href="#hl-10-7"> 7</a>
</span><span class="lnt" id="hl-10-8"><a class="lnlinks" href="#hl-10-8"> 8</a>
</span><span class="lnt" id="hl-10-9"><a class="lnlinks" href="#hl-10-9"> 9</a>
</span><span class="lnt" id="hl-10-10"><a class="lnlinks" href="#hl-10-10">10</a>
</span><span class="lnt" id="hl-10-11"><a class="lnlinks" href="#hl-10-11">11</a>
</span><span class="lnt" id="hl-10-12"><a class="lnlinks" href="#hl-10-12">12</a>
</span><span class="lnt" id="hl-10-13"><a class="lnlinks" href="#hl-10-13">13</a>
</span><span class="lnt" id="hl-10-14"><a class="lnlinks" href="#hl-10-14">14</a>
</span><span class="lnt" id="hl-10-15"><a class="lnlinks" href="#hl-10-15">15</a>
</span><span class="lnt" id="hl-10-16"><a class="lnlinks" href="#hl-10-16">16</a>
</span><span class="lnt" id="hl-10-17"><a class="lnlinks" href="#hl-10-17">17</a>
</span><span class="lnt" id="hl-10-18"><a class="lnlinks" href="#hl-10-18">18</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@jax.jit</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">update</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">params</span><span class="p">:</span> <span class="n">hk</span><span class="o">.</span><span class="n">Params</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">state</span><span class="p">:</span> <span class="n">hk</span><span class="o">.</span><span class="n">State</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">opt_state</span><span class="p">:</span> <span class="n">optax</span><span class="o">.</span><span class="n">OptState</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch</span><span class="p">:</span> <span class="n">Batch</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">is_energy_method</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">hk</span><span class="o">.</span><span class="n">Params</span><span class="p">,</span> <span class="n">optax</span><span class="o">.</span><span class="n">OptState</span><span class="p">,</span> <span class="n">hk</span><span class="o">.</span><span class="n">State</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;&#34;&#34;Compute gradients and update the weights&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="n">grads</span><span class="p">,</span> <span class="p">(</span><span class="n">loss_value</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span><span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">compute_loss</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">is_energy_method</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">updates</span><span class="p">,</span> <span class="n">opt_state</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">new_params</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">apply_updates</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">loss_value</span><span class="p">,</span> <span class="n">new_params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">state</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@jax.jit</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">ema_update</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">avg_params</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;&#34;&#34;Incrementally update parameters via polyak averaging.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Polyak averaging tracks an (exponential moving) average of the past parameters of a model, for use at test/evaluation time.</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">optax</span><span class="o">.</span><span class="n">incremental_update</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">avg_params</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="training-code-1">Training Code</h3>
<p>The training code is pretty straight-forward. Below, I have removed the code to checkpoint the different models.</p>
<p><strong>Pseudocode we are going for:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-11-1"><a class="lnlinks" href="#hl-11-1"> 1</a>
</span><span class="lnt" id="hl-11-2"><a class="lnlinks" href="#hl-11-2"> 2</a>
</span><span class="lnt" id="hl-11-3"><a class="lnlinks" href="#hl-11-3"> 3</a>
</span><span class="lnt" id="hl-11-4"><a class="lnlinks" href="#hl-11-4"> 4</a>
</span><span class="lnt" id="hl-11-5"><a class="lnlinks" href="#hl-11-5"> 5</a>
</span><span class="lnt" id="hl-11-6"><a class="lnlinks" href="#hl-11-6"> 6</a>
</span><span class="lnt" id="hl-11-7"><a class="lnlinks" href="#hl-11-7"> 7</a>
</span><span class="lnt" id="hl-11-8"><a class="lnlinks" href="#hl-11-8"> 8</a>
</span><span class="lnt" id="hl-11-9"><a class="lnlinks" href="#hl-11-9"> 9</a>
</span><span class="lnt" id="hl-11-10"><a class="lnlinks" href="#hl-11-10">10</a>
</span><span class="lnt" id="hl-11-11"><a class="lnlinks" href="#hl-11-11">11</a>
</span><span class="lnt" id="hl-11-12"><a class="lnlinks" href="#hl-11-12">12</a>
</span><span class="lnt" id="hl-11-13"><a class="lnlinks" href="#hl-11-13">13</a>
</span><span class="lnt" id="hl-11-14"><a class="lnlinks" href="#hl-11-14">14</a>
</span><span class="lnt" id="hl-11-15"><a class="lnlinks" href="#hl-11-15">15</a>
</span><span class="lnt" id="hl-11-16"><a class="lnlinks" href="#hl-11-16">16</a>
</span><span class="lnt" id="hl-11-17"><a class="lnlinks" href="#hl-11-17">17</a>
</span><span class="lnt" id="hl-11-18"><a class="lnlinks" href="#hl-11-18">18</a>
</span><span class="lnt" id="hl-11-19"><a class="lnlinks" href="#hl-11-19">19</a>
</span><span class="lnt" id="hl-11-20"><a class="lnlinks" href="#hl-11-20">20</a>
</span><span class="lnt" id="hl-11-21"><a class="lnlinks" href="#hl-11-21">21</a>
</span><span class="lnt" id="hl-11-22"><a class="lnlinks" href="#hl-11-22">22</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">diffusion</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="n">code</span> <span class="n">to</span> <span class="n">add</span> <span class="n">noise</span> <span class="n">to</span> <span class="n">x_0</span>
</span></span><span class="line"><span class="cl">	<span class="k">return</span> <span class="n">x_i</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">training</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="n">loop</span> <span class="n">until</span> <span class="n">convergence</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">		<span class="n">pick</span> <span class="n">an</span> <span class="n">image</span> <span class="n">x_0</span> <span class="kn">from</span> <span class="nn">X</span> <span class="p">(</span><span class="n">batch</span> <span class="n">of</span> <span class="n">images</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">sample</span> <span class="n">t</span> <span class="kn">from</span> <span class="mi">1</span> <span class="n">to</span> <span class="n">T</span>
</span></span><span class="line"><span class="cl">		<span class="n">x_t</span> <span class="o">=</span> <span class="n">diffusion_step</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">x_hat_t</span> <span class="o">=</span> <span class="n">NN</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># y is the label</span>
</span></span><span class="line"><span class="cl">		<span class="n">loss_</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">x_hat_t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">update</span><span class="p">(</span><span class="n">NN</span><span class="p">,</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss_</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># generating new data points through denoising steps</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_new_data</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">	<span class="n">sample</span> <span class="n">x_T</span> <span class="kn">from</span> <span class="nn">N</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">I</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># denoising step</span>
</span></span><span class="line"><span class="cl">		<span class="n">x_hat_t</span> <span class="o">=</span> <span class="n">NN</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># y is the desired label </span>
</span></span><span class="line"><span class="cl">		<span class="c1"># get x_{t-1}</span>
</span></span><span class="line"><span class="cl">		<span class="n">x_</span><span class="p">{</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">x_hat_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="c1"># refer EQ - 12</span>
</span></span><span class="line"><span class="cl">	<span class="n">x_hat_0</span> <span class="o">=</span> <span class="n">x_0</span>  
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-12-1"><a class="lnlinks" href="#hl-12-1"> 1</a>
</span><span class="lnt" id="hl-12-2"><a class="lnlinks" href="#hl-12-2"> 2</a>
</span><span class="lnt" id="hl-12-3"><a class="lnlinks" href="#hl-12-3"> 3</a>
</span><span class="lnt" id="hl-12-4"><a class="lnlinks" href="#hl-12-4"> 4</a>
</span><span class="lnt" id="hl-12-5"><a class="lnlinks" href="#hl-12-5"> 5</a>
</span><span class="lnt" id="hl-12-6"><a class="lnlinks" href="#hl-12-6"> 6</a>
</span><span class="lnt" id="hl-12-7"><a class="lnlinks" href="#hl-12-7"> 7</a>
</span><span class="lnt" id="hl-12-8"><a class="lnlinks" href="#hl-12-8"> 8</a>
</span><span class="lnt" id="hl-12-9"><a class="lnlinks" href="#hl-12-9"> 9</a>
</span><span class="lnt" id="hl-12-10"><a class="lnlinks" href="#hl-12-10">10</a>
</span><span class="lnt" id="hl-12-11"><a class="lnlinks" href="#hl-12-11">11</a>
</span><span class="lnt" id="hl-12-12"><a class="lnlinks" href="#hl-12-12">12</a>
</span><span class="lnt" id="hl-12-13"><a class="lnlinks" href="#hl-12-13">13</a>
</span><span class="lnt" id="hl-12-14"><a class="lnlinks" href="#hl-12-14">14</a>
</span><span class="lnt" id="hl-12-15"><a class="lnlinks" href="#hl-12-15">15</a>
</span><span class="lnt" id="hl-12-16"><a class="lnlinks" href="#hl-12-16">16</a>
</span><span class="lnt" id="hl-12-17"><a class="lnlinks" href="#hl-12-17">17</a>
</span><span class="lnt" id="hl-12-18"><a class="lnlinks" href="#hl-12-18">18</a>
</span><span class="lnt" id="hl-12-19"><a class="lnlinks" href="#hl-12-19">19</a>
</span><span class="lnt" id="hl-12-20"><a class="lnlinks" href="#hl-12-20">20</a>
</span><span class="lnt" id="hl-12-21"><a class="lnlinks" href="#hl-12-21">21</a>
</span><span class="lnt" id="hl-12-22"><a class="lnlinks" href="#hl-12-22">22</a>
</span><span class="lnt" id="hl-12-23"><a class="lnlinks" href="#hl-12-23">23</a>
</span><span class="lnt" id="hl-12-24"><a class="lnlinks" href="#hl-12-24">24</a>
</span><span class="lnt" id="hl-12-25"><a class="lnlinks" href="#hl-12-25">25</a>
</span><span class="lnt" id="hl-12-26"><a class="lnlinks" href="#hl-12-26">26</a>
</span><span class="lnt" id="hl-12-27"><a class="lnlinks" href="#hl-12-27">27</a>
</span><span class="lnt" id="hl-12-28"><a class="lnlinks" href="#hl-12-28">28</a>
</span><span class="lnt" id="hl-12-29"><a class="lnlinks" href="#hl-12-29">29</a>
</span><span class="lnt" id="hl-12-30"><a class="lnlinks" href="#hl-12-30">30</a>
</span><span class="lnt" id="hl-12-31"><a class="lnlinks" href="#hl-12-31">31</a>
</span><span class="lnt" id="hl-12-32"><a class="lnlinks" href="#hl-12-32">32</a>
</span><span class="lnt" id="hl-12-33"><a class="lnlinks" href="#hl-12-33">33</a>
</span><span class="lnt" id="hl-12-34"><a class="lnlinks" href="#hl-12-34">34</a>
</span><span class="lnt" id="hl-12-35"><a class="lnlinks" href="#hl-12-35">35</a>
</span><span class="lnt" id="hl-12-36"><a class="lnlinks" href="#hl-12-36">36</a>
</span><span class="lnt" id="hl-12-37"><a class="lnlinks" href="#hl-12-37">37</a>
</span><span class="lnt" id="hl-12-38"><a class="lnlinks" href="#hl-12-38">38</a>
</span><span class="lnt" id="hl-12-39"><a class="lnlinks" href="#hl-12-39">39</a>
</span><span class="lnt" id="hl-12-40"><a class="lnlinks" href="#hl-12-40">40</a>
</span><span class="lnt" id="hl-12-41"><a class="lnlinks" href="#hl-12-41">41</a>
</span><span class="lnt" id="hl-12-42"><a class="lnlinks" href="#hl-12-42">42</a>
</span><span class="lnt" id="hl-12-43"><a class="lnlinks" href="#hl-12-43">43</a>
</span><span class="lnt" id="hl-12-44"><a class="lnlinks" href="#hl-12-44">44</a>
</span><span class="lnt" id="hl-12-45"><a class="lnlinks" href="#hl-12-45">45</a>
</span><span class="lnt" id="hl-12-46"><a class="lnlinks" href="#hl-12-46">46</a>
</span><span class="lnt" id="hl-12-47"><a class="lnlinks" href="#hl-12-47">47</a>
</span><span class="lnt" id="hl-12-48"><a class="lnlinks" href="#hl-12-48">48</a>
</span><span class="lnt" id="hl-12-49"><a class="lnlinks" href="#hl-12-49">49</a>
</span><span class="lnt" id="hl-12-50"><a class="lnlinks" href="#hl-12-50">50</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># initialization</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">is_training</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">unet</span> <span class="o">=</span> <span class="n">UNet</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">unet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">is_training</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">f_t</span> <span class="o">=</span> <span class="n">hk</span><span class="o">.</span><span class="n">transform_with_state</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">net</span> <span class="o">=</span> <span class="n">hk</span><span class="o">.</span><span class="n">without_apply_rng</span><span class="p">(</span><span class="n">f_t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">params</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">image</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="n">batch_size</span><span class="p">],</span> <span class="n">timesteps_</span><span class="p">,</span> <span class="n">label</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="n">batch_size</span><span class="p">],</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">opt</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">avg_params</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">opt_state</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">batches_iter</span> <span class="o">=</span> <span class="mi">10000</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># maintaining a batch on which we will measure loss; we will save the model based on performance on this batch</span>
</span></span><span class="line"><span class="cl"><span class="n">one_timestep</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mod</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">timesteps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">train</span> <span class="o">=</span> <span class="n">create_input_iter</span><span class="p">(</span><span class="n">train_ds</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="n">data_in_batch_</span><span class="p">,</span> <span class="n">label_</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">data_in_batch_</span> <span class="o">=</span> <span class="n">data_in_batch_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">label_</span> <span class="o">=</span> <span class="n">label_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">data_noisy_temp_</span><span class="p">,</span> <span class="n">noise_temp_</span> <span class="o">=</span> <span class="n">get_noisy</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">data_in_batch_</span><span class="p">,</span> <span class="n">one_timestep</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># main method for training</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">opt_state</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">avg_params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">energy_method</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">best_loss</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">float_info</span><span class="o">.</span><span class="n">max</span> <span class="c1"># initialization   </span>
</span></span><span class="line"><span class="cl">	<span class="n">unique_key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">fold_in</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	
</span></span><span class="line"><span class="cl">	<span class="c1"># same subkey being used for noise sampling, as it doesn&#39;t matter :)</span>
</span></span><span class="line"><span class="cl">	<span class="n">_</span><span class="p">,</span> <span class="o">*</span><span class="n">timestep_subkeys</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">unique_key</span><span class="p">,</span> <span class="n">batches_iter</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	
</span></span><span class="line"><span class="cl">	<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">batches_iter</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="n">data_in_batch</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">data_in_batch</span> <span class="o">=</span> <span class="n">data_in_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">		<span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">		<span class="n">idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">timestep_subkeys</span><span class="p">[</span><span class="n">iteration</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">timesteps</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">idx</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">rearrange</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;a b -&gt; (a b)&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">timestep</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">		<span class="n">data_noisy</span><span class="p">,</span> <span class="n">noise</span> <span class="o">=</span> <span class="n">get_noisy</span><span class="p">(</span><span class="n">timestep_subkeys</span><span class="p">[</span><span class="n">iteration</span><span class="p">],</span> <span class="n">data_in_batch</span><span class="p">,</span> <span class="n">timestep</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># todo: call gradient update function here</span>
</span></span><span class="line"><span class="cl">		<span class="n">loss_value</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">update</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="p">[</span><span class="n">data_noisy</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">noise</span><span class="p">],</span> <span class="n">energy_method</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">avg_params</span> <span class="o">=</span> <span class="n">ema_update</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">avg_params</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		                
</span></span><span class="line"><span class="cl">		<span class="c1">## evaluating noise on a fixed timestep to calculate best model</span>
</span></span><span class="line"><span class="cl">		<span class="n">loss_temp</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_get</span><span class="p">(</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">avg_params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="p">[</span><span class="n">data_noisy_temp_</span><span class="p">,</span> <span class="n">label_</span><span class="p">,</span> <span class="n">one_timestep</span><span class="p">,</span> <span class="n">noise_temp_</span><span class="p">],</span> <span class="n">energy_method</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">		<span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_temp</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="n">loss_temp</span> <span class="o">&lt;</span> <span class="n">best_loss</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">			<span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss_temp</span>
</span></span><span class="line"><span class="cl">			<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;saving iteration: </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2"> loss: </span><span class="si">{</span><span class="n">best_loss</span><span class="si">:</span><span class="s2">&gt;7f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">return</span> <span class="n">data_noisy</span><span class="p">,</span> <span class="n">data_in_batch</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">avg_params</span><span class="p">,</span> <span class="n">state</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="generating-conditional-samples">Generating Conditional Samples</h2>
<h3 id="using-all-time-steps-for-generation">Using all time steps for generation</h3>
<p>Code for generating samples is below. We start off with random samples from a Standard Gaussian Distribution and follow steps as described <a href="/posts/diffusion-models/denoising-diffusion-models-1/#2-denoising-step" >here</a>.
We will be using the naïve version of classifier guidance described <a href="/posts/diffusion-models/denoising-diffusion-models-2/#guidance--classifier-free-guidance" >here</a>.</p>
<ul>
<li>Start by sampling a random image from a Sandard Gaussian distribution at time step $T$.</li>
<li>Get an estimate of the error added to the input image from the U-net.</li>
<li>Use the equation 5, in the link above, to calculate the image at the previous time step $T-1$.</li>
<li>Repeat until time step 1.</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-13-1"><a class="lnlinks" href="#hl-13-1">1</a>
</span><span class="lnt" id="hl-13-2"><a class="lnlinks" href="#hl-13-2">2</a>
</span><span class="lnt" id="hl-13-3"><a class="lnlinks" href="#hl-13-3">3</a>
</span><span class="lnt" id="hl-13-4"><a class="lnlinks" href="#hl-13-4">4</a>
</span><span class="lnt" id="hl-13-5"><a class="lnlinks" href="#hl-13-5">5</a>
</span><span class="lnt" id="hl-13-6"><a class="lnlinks" href="#hl-13-6">6</a>
</span><span class="lnt" id="hl-13-7"><a class="lnlinks" href="#hl-13-7">7</a>
</span><span class="lnt" id="hl-13-8"><a class="lnlinks" href="#hl-13-8">8</a>
</span><span class="lnt" id="hl-13-9"><a class="lnlinks" href="#hl-13-9">9</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># defining useful variables</span>
</span></span><span class="line"><span class="cl"><span class="n">alphas_prev_</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">alphas_</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&#34;constant&#34;</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sigma_squared_q_t</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas_</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas_prev_</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">log_sigma_squared_q_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alphas</span><span class="p">)</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alphas_</span><span class="p">)</span> <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alphas_prev_</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sigma_squared_q_t_corrected</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_sigma_squared_q_t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">mean_coeff_1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alphas</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas_prev_</span><span class="p">)</span> <span class="o">/</span> <span class="n">variance</span>
</span></span><span class="line"><span class="cl"><span class="n">mean_coeff_2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alphas_prev_</span><span class="p">)</span> <span class="o">*</span> <span class="n">betas</span> <span class="o">/</span> <span class="n">variance</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>Generate Samples:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-14-1"><a class="lnlinks" href="#hl-14-1"> 1</a>
</span><span class="lnt" id="hl-14-2"><a class="lnlinks" href="#hl-14-2"> 2</a>
</span><span class="lnt" id="hl-14-3"><a class="lnlinks" href="#hl-14-3"> 3</a>
</span><span class="lnt" id="hl-14-4"><a class="lnlinks" href="#hl-14-4"> 4</a>
</span><span class="lnt" id="hl-14-5"><a class="lnlinks" href="#hl-14-5"> 5</a>
</span><span class="lnt" id="hl-14-6"><a class="lnlinks" href="#hl-14-6"> 6</a>
</span><span class="lnt" id="hl-14-7"><a class="lnlinks" href="#hl-14-7"> 7</a>
</span><span class="lnt" id="hl-14-8"><a class="lnlinks" href="#hl-14-8"> 8</a>
</span><span class="lnt" id="hl-14-9"><a class="lnlinks" href="#hl-14-9"> 9</a>
</span><span class="lnt" id="hl-14-10"><a class="lnlinks" href="#hl-14-10">10</a>
</span><span class="lnt" id="hl-14-11"><a class="lnlinks" href="#hl-14-11">11</a>
</span><span class="lnt" id="hl-14-12"><a class="lnlinks" href="#hl-14-12">12</a>
</span><span class="lnt" id="hl-14-13"><a class="lnlinks" href="#hl-14-13">13</a>
</span><span class="lnt" id="hl-14-14"><a class="lnlinks" href="#hl-14-14">14</a>
</span><span class="lnt" id="hl-14-15"><a class="lnlinks" href="#hl-14-15">15</a>
</span><span class="lnt" id="hl-14-16"><a class="lnlinks" href="#hl-14-16">16</a>
</span><span class="lnt" id="hl-14-17"><a class="lnlinks" href="#hl-14-17">17</a>
</span><span class="lnt" id="hl-14-18"><a class="lnlinks" href="#hl-14-18">18</a>
</span><span class="lnt" id="hl-14-19"><a class="lnlinks" href="#hl-14-19">19</a>
</span><span class="lnt" id="hl-14-20"><a class="lnlinks" href="#hl-14-20">20</a>
</span><span class="lnt" id="hl-14-21"><a class="lnlinks" href="#hl-14-21">21</a>
</span><span class="lnt" id="hl-14-22"><a class="lnlinks" href="#hl-14-22">22</a>
</span><span class="lnt" id="hl-14-23"><a class="lnlinks" href="#hl-14-23">23</a>
</span><span class="lnt" id="hl-14-24"><a class="lnlinks" href="#hl-14-24">24</a>
</span><span class="lnt" id="hl-14-25"><a class="lnlinks" href="#hl-14-25">25</a>
</span><span class="lnt" id="hl-14-26"><a class="lnlinks" href="#hl-14-26">26</a>
</span><span class="lnt" id="hl-14-27"><a class="lnlinks" href="#hl-14-27">27</a>
</span><span class="lnt" id="hl-14-28"><a class="lnlinks" href="#hl-14-28">28</a>
</span><span class="lnt" id="hl-14-29"><a class="lnlinks" href="#hl-14-29">29</a>
</span><span class="lnt" id="hl-14-30"><a class="lnlinks" href="#hl-14-30">30</a>
</span><span class="lnt" id="hl-14-31"><a class="lnlinks" href="#hl-14-31">31</a>
</span><span class="lnt" id="hl-14-32"><a class="lnlinks" href="#hl-14-32">32</a>
</span><span class="lnt" id="hl-14-33"><a class="lnlinks" href="#hl-14-33">33</a>
</span><span class="lnt" id="hl-14-34"><a class="lnlinks" href="#hl-14-34">34</a>
</span><span class="lnt" id="hl-14-35"><a class="lnlinks" href="#hl-14-35">35</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">random</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_data</span><span class="p">(</span><span class="n">avg_params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">energy_method</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">clipped_version</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size_generation</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">unique_key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">fold_in</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">_</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">unique_key</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">_</span><span class="p">,</span> <span class="o">*</span><span class="n">subkeys</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">unique_key</span><span class="p">,</span> <span class="n">timesteps</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># need to generate new keys everytime</span>
</span></span><span class="line"><span class="cl">    <span class="n">data_noisy</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size_generation</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">    <span class="n">data_in_batch</span> <span class="o">=</span> <span class="n">data_noisy</span>
</span></span><span class="line"><span class="cl">    <span class="n">datas</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">datas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">device_get</span><span class="p">(</span><span class="n">data_noisy</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">timesteps</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">timestep</span> <span class="o">=</span> <span class="n">timesteps</span><span class="o">-</span><span class="n">t</span>
</span></span><span class="line"><span class="cl">        <span class="n">t_repeated</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">timestep</span><span class="p">]),</span> <span class="n">batch_size_generation</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># data_stacked = torch.vstack([data_in_batch, labelled_values])</span>
</span></span><span class="line"><span class="cl">        <span class="n">pred_data</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">avg_params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">data_in_batch</span><span class="p">,</span> <span class="n">t_repeated</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># clipping an improvement as recommended in https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils.py#L171</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># this helps in improving the samples generated as it keeps the random variables in the range of 0 to +1</span>
</span></span><span class="line"><span class="cl">		<span class="n">x_reconstructed</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">data_in_batch</span><span class="p">,</span> <span class="n">pred_data</span> <span class="o">*</span> <span class="n">sd</span><span class="p">[</span><span class="n">timestep</span><span class="p">])</span><span class="o">/</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alphas_</span><span class="p">[</span><span class="n">timestep</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="n">timestep</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">			<span class="n">x_reconstructed</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">x_reconstructed</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="n">mean_data_1</span> <span class="o">=</span> <span class="n">data_in_batch</span> <span class="o">*</span> <span class="n">mean_coeff_1</span><span class="p">[</span><span class="n">timestep</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">		<span class="n">mean_data_2</span> <span class="o">=</span> <span class="n">x_reconstructed</span> <span class="o">*</span> <span class="n">mean_coeff_2</span><span class="p">[</span><span class="n">timestep</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">		<span class="n">mean_data</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">mean_data_1</span><span class="p">,</span> <span class="n">mean_data_2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">posterior_data</span> <span class="o">=</span> <span class="n">posterior_variance_corrected</span><span class="p">[</span><span class="n">timestep</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">data_noisy</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">subkeys</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size_generation</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">data_in_batch</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">mean_data</span><span class="p">,</span>  <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">posterior_data</span><span class="p">)</span> <span class="o">*</span> <span class="n">data_noisy</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">datas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">device_get</span><span class="p">(</span><span class="n">data_in_batch</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">datas</span><span class="p">,</span> <span class="n">data_in_batch</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="striding-reducing-steps-needed-for-generation">Striding: Reducing steps needed for Generation</h3>
<p>We could reduce the number of steps needed for generation. One of the popular approaches to do this is by using <em>Time Step Striding</em>.</p>
<p>Instead of moving stepsone at a time from $T$ to 1, <code>range(T, 1, -1)</code>, we will move $s$ steps at a time, <code>range(T, 1, -s)</code>. Doing this will speed up generation of new samples from the model by s times. I have been able to produce good quality samples with $s$ set to 5.</p>
<p>We just need to make minor adjustments to the variables we created so that the maths still works.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-15-1"><a class="lnlinks" href="#hl-15-1"> 1</a>
</span><span class="lnt" id="hl-15-2"><a class="lnlinks" href="#hl-15-2"> 2</a>
</span><span class="lnt" id="hl-15-3"><a class="lnlinks" href="#hl-15-3"> 3</a>
</span><span class="lnt" id="hl-15-4"><a class="lnlinks" href="#hl-15-4"> 4</a>
</span><span class="lnt" id="hl-15-5"><a class="lnlinks" href="#hl-15-5"> 5</a>
</span><span class="lnt" id="hl-15-6"><a class="lnlinks" href="#hl-15-6"> 6</a>
</span><span class="lnt" id="hl-15-7"><a class="lnlinks" href="#hl-15-7"> 7</a>
</span><span class="lnt" id="hl-15-8"><a class="lnlinks" href="#hl-15-8"> 8</a>
</span><span class="lnt" id="hl-15-9"><a class="lnlinks" href="#hl-15-9"> 9</a>
</span><span class="lnt" id="hl-15-10"><a class="lnlinks" href="#hl-15-10">10</a>
</span><span class="lnt" id="hl-15-11"><a class="lnlinks" href="#hl-15-11">11</a>
</span><span class="lnt" id="hl-15-12"><a class="lnlinks" href="#hl-15-12">12</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">strided_schedule</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span> <span class="o">+</span> <span class="p">[</span><span class="n">timesteps</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">alphas_strided_</span> <span class="o">=</span> <span class="n">alphas_</span><span class="p">[</span><span class="n">strided_schedule</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">alphas_prev_strided_</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">alphas_strided_</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&#34;constant&#34;</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">betas_strided</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">alphas_strided_</span><span class="o">/</span><span class="n">alphas_prev_strided_</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">posterior_variance_new_schedule</span> <span class="o">=</span> <span class="n">betas_strided</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas_prev_strided_</span><span class="p">)</span><span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alphas_strided_</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">log_posterior_variance</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">posterior_variance_new_schedule</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">posterior_variance_new_schedule</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]))</span>
</span></span><span class="line"><span class="cl"><span class="n">posterior_variance_new_schedule_corrected</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_posterior_variance</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">mean_coeff_1_strided</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">betas_strided</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas_prev_strided_</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas_strided_</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mean_coeff_2_strided</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alphas_prev_strided_</span><span class="p">)</span> <span class="o">*</span> <span class="n">betas_strided</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas_strided_</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>Generate Samples:</strong></p>
<p>The code to generate samples is pretty similar, now we need to use the strided variables defined above.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-16-1"><a class="lnlinks" href="#hl-16-1"> 1</a>
</span><span class="lnt" id="hl-16-2"><a class="lnlinks" href="#hl-16-2"> 2</a>
</span><span class="lnt" id="hl-16-3"><a class="lnlinks" href="#hl-16-3"> 3</a>
</span><span class="lnt" id="hl-16-4"><a class="lnlinks" href="#hl-16-4"> 4</a>
</span><span class="lnt" id="hl-16-5"><a class="lnlinks" href="#hl-16-5"> 5</a>
</span><span class="lnt" id="hl-16-6"><a class="lnlinks" href="#hl-16-6"> 6</a>
</span><span class="lnt" id="hl-16-7"><a class="lnlinks" href="#hl-16-7"> 7</a>
</span><span class="lnt" id="hl-16-8"><a class="lnlinks" href="#hl-16-8"> 8</a>
</span><span class="lnt" id="hl-16-9"><a class="lnlinks" href="#hl-16-9"> 9</a>
</span><span class="lnt" id="hl-16-10"><a class="lnlinks" href="#hl-16-10">10</a>
</span><span class="lnt" id="hl-16-11"><a class="lnlinks" href="#hl-16-11">11</a>
</span><span class="lnt" id="hl-16-12"><a class="lnlinks" href="#hl-16-12">12</a>
</span><span class="lnt" id="hl-16-13"><a class="lnlinks" href="#hl-16-13">13</a>
</span><span class="lnt" id="hl-16-14"><a class="lnlinks" href="#hl-16-14">14</a>
</span><span class="lnt" id="hl-16-15"><a class="lnlinks" href="#hl-16-15">15</a>
</span><span class="lnt" id="hl-16-16"><a class="lnlinks" href="#hl-16-16">16</a>
</span><span class="lnt" id="hl-16-17"><a class="lnlinks" href="#hl-16-17">17</a>
</span><span class="lnt" id="hl-16-18"><a class="lnlinks" href="#hl-16-18">18</a>
</span><span class="lnt" id="hl-16-19"><a class="lnlinks" href="#hl-16-19">19</a>
</span><span class="lnt" id="hl-16-20"><a class="lnlinks" href="#hl-16-20">20</a>
</span><span class="lnt" id="hl-16-21"><a class="lnlinks" href="#hl-16-21">21</a>
</span><span class="lnt" id="hl-16-22"><a class="lnlinks" href="#hl-16-22">22</a>
</span><span class="lnt" id="hl-16-23"><a class="lnlinks" href="#hl-16-23">23</a>
</span><span class="lnt" id="hl-16-24"><a class="lnlinks" href="#hl-16-24">24</a>
</span><span class="lnt" id="hl-16-25"><a class="lnlinks" href="#hl-16-25">25</a>
</span><span class="lnt" id="hl-16-26"><a class="lnlinks" href="#hl-16-26">26</a>
</span><span class="lnt" id="hl-16-27"><a class="lnlinks" href="#hl-16-27">27</a>
</span><span class="lnt" id="hl-16-28"><a class="lnlinks" href="#hl-16-28">28</a>
</span><span class="lnt" id="hl-16-29"><a class="lnlinks" href="#hl-16-29">29</a>
</span><span class="lnt" id="hl-16-30"><a class="lnlinks" href="#hl-16-30">30</a>
</span><span class="lnt" id="hl-16-31"><a class="lnlinks" href="#hl-16-31">31</a>
</span><span class="lnt" id="hl-16-32"><a class="lnlinks" href="#hl-16-32">32</a>
</span><span class="lnt" id="hl-16-33"><a class="lnlinks" href="#hl-16-33">33</a>
</span><span class="lnt" id="hl-16-34"><a class="lnlinks" href="#hl-16-34">34</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">random</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_data_strided</span><span class="p">(</span><span class="n">avg_params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">energy_method</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">clipped_version</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size_generation</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">unique_key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">fold_in</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">_</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">unique_key</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">_</span><span class="p">,</span> <span class="o">*</span><span class="n">subkeys</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">unique_key</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">strided_schedule</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">data_noisy</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size_generation</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">          
</span></span><span class="line"><span class="cl">    <span class="n">data_in_batch</span> <span class="o">=</span> <span class="n">data_noisy</span>
</span></span><span class="line"><span class="cl">	<span class="n">datas</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">datas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">device_get</span><span class="p">(</span><span class="n">data_noisy</span><span class="p">))</span>                   
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">strided_schedule</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">stride_timestep</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">strided_schedule</span><span class="p">)</span><span class="o">-</span><span class="n">t</span>
</span></span><span class="line"><span class="cl">        <span class="n">timestep</span> <span class="o">=</span> <span class="n">strided_schedule</span><span class="p">[</span><span class="n">stride_timestep</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">t_repeated</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">timestep</span><span class="p">]),</span> <span class="n">batch_size_generation</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># data_stacked = torch.vstack([data_in_batch, labelled_values])</span>
</span></span><span class="line"><span class="cl">        <span class="n">pred_data</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">avg_params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">data_in_batch</span><span class="p">,</span> <span class="n">t_repeated</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># Clipping helps in improving the samples generated as it keeps the random variables in the range of 0 to +1</span>
</span></span><span class="line"><span class="cl">		<span class="n">x_reconstructed</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">data_in_batch</span><span class="p">,</span> <span class="n">pred_data</span> <span class="o">*</span> <span class="n">sd</span><span class="p">[</span><span class="n">timestep</span><span class="p">])</span><span class="o">/</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alphas_</span><span class="p">[</span><span class="n">timestep</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">		
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="n">timestep</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">			<span class="n">x_reconstructed</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">x_reconstructed</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="n">mean_data_1</span> <span class="o">=</span> <span class="n">data_in_batch</span> <span class="o">*</span> <span class="n">mean_coeff_1_strided</span><span class="p">[</span><span class="n">stride_timestep</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">		<span class="n">mean_data_2</span> <span class="o">=</span> <span class="n">x_reconstructed</span> <span class="o">*</span> <span class="n">mean_coeff_2_strided</span><span class="p">[</span><span class="n">stride_timestep</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">		<span class="n">mean_data</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">mean_data_1</span><span class="p">,</span> <span class="n">mean_data_2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">posterior_data</span> <span class="o">=</span> <span class="n">posterior_variance_new_schedule_corrected</span><span class="p">[</span><span class="n">stride_timestep</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">data_noisy</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">subkeys</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size_generation</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">data_in_batch</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">mean_data</span><span class="p">,</span>  <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">posterior_data</span><span class="p">)</span> <span class="o">*</span> <span class="n">data_noisy</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">        </span><span class="n">datas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">device_get</span><span class="p">(</span><span class="n">data_in_batch</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="err">    </span><span class="k">return</span><span class="err"> </span><span class="n">datas</span><span class="p">,</span><span class="err"> </span><span class="n">data_in_batch</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="generating-samples--outputs">Generating Samples &amp; Outputs</h2>
<p>Let&rsquo;s first create a map between the label values and the characters.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-17-1"><a class="lnlinks" href="#hl-17-1"> 1</a>
</span><span class="lnt" id="hl-17-2"><a class="lnlinks" href="#hl-17-2"> 2</a>
</span><span class="lnt" id="hl-17-3"><a class="lnlinks" href="#hl-17-3"> 3</a>
</span><span class="lnt" id="hl-17-4"><a class="lnlinks" href="#hl-17-4"> 4</a>
</span><span class="lnt" id="hl-17-5"><a class="lnlinks" href="#hl-17-5"> 5</a>
</span><span class="lnt" id="hl-17-6"><a class="lnlinks" href="#hl-17-6"> 6</a>
</span><span class="lnt" id="hl-17-7"><a class="lnlinks" href="#hl-17-7"> 7</a>
</span><span class="lnt" id="hl-17-8"><a class="lnlinks" href="#hl-17-8"> 8</a>
</span><span class="lnt" id="hl-17-9"><a class="lnlinks" href="#hl-17-9"> 9</a>
</span><span class="lnt" id="hl-17-10"><a class="lnlinks" href="#hl-17-10">10</a>
</span><span class="lnt" id="hl-17-11"><a class="lnlinks" href="#hl-17-11">11</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">string</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">63</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)))]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">string</span><span class="o">.</span><span class="n">ascii_uppercase</span> <span class="o">+</span> <span class="n">string</span><span class="o">.</span><span class="n">ascii_lowercase</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">dict_</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">dict_</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">i</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_label</span><span class="p">(</span><span class="n">ans</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">dict_</span><span class="p">[</span><span class="nb">str</span><span class="o">.</span><span class="n">upper</span><span class="p">(</span><span class="n">char</span><span class="p">)]</span> <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">ans</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>Generating Samples:</strong></p>
<p>Generating &lsquo;<em><strong>varun</strong></em>&rsquo; using the trained diffusion model.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-18-1"><a class="lnlinks" href="#hl-18-1">1</a>
</span><span class="lnt" id="hl-18-2"><a class="lnlinks" href="#hl-18-2">2</a>
</span><span class="lnt" id="hl-18-3"><a class="lnlinks" href="#hl-18-3">3</a>
</span><span class="lnt" id="hl-18-4"><a class="lnlinks" href="#hl-18-4">4</a>
</span><span class="lnt" id="hl-18-5"><a class="lnlinks" href="#hl-18-5">5</a>
</span><span class="lnt" id="hl-18-6"><a class="lnlinks" href="#hl-18-6">6</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">datas</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">generate_data_strided</span><span class="p">(</span><span class="n">avg_params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span> <span class="s2">&#34;varun&#34;</span><span class="p">,</span> <span class="n">energy_method</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">clipped_version</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">datas_</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">datas</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span> <span class="p">:</span><span class="mi">2</span><span class="p">]),</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">d_</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">rearrange</span><span class="p">(</span><span class="n">datas_</span><span class="p">,</span> <span class="s1">&#39;a b c d e -&gt; (b a) c d e&#39;</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">rawarrview</span><span class="p">(</span><span class="n">reshape_image_batch</span><span class="p">(</span><span class="n">datas</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">rows</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;bone_r&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">rawarrview</span><span class="p">(</span><span class="n">reshape_image_batch</span><span class="p">(</span><span class="n">d_</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span><span class="err"> </span><span class="n">rows</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span><span class="err"> </span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;bone_r&#39;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><figure class="align-center ">
    <img loading="lazy" src="/images/varun.png#center"
         alt="Figure 4: &amp;lsquo;varun&amp;rsquo; generated using strided sampling technique." width="100%"/> <figcaption>
            <p>Figure 4: &lsquo;varun&rsquo; generated using strided sampling technique.</p>
        </figcaption>
</figure>

<figure class="align-center ">
    <img loading="lazy" src="/images/denoising_varun.png#center"
         alt="Figure 5: Visualizing the steps in the diffusion process. If you notice carefully, not much is happening at the early stages." width="100%"/> <figcaption>
            <p>Figure 5: Visualizing the steps in the diffusion process. If you notice carefully, not much is happening at the early stages.</p>
        </figcaption>
</figure>
</p>
<p>Generating &lsquo;<em><strong>tulsian</strong></em>&rsquo; using the trained diffusion model.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-19-1"><a class="lnlinks" href="#hl-19-1">1</a>
</span><span class="lnt" id="hl-19-2"><a class="lnlinks" href="#hl-19-2">2</a>
</span><span class="lnt" id="hl-19-3"><a class="lnlinks" href="#hl-19-3">3</a>
</span><span class="lnt" id="hl-19-4"><a class="lnlinks" href="#hl-19-4">4</a>
</span><span class="lnt" id="hl-19-5"><a class="lnlinks" href="#hl-19-5">5</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">datas</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">generate_data_strided</span><span class="p">(</span><span class="n">avg_params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span> <span class="s2">&#34;tulsian&#34;</span><span class="p">,</span> <span class="n">energy_method</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">clipped_version</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">datas_</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">datas</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span> <span class="p">:</span><span class="mi">2</span><span class="p">]),</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">d_</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">rearrange</span><span class="p">(</span><span class="n">datas_</span><span class="p">,</span> <span class="s1">&#39;a b c d e -&gt; (b a) c d e&#39;</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="n">rawarrview</span><span class="p">(</span><span class="n">reshape_image_batch</span><span class="p">(</span><span class="n">datas</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">rows</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;bone_r&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">rawarrview</span><span class="p">(</span><span class="n">reshape_image_batch</span><span class="p">(</span><span class="n">d_</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span><span class="err"> </span><span class="n">rows</span><span class="o">=</span><span class="mi">7</span><span class="p">),</span><span class="err"> </span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;bone_r&#39;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><figure class="align-center ">
    <img loading="lazy" src="/images/tulsian.png#center"
         alt="Figure 6: &amp;rsquo;tulsian&amp;rsquo; generated using strided sampling technique." width="100%"/> <figcaption>
            <p>Figure 6: &rsquo;tulsian&rsquo; generated using strided sampling technique.</p>
        </figcaption>
</figure>

<figure class="align-center ">
    <img loading="lazy" src="/images/denoising_tulsian.png#center"
         alt="Figure 7: Visualizing the steps in the diffusion process. Similar to earlier, not much is happening at the early stages." width="100%"/> <figcaption>
            <p>Figure 7: Visualizing the steps in the diffusion process. Similar to earlier, not much is happening at the early stages.</p>
        </figcaption>
</figure>
</p>
<h2 id="conclusion">Conclusion</h2>
<p>Denoising Diffusion models are a powerful algorithmic tool for Generative AI. Although much of the work done so far focusses on images, we could generate any distribution using these techniques.</p>
<p>I sincerely hope this introduction was useful to you. Please explore additional resources <a href="/posts/diffusion-models/bonus-denoising-diffusion-models-resources/" >here.</a></p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Ioffe et al. 2015 <a href="https://arxiv.org/abs/1502.03167" target="_blank" >Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Nichol et al. 2021 <a href="https://arxiv.org/abs/2102.09672" target="_blank" >Improved Denoising Diffusion Probabilistic Models</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Polyak et al. 1991<a href="https://epubs.siam.org/doi/10.1137/0330046" target="_blank" >Acceleration of Stochastic Approximation by Averaging</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Denoising Diffusion Models Part 2: Improving Diffusion Models</title>
      <link>https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-2/</link>
      <pubDate>Fri, 09 Dec 2022 17:38:49 +0530</pubDate>
      
      <guid>https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-2/</guid>
      <description>Code for this blog post: Notebook Github Link Colab Predicting Error and Score Function Error / Score Prediction Classifier free Guidance and other improvements Advanced concepts Topics to cover We have done most of the heavy-lifting in Part 1 of this series on Diffusion Models. To be able to use them well in practice, we may need to make some more improvements. That&amp;rsquo;s what we will do.
Time step embedding and concatenation/fusion to the input data.</description>
      <content:encoded><![CDATA[<h2 id="code-for-this-blog-post">Code for this blog post:</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Notebook</th>
<th style="text-align:left">Github Link</th>
<th style="text-align:left">Colab</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Predicting Error and Score Function</td>
<td style="text-align:left"><a href="https://github.com/varun-ml/diffusion-models-tutorial/blob/master/toy-density-estimation/errors.diffusion_model_interpretations.ipynb" target="_blank" >Error / Score Prediction</a></td>
<td style="text-align:left"><a href="https://colab.research.google.com/github/varun-ml/diffusion-models-tutorial/blob/master/toy-density-estimation/colab_errors.diffusion_model_interpretations.ipynb" target="_blank" >
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Colab (Large)">
</a></td>
</tr>
<tr>
<td style="text-align:left">Classifier free Guidance and other improvements</td>
<td style="text-align:left"><a href="https://github.com/varun-ml/diffusion-models-tutorial/blob/master/toy-density-estimation/guidance_free_classifiers.ipynb" target="_blank" >Advanced concepts</a></td>
<td style="text-align:left"><a href="https://colab.research.google.com/github/varun-ml/diffusion-models-tutorial/blob/master/toy-density-estimation/colab_guidance_free_classifiers.ipynb" target="_blank" >
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Colab (Large)">
</a></td>
</tr>
</tbody>
</table>
<h2 id="topics-to-cover">Topics to cover</h2>
<p>We have done most of the heavy-lifting in <a href="/posts/diffusion-models/denoising-diffusion-models-1/" >Part 1</a> of this series on Diffusion Models. To be able to use them well in practice, we may need to make some more improvements. That&rsquo;s what we will do.</p>
<ol>
<li><strong>Time step embedding</strong> and concatenation/fusion to the input data.</li>
<li><strong>Error Prediction $\hat \epsilon_0^\ast$ and Score Function Prediction $s$</strong> instead of predicting the actual input $x_0$.</li>
<li><strong>Class conditioned generation or Classifier free guidance</strong>, where we guide the diffusion model to generate data based on class labels.</li>
</ol>
<p>The ideas are an extension to the concepts introduced earlier, but are vital parts of any practical diffusion model implementation.</p>
<h2 id="time-step-embedding">Time Step Embedding</h2>
<p>During the denoising process, the Neural Network needs to know the time step at which denoising is being done. Passing the time step $t$ as a scalar value is not ideal. Rather, it would be preferable to
pass the time step as an embedding to the Neural Network. In the <cite>Attention is all you need<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></cite> paper, the authors proposed sinusoidal embeddings to encode position of the tokens (time steps in our case).</p>
<p><strong>Pseudocode:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-0-1"><a class="lnlinks" href="#hl-0-1"> 1</a>
</span><span class="lnt" id="hl-0-2"><a class="lnlinks" href="#hl-0-2"> 2</a>
</span><span class="lnt" id="hl-0-3"><a class="lnlinks" href="#hl-0-3"> 3</a>
</span><span class="lnt" id="hl-0-4"><a class="lnlinks" href="#hl-0-4"> 4</a>
</span><span class="lnt" id="hl-0-5"><a class="lnlinks" href="#hl-0-5"> 5</a>
</span><span class="lnt" id="hl-0-6"><a class="lnlinks" href="#hl-0-6"> 6</a>
</span><span class="lnt" id="hl-0-7"><a class="lnlinks" href="#hl-0-7"> 7</a>
</span><span class="lnt" id="hl-0-8"><a class="lnlinks" href="#hl-0-8"> 8</a>
</span><span class="lnt" id="hl-0-9"><a class="lnlinks" href="#hl-0-9"> 9</a>
</span><span class="lnt" id="hl-0-10"><a class="lnlinks" href="#hl-0-10">10</a>
</span><span class="lnt" id="hl-0-11"><a class="lnlinks" href="#hl-0-11">11</a>
</span><span class="lnt" id="hl-0-12"><a class="lnlinks" href="#hl-0-12">12</a>
</span><span class="lnt" id="hl-0-13"><a class="lnlinks" href="#hl-0-13">13</a>
</span><span class="lnt" id="hl-0-14"><a class="lnlinks" href="#hl-0-14">14</a>
</span><span class="lnt" id="hl-0-15"><a class="lnlinks" href="#hl-0-15">15</a>
</span><span class="lnt" id="hl-0-16"><a class="lnlinks" href="#hl-0-16">16</a>
</span><span class="lnt" id="hl-0-17"><a class="lnlinks" href="#hl-0-17">17</a>
</span><span class="lnt" id="hl-0-18"><a class="lnlinks" href="#hl-0-18">18</a>
</span><span class="lnt" id="hl-0-19"><a class="lnlinks" href="#hl-0-19">19</a>
</span><span class="lnt" id="hl-0-20"><a class="lnlinks" href="#hl-0-20">20</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">math</span>
</span></span><span class="line"><span class="cl"><span class="c1"># adapted from HuggingFace -- https://huggingface.co/blog/annotated-diffusion</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">SinusoidalPositionEmbeddings</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">time</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">half_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">//</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># embedding values need to be small</span>
</span></span><span class="line"><span class="cl">        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">half_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">half_dim</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="n">embeddings</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">time</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">embeddings</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">embeddings</span><span class="o">.</span><span class="n">sin</span><span class="p">(),</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">cos</span><span class="p">()),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">embeddings</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl"><span class="c1"># generate 8 dimensional time step embeddings</span>
</span></span><span class="line"><span class="cl"><span class="n">sinusoidalPositionEmbeddings</span> <span class="o">=</span> <span class="n">SinusoidalPositionEmbeddings</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Sinusoidal embeddings are small in absolute value and can be fused(added) or concatenated to the input data to provide the Neural Network some information about the time step at which the denoising process is happening.</p>
<p><strong>Passing Time Step Embedding (Fusing and Concatenation):</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-1-1"><a class="lnlinks" href="#hl-1-1"> 1</a>
</span><span class="lnt" id="hl-1-2"><a class="lnlinks" href="#hl-1-2"> 2</a>
</span><span class="lnt" id="hl-1-3"><a class="lnlinks" href="#hl-1-3"> 3</a>
</span><span class="lnt" id="hl-1-4"><a class="lnlinks" href="#hl-1-4"> 4</a>
</span><span class="lnt" id="hl-1-5"><a class="lnlinks" href="#hl-1-5"> 5</a>
</span><span class="lnt" id="hl-1-6"><a class="lnlinks" href="#hl-1-6"> 6</a>
</span><span class="lnt" id="hl-1-7"><a class="lnlinks" href="#hl-1-7"> 7</a>
</span><span class="lnt" id="hl-1-8"><a class="lnlinks" href="#hl-1-8"> 8</a>
</span><span class="lnt" id="hl-1-9"><a class="lnlinks" href="#hl-1-9"> 9</a>
</span><span class="lnt" id="hl-1-10"><a class="lnlinks" href="#hl-1-10">10</a>
</span><span class="lnt" id="hl-1-11"><a class="lnlinks" href="#hl-1-11">11</a>
</span><span class="lnt" id="hl-1-12"><a class="lnlinks" href="#hl-1-12">12</a>
</span><span class="lnt" id="hl-1-13"><a class="lnlinks" href="#hl-1-13">13</a>
</span><span class="lnt" id="hl-1-14"><a class="lnlinks" href="#hl-1-14">14</a>
</span><span class="lnt" id="hl-1-15"><a class="lnlinks" href="#hl-1-15">15</a>
</span><span class="lnt" id="hl-1-16"><a class="lnlinks" href="#hl-1-16">16</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># MLP to project time step embedding before we pass it to the input</span>
</span></span><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">position_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">          <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span> 
</span></span><span class="line"><span class="cl">          <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># 2 is the input data dimension</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">timestep_embeddings</span> <span class="o">=</span> <span class="n">position_embeddings</span><span class="p">[</span><span class="n">timestep</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl"><span class="n">time_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_mlp</span><span class="p">(</span><span class="n">timestep_embeddings</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># concatenation</span>
</span></span><span class="line"><span class="cl"><span class="c1"># x = (batch_size, input data dimension); time_embeddings = (batch_size, input data dimension)</span>
</span></span><span class="line"><span class="cl"><span class="n">concat_x</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">time_embeddings</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># fusing</span>
</span></span><span class="line"><span class="cl"><span class="n">shift</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">time_embedding</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">shift</span> <span class="o">+</span> <span class="p">(</span><span class="n">scale</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">x</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Fusing the data with the data is computationally efficient as it requires significantly fewer number of weight parameters. The key idea here is that Neural Networks are powerful enough to separate any added information to the original data without the information being explicitly passed. Have a look at this video if you want to get better intuition. <a href="https://www.youtube.com/watch?v=1biZfFLPRSY" target="_blank" >Positional embeddings in transformers EXPLAINED</a>.</p>
<p>If we want to add time step information to an image, it is typically done by broadcasting the time step embedding along the channel dimension.</p>
<p><strong>Adding Time Step to Image (Fusing):</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-2-1"><a class="lnlinks" href="#hl-2-1">1</a>
</span><span class="lnt" id="hl-2-2"><a class="lnlinks" href="#hl-2-2">2</a>
</span><span class="lnt" id="hl-2-3"><a class="lnlinks" href="#hl-2-3">3</a>
</span><span class="lnt" id="hl-2-4"><a class="lnlinks" href="#hl-2-4">4</a>
</span><span class="lnt" id="hl-2-5"><a class="lnlinks" href="#hl-2-5">5</a>
</span><span class="lnt" id="hl-2-6"><a class="lnlinks" href="#hl-2-6">6</a>
</span><span class="lnt" id="hl-2-7"><a class="lnlinks" href="#hl-2-7">7</a>
</span><span class="lnt" id="hl-2-8"><a class="lnlinks" href="#hl-2-8">8</a>
</span><span class="lnt" id="hl-2-9"><a class="lnlinks" href="#hl-2-9">9</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># image batch data -- (batch_size, height, width, channels)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># timestep embedding -- (batch_size, embedding_dimension)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># ensure channels == embedding_dimension</span>
</span></span><span class="line"><span class="cl"><span class="c1"># broadcast and add timestep to image data</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">einops</span>
</span></span><span class="line"><span class="cl"><span class="n">timestep_embedding</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">einops</span><span class="o">.</span><span class="n">rearrange</span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="err"> </span><span class="s1">&#39;b c -&gt; b 1 1 c&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">fused_image_data</span> <span class="o">=</span> <span class="n">image_data</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">timestep_embedding</span><span class="p">)</span> 
</span></span></code></pre></td></tr></table>
</div>
</div><p>Fusing data to an input is a technique useful to add any kind of conditional information to the data. We will use this idea in <a href="/posts/diffusion-models/denoising-diffusion-models-3/" >EMNIST: Blog 3</a> where we use this concept to fuse label and time step information with the images during class conditional generation. This technique was used in the work, <a href="https://github.com/openai/improved-diffusion" target="_blank" >Improved Denoising Diffusion Probabilistic Models</a>.</p>
<h2 id="error-prediction-and-score-function-prediction">Error Prediction and Score Function Prediction</h2>
<h3 id="predicting-the-error-hat-epsilon_0ast">Predicting the Error $\hat \epsilon_0^\ast$</h3>
<p><strong>During the denoising step</strong>, the model ingests $\hat x_t$ and spits out $\hat x_t^0$, a prediction for $x_0$. Instead of outputting predictions of the input data, we want the model to output a prediction of the error $\hat\epsilon^\ast_0$.</p>
<p>Let&rsquo;s investigate the relationship between error $\epsilon_0^\ast$ and input data $x_0$.

$$
\begin{align}
x_t &= \sqrt{\bar\alpha_t}x_0 + \sqrt{(1 - \bar\alpha_t )}\ast\epsilon_0^\ast \cr
x_0 &= \frac{x_t-\sqrt{(1 - \bar\alpha_t )}\ast\epsilon_0^\ast}{\sqrt{\bar\alpha_t}} 
\end{align}
$$

Equation 2 shows that if we have $x_t$ and $\epsilon^\ast_0$ than it determines $x_0$.</p>
<p>The denoising step now looks like this:</p>
<ol>
<li>Make a prediction of source error $\hat\epsilon^\ast_0$ using the $NN(\hat x_t, t)$.</li>
<li>Using Equation 2, evaluate $x_0^t$, which is the prediction of the input data at time step $t$.</li>
<li>Next step:
<ul>
<li>During training: We want an equivalent version of the loss function $Error\_Loss(\epsilon^\ast_0, \hat \epsilon_0^\ast, t)$.

	$$
	\begin{align}
	Loss(x_0, \hat x_t, t) = 1/2\ast(\frac{\bar\alpha_{t-1}}{1-\bar\alpha_{t-1}} - \frac{\bar\alpha_t}{1-\bar\alpha_t})\ast\mid\mid x_0-\hat x_0^t\mid\mid_2^2 \cr
	\text{Making modifications to loss using Equation 2} \nonumber \cr
	Error\_Loss(\epsilon^\ast_0, \hat \epsilon_0^\ast, t) = \frac{1}{2\sigma^2_q(t)} \frac{(1-\alpha_t)^2}{(1-\bar\alpha_t)\alpha_t}\mid\mid \epsilon^\ast_0-\hat \epsilon^\ast_0\mid\mid_2^2
	\end{align}
	$$
	</li>
<li>During data generation:
<ol>
<li>Using Equation 2, reconstruct $\hat x_0^t$. Remember, $x_0$ is a function of the Gaussian error and the latent variable.</li>
<li><strong>Clip the $\hat x_0^t$ to make sure it lies in the range of -1 to +1 (normalized range for input data)</strong>. <code>torch.clip(x_reconstructed, -1, 1)</code></li>
<li>Using Equation 5 (below) get a prediction for the latent at $t-1$ time step.</li>
</ol>
</li>
</ul>
</li>
</ol>

$$
\begin{align}
p(\hat x_{t-1}| \hat x_t) \varpropto N(\hat x_{t-1};\frac{\sqrt\alpha_t(1-\bar\alpha_{t-1})\hat x_t + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)\hat x_t^0}{1-\bar\alpha_t}, \frac{(1-\alpha_t)(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}I)	
\end{align}
$$


<p>Predicting error is empirically shown to work well, refer <cite>Denoising paper<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></cite>. This is probably due to the clipping of the predicted output at every time step so that the predicted data is in the normalized range. <a href="https://github.com/hojonathanho/diffusion/issues/5#issuecomment-1283946094" target="_blank" >Refer this discussion</a>.</p>
<p>Here is an interesting discussion on why predicting error works for images, but may not work well for other domains such as voice generation.</p>

  <blockquote class="twitter-tweet" data-dnt="true"><p lang="en" dir="ltr">Another important reason is that the standard formulation everyone has ended up using for images (predict the standardised noise given noisy input) implicitly downweights high frequency components, which is an excellent match for the human visual system.</p>&mdash; Sander Dieleman (@sedielem) <a href="https://twitter.com/sedielem/status/1557692621199400960?ref_src=twsrc%5Etfw">August 11, 2022</a></blockquote>

<p><strong>Denoising Step during data generation:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-3-1"><a class="lnlinks" href="#hl-3-1"> 1</a>
</span><span class="lnt" id="hl-3-2"><a class="lnlinks" href="#hl-3-2"> 2</a>
</span><span class="lnt" id="hl-3-3"><a class="lnlinks" href="#hl-3-3"> 3</a>
</span><span class="lnt" id="hl-3-4"><a class="lnlinks" href="#hl-3-4"> 4</a>
</span><span class="lnt" id="hl-3-5"><a class="lnlinks" href="#hl-3-5"> 5</a>
</span><span class="lnt" id="hl-3-6"><a class="lnlinks" href="#hl-3-6"> 6</a>
</span><span class="lnt" id="hl-3-7"><a class="lnlinks" href="#hl-3-7"> 7</a>
</span><span class="lnt" id="hl-3-8"><a class="lnlinks" href="#hl-3-8"> 8</a>
</span><span class="lnt" id="hl-3-9"><a class="lnlinks" href="#hl-3-9"> 9</a>
</span><span class="lnt" id="hl-3-10"><a class="lnlinks" href="#hl-3-10">10</a>
</span><span class="lnt" id="hl-3-11"><a class="lnlinks" href="#hl-3-11">11</a>
</span><span class="lnt" id="hl-3-12"><a class="lnlinks" href="#hl-3-12">12</a>
</span><span class="lnt" id="hl-3-13"><a class="lnlinks" href="#hl-3-13">13</a>
</span><span class="lnt" id="hl-3-14"><a class="lnlinks" href="#hl-3-14">14</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="n">x_reconstructed</span> <span class="o">=</span> 
</span></span><span class="line"><span class="cl"><span class="c1"># data_in_batch is latent variable x at timestep</span>
</span></span><span class="line"><span class="cl"><span class="n">data_in_batch</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">pred_data</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">sd</span><span class="p">[</span><span class="n">timestep</span><span class="p">]))</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alphas_</span><span class="p">[</span><span class="n">timestep</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">timestep</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">	<span class="n">x_reconstructed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">x_reconstructed</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">mean_data_1</span> <span class="o">=</span> <span class="n">data_in_batch</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">mean_coeff_1</span><span class="p">[</span><span class="n">timestep</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">mean_data_2</span> <span class="o">=</span> <span class="n">x_reconstructed</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">mean_coeff_2</span><span class="p">[</span><span class="n">timestep</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">mean_data</span> <span class="o">=</span> <span class="n">mean_data_1</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">mean_data_2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	
</span></span><span class="line"><span class="cl"><span class="n">posterior_data</span> <span class="o">=</span> <span class="n">posterior_variance_corrected</span><span class="p">[</span><span class="n">timestep</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># data_in_batch is latent variable x at previous timestep</span>
</span></span><span class="line"><span class="cl"><span class="n">data_in_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean_data</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">posterior_data</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="score-function-prediction">Score Function Prediction</h3>
<p>This is yet another variation on diffusion models. Similar to predicting the error as discussed earlier, we could also predict the score function. The score function $s$ is defined as $\nabla p(x_t)$.</p>
<p>All we need is to define $x_t$ as $f(s)$. We could then define a $Score\_Loss(s, \hat s, t)$ by substituting $x_t$ for the function $f(s)$ in the training step. In the denoising step we will use the function $f(s)$ defined to compute $x_t$, perform clipping so that the output lies in the normalized range and proceed as we did in the earlier section.</p>
<p>The relation between $s$ and $x$ is defined as below:

$$
x_0 = \frac{x_t + (1 - \bar\alpha_t )\ast\nabla p(x_t)}{\sqrt{\bar\alpha_t}}
$$
</p>
<p>For more details and intuition on why this is an important interpretation, please refer to the section on <cite>Three equivalent interpretations.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></cite>
Yang Song has an excellent blog post on <a href="https://yang-song.net/blog/2021/score/" target="_blank" >Score based generative models.</a></p>
<h2 id="guidance--classifier-free-guidance">Guidance &amp; Classifier-free Guidance</h2>
<p>We want to control the data we generate. For example, in the image below, we have 2 labels. During generation, we want to direct the model to generate samples either from Yellow or Purple classes.
Classifier Guidance is a way to do this. We will be guiding the denoising process to generate samples that are more likely to belong to the conditioned class.</p>
<figure class="align-center ">
    <img loading="lazy" src="/images/data-labels.png#center"
         alt="Figure 1: Data with 2 labels, Circles in Purple and Moons in Yellow" width="80%"/> <figcaption>
            <p>Figure 1: Data with 2 labels, Circles in Purple and Moons in Yellow</p>
        </figcaption>
</figure>

<p>Text-guided diffusion models like <a href="https://www.youtube.com/watch?v=gwI6g1pBD84" target="_blank" >Glide</a> use the powerful Neural Network called <a href="https://openai.com/blog/clip/" target="_blank" >CLIP</a> and classifier guidance techniques to perform text-based image generation.</p>
<p>So far, we have been trying to maximize the likelihood of the data distribution $p(x)$ with diffusion models. This allowed us to randomly sample data points from the data distribution.</p>
<p><strong>A naïve idea</strong> to do class-conditioned generation could be to fuse the conditional label information with the input data.</p>
<p><strong>Pseudocode for naïve idea:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-4-1"><a class="lnlinks" href="#hl-4-1"> 1</a>
</span><span class="lnt" id="hl-4-2"><a class="lnlinks" href="#hl-4-2"> 2</a>
</span><span class="lnt" id="hl-4-3"><a class="lnlinks" href="#hl-4-3"> 3</a>
</span><span class="lnt" id="hl-4-4"><a class="lnlinks" href="#hl-4-4"> 4</a>
</span><span class="lnt" id="hl-4-5"><a class="lnlinks" href="#hl-4-5"> 5</a>
</span><span class="lnt" id="hl-4-6"><a class="lnlinks" href="#hl-4-6"> 6</a>
</span><span class="lnt" id="hl-4-7"><a class="lnlinks" href="#hl-4-7"> 7</a>
</span><span class="lnt" id="hl-4-8"><a class="lnlinks" href="#hl-4-8"> 8</a>
</span><span class="lnt" id="hl-4-9"><a class="lnlinks" href="#hl-4-9"> 9</a>
</span><span class="lnt" id="hl-4-10"><a class="lnlinks" href="#hl-4-10">10</a>
</span><span class="lnt" id="hl-4-11"><a class="lnlinks" href="#hl-4-11">11</a>
</span><span class="lnt" id="hl-4-12"><a class="lnlinks" href="#hl-4-12">12</a>
</span><span class="lnt" id="hl-4-13"><a class="lnlinks" href="#hl-4-13">13</a>
</span><span class="lnt" id="hl-4-14"><a class="lnlinks" href="#hl-4-14">14</a>
</span><span class="lnt" id="hl-4-15"><a class="lnlinks" href="#hl-4-15">15</a>
</span><span class="lnt" id="hl-4-16"><a class="lnlinks" href="#hl-4-16">16</a>
</span><span class="lnt" id="hl-4-17"><a class="lnlinks" href="#hl-4-17">17</a>
</span><span class="lnt" id="hl-4-18"><a class="lnlinks" href="#hl-4-18">18</a>
</span><span class="lnt" id="hl-4-19"><a class="lnlinks" href="#hl-4-19">19</a>
</span><span class="lnt" id="hl-4-20"><a class="lnlinks" href="#hl-4-20">20</a>
</span><span class="lnt" id="hl-4-21"><a class="lnlinks" href="#hl-4-21">21</a>
</span><span class="lnt" id="hl-4-22"><a class="lnlinks" href="#hl-4-22">22</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">diffusion</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="n">code</span> <span class="n">to</span> <span class="n">add</span> <span class="n">noise</span> <span class="n">to</span> <span class="n">x_0</span>
</span></span><span class="line"><span class="cl">	<span class="k">return</span> <span class="n">x_i</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">training</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="n">loop</span> <span class="n">until</span> <span class="n">convergence</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">		<span class="n">pick</span> <span class="n">an</span> <span class="n">image</span> <span class="n">x_0</span> <span class="kn">from</span> <span class="nn">X</span> <span class="p">(</span><span class="n">batch</span> <span class="n">of</span> <span class="n">images</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">sample</span> <span class="n">t</span> <span class="kn">from</span> <span class="mi">1</span> <span class="n">to</span> <span class="n">T</span>
</span></span><span class="line"><span class="cl">		<span class="n">x_t</span> <span class="o">=</span> <span class="n">diffusion_step</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">x_hat_t</span> <span class="o">=</span> <span class="n">NN</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># y is the label</span>
</span></span><span class="line"><span class="cl">		<span class="n">loss_</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">x_hat_t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">update</span><span class="p">(</span><span class="n">NN</span><span class="p">,</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss_</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># generating new data points through denoising steps</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_new_data</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">	<span class="n">sample</span> <span class="n">x_T</span> <span class="kn">from</span> <span class="nn">N</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">I</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># denoising step</span>
</span></span><span class="line"><span class="cl">		<span class="n">x_hat_t</span> <span class="o">=</span> <span class="n">NN</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># y is the desired label </span>
</span></span><span class="line"><span class="cl">		<span class="c1"># get x_{t-1}</span>
</span></span><span class="line"><span class="cl">		<span class="n">x_</span><span class="p">{</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">x_hat_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="c1"># refer EQ - 12</span>
</span></span><span class="line"><span class="cl">	<span class="n">x_hat_0</span> <span class="o">=</span> <span class="n">x_0</span>  
</span></span></code></pre></td></tr></table>
</div>
</div><p>During training and data generation, we will add the conditioned label as an input along with the noisy data and the time step embedding. The output of the model will stay the same as earlier.</p>
<p>The conditioned data can be fused with the input data, just like we fused the time step information.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-5-1"><a class="lnlinks" href="#hl-5-1"> 1</a>
</span><span class="lnt" id="hl-5-2"><a class="lnlinks" href="#hl-5-2"> 2</a>
</span><span class="lnt" id="hl-5-3"><a class="lnlinks" href="#hl-5-3"> 3</a>
</span><span class="lnt" id="hl-5-4"><a class="lnlinks" href="#hl-5-4"> 4</a>
</span><span class="lnt" id="hl-5-5"><a class="lnlinks" href="#hl-5-5"> 5</a>
</span><span class="lnt" id="hl-5-6"><a class="lnlinks" href="#hl-5-6"> 6</a>
</span><span class="lnt" id="hl-5-7"><a class="lnlinks" href="#hl-5-7"> 7</a>
</span><span class="lnt" id="hl-5-8"><a class="lnlinks" href="#hl-5-8"> 8</a>
</span><span class="lnt" id="hl-5-9"><a class="lnlinks" href="#hl-5-9"> 9</a>
</span><span class="lnt" id="hl-5-10"><a class="lnlinks" href="#hl-5-10">10</a>
</span><span class="lnt" id="hl-5-11"><a class="lnlinks" href="#hl-5-11">11</a>
</span><span class="lnt" id="hl-5-12"><a class="lnlinks" href="#hl-5-12">12</a>
</span><span class="lnt" id="hl-5-13"><a class="lnlinks" href="#hl-5-13">13</a>
</span><span class="lnt" id="hl-5-14"><a class="lnlinks" href="#hl-5-14">14</a>
</span><span class="lnt" id="hl-5-15"><a class="lnlinks" href="#hl-5-15">15</a>
</span><span class="lnt" id="hl-5-16"><a class="lnlinks" href="#hl-5-16">16</a>
</span><span class="lnt" id="hl-5-17"><a class="lnlinks" href="#hl-5-17">17</a>
</span><span class="lnt" id="hl-5-18"><a class="lnlinks" href="#hl-5-18">18</a>
</span><span class="lnt" id="hl-5-19"><a class="lnlinks" href="#hl-5-19">19</a>
</span><span class="lnt" id="hl-5-20"><a class="lnlinks" href="#hl-5-20">20</a>
</span><span class="lnt" id="hl-5-21"><a class="lnlinks" href="#hl-5-21">21</a>
</span><span class="lnt" id="hl-5-22"><a class="lnlinks" href="#hl-5-22">22</a>
</span><span class="lnt" id="hl-5-23"><a class="lnlinks" href="#hl-5-23">23</a>
</span><span class="lnt" id="hl-5-24"><a class="lnlinks" href="#hl-5-24">24</a>
</span><span class="lnt" id="hl-5-25"><a class="lnlinks" href="#hl-5-25">25</a>
</span><span class="lnt" id="hl-5-26"><a class="lnlinks" href="#hl-5-26">26</a>
</span><span class="lnt" id="hl-5-27"><a class="lnlinks" href="#hl-5-27">27</a>
</span><span class="lnt" id="hl-5-28"><a class="lnlinks" href="#hl-5-28">28</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># haiku code</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">hk</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">  <span class="n">hk</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">  <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">gelu</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="n">hk</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># conditional vectors encoding</span>
</span></span><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">embedding_vectors</span> <span class="o">=</span> <span class="n">hk</span><span class="o">.</span><span class="n">Embed</span><span class="p">(</span><span class="mi">10</span><span class="o">+</span><span class="mi">26</span><span class="o">+</span><span class="mi">26</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">timestep_embeddings</span> <span class="o">=</span> <span class="n">TimeEmbeddings</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># the diffusion model is given x, time step and label as input</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="n">cond</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">cond_embedding</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">conditioning</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">timesteps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">timestep_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">timestep_embeddings</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">conditioning</span> <span class="o">=</span> <span class="n">timestep_embeddings</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">cond</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">label_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_vectors</span><span class="p">(</span><span class="n">cond</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">conditioning</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">label_embeddings</span><span class="p">,</span> <span class="n">conditioning</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="n">cond_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">conditioning</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="o">...</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1"># fusing time step and label information with input x</span>
</span></span><span class="line"><span class="cl">	<span class="n">shift</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">cond_embedding</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="n">x</span> <span class="o">=</span> <span class="n">shift</span> <span class="o">+</span> <span class="p">(</span><span class="n">scale</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">x</span>
</span></span><span class="line"><span class="cl">	<span class="o">...</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="guidance">Guidance</h3>
<p>The above approach may lead to models that have low sample diversity. Researchers have proposed two other forms of guidance: <em>classifier guidance</em> and <cite>classifier-free guidance<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup></cite>.</p>
<p><strong>Classifier Guidance</strong> guides the generation of new samples with the help of a classifier. The classifier takes a noisy image ($x_t$) as input and predicts the label $y$. The gradient of the distribution $p(y|x)$ is used to make updates to the weights of the Neural Network to guide it to produce samples that are likely to be $y$. This is an adversarial loss, and this approach has similarities to GANs.</p>
<p><strong>Classifier-Free Guidance</strong> would be ideal if we did not want to build a classifier. The classifier-free guidance approach models the conditional likelihood of samples as follows:

$$
\nabla p(x|y) = \lambda \ast \underbrace{\nabla p(x|y)}_{\text{conditional}} + (1-\lambda) \ast \underbrace{\nabla p(x)}_{\text{unconditional}}
$$

The conditional and the unconditional distributions are modelled by the same neural network. To model the conditional distribution, we fuse the label information as shown in the naïve approach above. To model the unconditional distribution, we mask the label information and pass it to the diffusion model.
The lambda parameter, controls the diversity of the sample we want to generate. $\lambda=1$ would be equivalent to the naïve approach.</p>
<p>
  <blockquote class="twitter-tweet" data-dnt="true"><p lang="en" dir="ltr">Classifier-free guidance is a cheatcode that makes these models perform as if they had 10x the parameters. At least in terms of sample quality, and at the cost of diversity. All of the recent spectacular results rely heavily on this trick.</p>&mdash; Sander Dieleman (@sedielem) <a href="https://twitter.com/sedielem/status/1557691965076021252?ref_src=twsrc%5Etfw">August 11, 2022</a></blockquote>

Adding this perspective about efficiency of diffusion models. Even though to be honest, I am not sure myself :).</p>
<h2 id="lets-look-at-some-outputs">Let&rsquo;s look at some outputs</h2>
<figure class="align-center ">
    <img loading="lazy" src="/images/class-conditioned-generation.gif#center"
         alt="Figure 2: A GIF show-casing the denoising process; Generating class conditioned samples over T time steps" width="80%"/> <figcaption>
            <p>Figure 2: A GIF show-casing the denoising process; Generating class conditioned samples over T time steps</p>
        </figcaption>
</figure>

<p>See you in the <a href="/posts/diffusion-models/denoising-diffusion-models-3" >next part</a>.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Vaswani et al. 2017 <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank" >&ldquo;Attention is all you need&rdquo;</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Ho et al. 2020 <a href="https://arxiv.org/abs/2006.11239" target="_blank" >&ldquo;Denoising diffusion probabilistic models&rdquo;</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Calvin Luo; 2019 <a href="https://arxiv.org/abs/2208.11970" target="_blank" >&ldquo;Understanding Diffusion Models: A Unified Perspective&rdquo;</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Ho et al. 2021 <a href="https://arxiv.org/abs/2207.12598" target="_blank" >&ldquo;Classifier-free diffusion guidance&rdquo;</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Denoising Diffusion Models Part 1: Estimating True Distribution</title>
      <link>https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-1/</link>
      <pubDate>Fri, 09 Dec 2022 11:28:30 +0530</pubDate>
      
      <guid>https://varun-ml.github.io/posts/diffusion-models/denoising-diffusion-models-1/</guid>
      <description>Code for this blog post: Notebook Github Link Colab Basic: Predicting Original Distribution Vanilla Implementation What are Denoising Diffusion Models? Denoising Diffusion Models, commonly referred to as &amp;ldquo;Diffusion models&amp;rdquo;, are a class of generative models based on the Variational Auto Encoder (VAE) architecture. These models are called likelihood-based models because they assign a high likelihood to the observed data samples $p(X)$. In contrast to other generative models, such as GANs, which learn the sampling process of a complex distribution and are trained adversarially.</description>
      <content:encoded><![CDATA[<h2 id="code-for-this-blog-post">Code for this blog post:</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Notebook</th>
<th style="text-align:left">Github Link</th>
<th style="text-align:left">Colab</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Basic: Predicting Original Distribution</td>
<td style="text-align:left"><a href="https://github.com/varun-ml/diffusion-models-tutorial/blob/master/toy-density-estimation/vanilla_diffusion_model.ipynb" target="_blank" >Vanilla Implementation</a></td>
<td style="text-align:left"><a href="https://colab.research.google.com/github/varun-ml/diffusion-models-tutorial/blob/master/toy-density-estimation/colab_vanilla_diffusion_model.ipynb" target="_blank" >
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Colab (Large)">
</a></td>
</tr>
</tbody>
</table>
<h2 id="what-are-denoising-diffusion-models">What are Denoising Diffusion Models?</h2>
<p>Denoising Diffusion Models, commonly referred to as &ldquo;<strong>Diffusion models</strong>&rdquo;, are a class of generative models based on the <strong>Variational Auto Encoder</strong> (VAE) architecture. These models are called <em>likelihood-based models</em> because they assign a high likelihood to the observed data samples $p(X)$. In contrast to other <em>generative models</em>, such as GANs, which learn the sampling process of a complex distribution and are trained adversarially.</p>
<p>These models are currently the State of the art for image generation. Images generated by diffusion models are photo-realistic, we can even tell the model to generate objects by giving it prompts. Diffusion models can be used to generate distributions coming from non-image domains and have been successfully applied in <cite>speech<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></cite>, NLP, time-series modelling etc. <cite>Survey of Applications <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></cite> discusses various other areas in which diffusion models have been used.</p>
<h3 id="vaes">VAE&rsquo;s</h3>
<p>Let&rsquo;s spend a bit of time on VAEs, as it will help with some intuition. VAEs are unsupervised models that are used to learn a latent representation for an input. It&rsquo;s an auto-encoder.
VAEs are composed of two processes: an <strong>encoder</strong> ($q$) (also referred to as the inference model), which generates a latent representation ($z$) of the input data ($x_0$), and a <strong>decoder</strong> ($p$) (also referred to as the generator), which generates the input data ($\hat x_0$) using the latent representation ($z$) as input. The encoder and decoder are trained together using a <em>variational objective</em>, referred to as the <em>ELBO</em>(Evidence Lower Bound). ELBO is a lower bound of the data likelihood $p(X)$.</p>
<figure>
    <img loading="lazy" src="/images/vae-max.png"
         alt="Figure 1: An architecture for a Variational Auto Encoder. (Image source: VAE tutorial, Kingma et.al; 2019)" width="100%"/> <figcaption>
            <p>Figure 1: An architecture for a Variational Auto Encoder. (Image source: <a href="https://arxiv.org/abs/1906.02691" target="_blank" >VAE tutorial, Kingma et.al; 2019</a>)</p>
        </figcaption>
</figure>

<figure class="align-center ">
    <img loading="lazy" src="/images/vae-graphical-representation.png#center"
         alt="Figure 2: Graphical representation of a Variational Auto Encoder. The $p$ function is the decoder and the $q$ function is the encoder. (Image source: Calvin Luo; 2022)" width="50%"/> <figcaption>
            <p>Figure 2: Graphical representation of a Variational Auto Encoder. The $p$ function is the decoder and the $q$ function is the encoder. (Image source: <a href="https://arxiv.org/abs/2208.11970" target="_blank" >Calvin Luo; 2022</a>)</p>
        </figcaption>
</figure>

<p>Figures 1 &amp; 2 give a simplistic representation of a VAE model.</p>
<h3 id="denoising-diffusion-models">Denoising Diffusion Models</h3>
<p>Analogous to VAEs, Denoising Diffusion models also consist of two processes: <strong>Diffusion</strong>, which is analogous to the VAE encoder, and <strong>Denoising</strong>, which is analogous to the VAE decoder.</p>
<ul>
<li><strong>Diffusion</strong>: The diffusion process repeatedly samples random noise and corrupts our input data by adding the noise in $T$ steps. In contrast to the VAE encoder, we typically do not learn this process. At the end of diffusion, at step $T$, the data would be so corrupted that it&rsquo;s just noise $N(0, I)$.</li>
</ul>
<figure>
    <img loading="lazy" src="/images/diffusion-z.png"
         alt="Figure 3: Illustration of the Diffusion Process. Image of &amp;lsquo;Z&amp;rsquo; char is corrupted step by step." width="100%"/> <figcaption>
            <p>Figure 3: Illustration of the Diffusion Process. Image of &lsquo;Z&rsquo; char is corrupted step by step.</p>
        </figcaption>
</figure>

<ul>
<li><strong>Denoising</strong>: The denoising is done by a learned model that takes <em>completely noisy data</em> and tries to generate the input data by repeatedly (over $t$ steps: $1$ to $T$) removing noise from the noisy data.
<figure>
    <img loading="lazy" src="/images/denoising-z.png"
         alt="Figure 4: Illustration of the Denoising Process. Image of &amp;lsquo;Z&amp;rsquo; character is generated step by step." width="100%"/> <figcaption>
            <p>Figure 4: Illustration of the Denoising Process. Image of &lsquo;Z&rsquo; character is generated step by step.</p>
        </figcaption>
</figure>
</li>
</ul>
<p>These models are able to generate images from pure noise.</p>
<blockquote>
<p>Humans paint, they are able to generate images in a stepwise manner by painting on a blank canvas. Diffusion models are similar, they generate images in a stepwise manner by <em>denoising</em> a noisy canvas.</p>
</blockquote>
<p>At this point, it may be helpful to look at the different models for generative modelling.</p>
<figure>
    <img loading="lazy" src="/images/generative_models.png"
         alt="Comparing different generative models. (Image source: Diffusion Models tutorial, Cao et.al; 2022)" width="100%"/> <figcaption>
            <p>Comparing different generative models. (Image source: <a href="https://arxiv.org/abs/2209.02646" target="_blank" >Diffusion Models tutorial, Cao et.al; 2022</a>)</p>
        </figcaption>
</figure>

<h2 id="introduction-to-this-series">Introduction to this series</h2>
<p>I have spent too much time understanding diffusion models, which started with some wild posts I saw on Twitter.</p>

  <blockquote class="twitter-tweet" data-dnt="true"><p lang="en" dir="ltr">Introducing Imagen, a new text-to-image synthesis model that can generate high-fidelity, photorealistic images from a deep level of language understanding. Learn more and and check out some examples of <a href="https://twitter.com/hashtag/imagen?src=hash&amp;ref_src=twsrc%5Etfw">#imagen</a> at <a href="https://t.co/RhD6siY6BY">https://t.co/RhD6siY6BY</a> <a href="https://t.co/C8javVu3iW">pic.twitter.com/C8javVu3iW</a></p>&mdash; Google AI (@GoogleAI) <a href="https://twitter.com/GoogleAI/status/1529165219997528064?ref_src=twsrc%5Etfw">May 24, 2022</a></blockquote>

<figure class="align-center ">
    <img loading="lazy" src="/images/diffusion-01.jpg#center"
         alt="Imagen Google AI Diffusion model. (Image source: Imagen Web Link)" width="60%"/> <figcaption>
            <p>Imagen Google AI Diffusion model. (Image source: <a href="https://imagen.research.google/" target="_blank" >Imagen Web Link</a>)</p>
        </figcaption>
</figure>

<p>My curiosity led to experiments, which enriched my ML skill-set. In this series, I will try to demystify some magic behind diffusion models. 
  <blockquote class="twitter-tweet" data-dnt="true"><p lang="en" dir="ltr">Finally read a tutorial on Diffusion models. Now, I understand how they work just not why they work. To me it&#39;s crazy that they do. <a href="https://t.co/LXm5OrD993">pic.twitter.com/LXm5OrD993</a></p>&mdash; Varun Tulsian (@varuntul22) <a href="https://twitter.com/varuntul22/status/1577715572271611904?ref_src=twsrc%5Etfw">October 5, 2022</a></blockquote>
</p>
<h3 id="diffusion-models-series">Diffusion Models Series</h3>
<p>In this series, I will attempt to simplify diffusion model concepts and provide you with some code that you can easily run on Google Colab or your local Jupyter server. The code requires minimal setup. We will be working on a 2D dataset (generated using scikit-learn) and also the <a href="https://www.tensorflow.org/datasets/catalog/emnist" target="_blank" >EMNIST dataset (28x28x1 images)</a>.</p>
<p>This is the first of 3 posts on diffusion models. You can check all the posts in the <a href="/tags/diffusion-model-series/" >Full Diffusion Model Series</a>. All the code for the diffusion model series is available <a href="/posts/diffusion-models/diffusion-models-notebooks/" >here</a>.</p>
<p>The first 2 parts of the series will focus on setting up the basic concepts and code. You won&rsquo;t need a GPU to run the code. The code is written in PyTorch.</p>
<ul>
<li>
<p><a href="/posts/diffusion-models/denoising-diffusion-models-1" ><strong>Part 1</strong></a>: I will introduce the basics of the denoising approach for the diffusion model. We will predict the original distribution directly, following the <cite>first part of Luo, 2022.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></cite></p>
</li>
<li>
<p><a href="/posts/diffusion-models/denoising-diffusion-models-2" ><strong>Part 2</strong></a>: I will introduce optimizations, predicting error distribution, time step embedding Aka <cite>Attention is all you need<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup></cite>, that have been shown to work better. We will also look at class conditioned guidance (<em>classifier free guidance</em>) and steps to generate distributions faster using <em>striding</em>. This will correspond to the <cite>second part (Three Equivalent Interpretations) of Luo, 2022.<sup id="fnref1:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> </cite></p>
</li>
</ul>
<ul>
<li><a href="/posts/diffusion-models/denoising-diffusion-models-3" ><strong>Part 3</strong></a>: In the last part of the series, we will be using the concepts learned to implement diffusion models for character generation by training a U-Net model over the <a href="https://www.tensorflow.org/datasets/catalog/emnist" target="_blank" >Extended-MNIST dataset</a>. The code is written in <a href="https://jax.readthedocs.io/" target="_blank" >JAX</a>, <a href="https://dm-haiku.readthedocs.io/" target="_blank" >Haiku</a>. This may serve as a good introduction to Jax and Haiku for the uninitiated.</li>
</ul>
<p>In addition, I have also curated a list of high-quality blogs that I have found helpful, they can be found <a href="/posts/diffusion-models/bonus-denoising-diffusion-models-resources/" >here</a>.</p>
<h2 id="what-are-diffusion-models">What are Diffusion Models?</h2>
<p>Let&rsquo;s go over this again, this time in a little more detail. We will start seeing mathematical equations and PyTorch code below.</p>
<figure class="align-center ">
    <img loading="lazy" src="/images/diffusion-graphical-representation.png#center"
         alt="Figure 5: Graphical representation of a Denoising Diffusion Model. The decoder is the $p$ function, the encoder is the $q$ function. (Image source: Calvin Luo; 2022)" width="80%"/> <figcaption>
            <p>Figure 5: Graphical representation of a Denoising Diffusion Model. The decoder is the $p$ function, the encoder is the $q$ function. (Image source: <a href="https://arxiv.org/abs/2208.11970" target="_blank" >Calvin Luo; 2022</a>)</p>
        </figcaption>
</figure>

<p>Denoising Diffusion models are a Markovian Hierarchical Variational Auto Encoder, unlike a standard VAE model the encoder process (diffusion) and the decoder process (denoising) occur in multiple steps.
Figure 5 depicts the diffusion process ($q$), which begins with a random variable $x$ and generates random variables $x_t$ at the $t^{th}$ step. The denoising process ($p$) starts at $x_T$ and attempts to generate $\hat x_t$ and ultimately $\hat x_0$. Let&rsquo;s call a step in the Diffusion Process as a <strong>Diffusion Step</strong>, and a step in the  Denoising Process as a <strong>Denoising Step</strong>.</p>
<p>We will be going over the <em>Diffusion Step</em> and the <em>Denoising Step,</em> and the <em>training procedure</em>.  But first, let&rsquo;s build an understanding of what needs to be done to implement such a model.</p>
<p><strong>Training a Denoising Diffusion model</strong></p>
<p>This is the main source of confusion when it comes to understanding diffusion models. From the description above and the analogy with the VAEs, it would seem that training a diffusion model would consist of the following steps.</p>
<ol>
<li>Using some data as an input ($x_0$) from the training data.</li>
<li>Perform a Forward Pass (Diffusion), generating $x_t$&rsquo;s in $T$ steps.</li>
<li>Run the noisy image ($x_T$) to the Backward pass (Denoising) to get $\hat x_0$ in $T$ steps. At each step using a $NN(\hat x_t, t)$ and the output should be $\hat x_{t-1}$.</li>
<li>At the end of the full passes, performing a weight update of the model $NN$ after computing a loss-based $L(x_0, \hat x_0)$.</li>
<li>Repeat steps 1 through 4.</li>
</ol>
<p>This would be a perfectly valid approach, however, we can do much better <span class="emojify">🙈</span>. First, let&rsquo;s investigate the issues with this approach. For every input in our dataset, this approach requires us to go through all the time steps, apply diffusion, and then go through the whole denoising steps. The learning (weight updates) only happen at the end of the both the passes. Training in such a way would be slow, and we would not be able to meaningfully learn anything useful.</p>
<p>Instead, we can show that we can effectively learn the distribution $p(X)$ while just doing the following steps.</p>
<ol>
<li>Using some data as an input ($x_0$) from the training data.
<ul>
<li>Take uniform and random samples of a time variable $t$ ranging from $1,to,T$.</li>
</ul>
</li>
<li>Compute the latent variable $x_t$ in a single step. <a href="/posts/diffusion-models/denoising-diffusion-models-1/#1-diffusion-step" >Refer to the diffusion step section</a>.</li>
<li>Apply the $NN$ model to the noisy image ($x_t$) to obtain $\hat x_t^0$. The result of $NN(\hat x_t, t)$ is $\hat x_t^0$. We will not go over each time step. <a href="/posts/diffusion-models/denoising-diffusion-models-1/#2-denoising-step" >Refer to the denoising step section</a>.</li>
</ol>
<blockquote>
<p>Note about notation: $\hat x_t^0$ is the predicted reconstruction of the input $x_0$ at time step $t$.</p>
</blockquote>
<ol start="4">
<li>Performing a weight update of the model $NN$ after computing a loss $L(x, \hat x_t^0, t)$.</li>
<li>Repeat steps 1 through 4.</li>
</ol>
<p>The proof requires us to reduce the ELBO loss, making use of the Markovian assumption in the diffusion model architecture, and using Monte-Carlo estimates to obtain an equivalent loss. I won&rsquo;t go into details on this proof, please refer <cite>Section: Variational Diffusion Models, equation 100 gives the reduced loss <sup id="fnref2:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></cite>. Would highly recommend you do.</p>
<p>We have made significant progress, now we don&rsquo;t need to go over the entire forward and backward pass before making an update to the model. This will help speed up training significantly.</p>
<p>When we want to generate a new data from noise, we would still go through the full denoising process as it&rsquo;s helps with the quality of the samples. Generation of new samples through diffusion models is slow. This is an active area of research and various approaches have been proposed to this faster. In my code, I employ <a href="/posts/diffusion-models/denoising-diffusion-models-3/#striding-reducing-steps-needed-for-generation" ><em>time step striding</em></a>, where we take multiple steps at the same time.</p>
<p><strong>Pseudocode:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-0-1"><a class="lnlinks" href="#hl-0-1"> 1</a>
</span><span class="lnt" id="hl-0-2"><a class="lnlinks" href="#hl-0-2"> 2</a>
</span><span class="lnt" id="hl-0-3"><a class="lnlinks" href="#hl-0-3"> 3</a>
</span><span class="lnt" id="hl-0-4"><a class="lnlinks" href="#hl-0-4"> 4</a>
</span><span class="lnt" id="hl-0-5"><a class="lnlinks" href="#hl-0-5"> 5</a>
</span><span class="lnt" id="hl-0-6"><a class="lnlinks" href="#hl-0-6"> 6</a>
</span><span class="lnt" id="hl-0-7"><a class="lnlinks" href="#hl-0-7"> 7</a>
</span><span class="lnt" id="hl-0-8"><a class="lnlinks" href="#hl-0-8"> 8</a>
</span><span class="lnt" id="hl-0-9"><a class="lnlinks" href="#hl-0-9"> 9</a>
</span><span class="lnt" id="hl-0-10"><a class="lnlinks" href="#hl-0-10">10</a>
</span><span class="lnt" id="hl-0-11"><a class="lnlinks" href="#hl-0-11">11</a>
</span><span class="lnt" id="hl-0-12"><a class="lnlinks" href="#hl-0-12">12</a>
</span><span class="lnt" id="hl-0-13"><a class="lnlinks" href="#hl-0-13">13</a>
</span><span class="lnt" id="hl-0-14"><a class="lnlinks" href="#hl-0-14">14</a>
</span><span class="lnt" id="hl-0-15"><a class="lnlinks" href="#hl-0-15">15</a>
</span><span class="lnt" id="hl-0-16"><a class="lnlinks" href="#hl-0-16">16</a>
</span><span class="lnt" id="hl-0-17"><a class="lnlinks" href="#hl-0-17">17</a>
</span><span class="lnt" id="hl-0-18"><a class="lnlinks" href="#hl-0-18">18</a>
</span><span class="lnt" id="hl-0-19"><a class="lnlinks" href="#hl-0-19">19</a>
</span><span class="lnt" id="hl-0-20"><a class="lnlinks" href="#hl-0-20">20</a>
</span><span class="lnt" id="hl-0-21"><a class="lnlinks" href="#hl-0-21">21</a>
</span><span class="lnt" id="hl-0-22"><a class="lnlinks" href="#hl-0-22">22</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">diffusion</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="n">code</span> <span class="n">to</span> <span class="n">add</span> <span class="n">noise</span> <span class="n">to</span> <span class="n">x_0</span>
</span></span><span class="line"><span class="cl">	<span class="k">return</span> <span class="n">x_i</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">training</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="n">loop</span> <span class="n">until</span> <span class="n">convergence</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">		<span class="n">pick</span> <span class="n">an</span> <span class="n">image</span> <span class="n">x_0</span> <span class="kn">from</span> <span class="nn">X</span> <span class="p">(</span><span class="n">batch</span> <span class="n">of</span> <span class="n">images</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">sample</span> <span class="n">t</span> <span class="kn">from</span> <span class="mi">1</span> <span class="n">to</span> <span class="n">T</span>
</span></span><span class="line"><span class="cl">		<span class="n">x_t</span> <span class="o">=</span> <span class="n">diffusion_step</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">x_hat_t</span> <span class="o">=</span> <span class="n">NN</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">		<span class="n">loss_</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">x_hat_t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">update</span><span class="p">(</span><span class="n">NN</span><span class="p">,</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss_</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># generating new data points through denoising steps</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_new_data</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">	<span class="n">sample</span> <span class="n">x_T</span> <span class="kn">from</span> <span class="nn">N</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">I</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># denoising step</span>
</span></span><span class="line"><span class="cl">		<span class="n">x_hat_t</span> <span class="o">=</span> <span class="n">NN</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">		<span class="c1"># get x_{t-1}</span>
</span></span><span class="line"><span class="cl">		<span class="n">x_</span><span class="p">{</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">x_hat_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="c1"># refer EQ - 12</span>
</span></span><span class="line"><span class="cl">	<span class="n">x_hat_0</span> <span class="o">=</span> <span class="n">x_0</span>  
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="1-diffusion-step">1. Diffusion Step</h3>
<p>The Diffusion steps add noise to the input vector.<br>
At each time step $t$ in the diffusion process, we sample from the latent variable $x_t$.
One way to do this is the following :

$$
q_t(x_t|x_{t-1}) = N(\sqrt\alpha_tx_{t-1}, (1 - \alpha_t )I)  
$$    

Note: The diffusion process is annotated as $q$.</p>
<p>At each time step, we are only using $\sqrt\alpha$ times the previous signal $x_{t-1}$ and adds additional noise to the data in a way such that the latent variables stay at a similar scale.
We have some additional conditions:</p>
<ul>
<li>$\alpha_t$, $t\in[1, T]$, where $\alpha_t &lt; 1$, <em>the diffusion schedule</em>.</li>
<li>$\alpha_{t-1} &lt; \alpha_t$, as we go along in the diffusion process, we are adding more and more noise.</li>
</ul>
<p>At this point, let me introduce the re-parameterization trick.

$$  
N(\sqrt\alpha_tx_{t-1}, (1 - \alpha_t )I) = \sqrt\alpha_tx_{t-1} + (1 - \alpha_t)*\epsilon \quad with \,\,  \epsilon \sim N(0, I)  
$$  
</p>
<p>The re-paremetarization trick simplifies sampling from a Gaussian distribution. If you want to sample from the diffusion step at $t$, you can simply sample from a standard Gaussian distribution $N(0, I)$ and plug in the mean and variance.
Consider another interesting result: suppose we want to sample from the latent $x_t$ ($q(x_t|x_0)$) directly given only the input $x_0$ without having to do it in $t$ steps. We can do so using the re-parameterization trick.</p>
<p>
$$  
\begin{align} 
q(x_t|x_0) &= N(\sqrt\alpha_tx_{t-1}, (1 - \alpha_t )I) \cr   
&= \sqrt\alpha_t x_{t-1} + \sqrt{(1-\alpha_t)}\ast\epsilon_t \cr   
&= \sqrt\alpha_t(\sqrt\alpha_t x_{t-2} + \sqrt{(1-\alpha_{t-1})}\ast\epsilon_{t-1}) + \sqrt(1-\alpha_t)\ast\epsilon_t \cr  
&= \sqrt\alpha_t\sqrt\alpha_t x_{t-2} + \sqrt\alpha_t\sqrt{(1-\alpha_{t-1})}\ast\epsilon_{t-1} + \sqrt(1-\alpha_t)\ast\epsilon_t \cr   
&= \sqrt\alpha_t\sqrt\alpha_tx_{t-2} + \sqrt{(1-\alpha_t\alpha_{t-1})}\ast\epsilon_{t-1}^\ast \quad where \thinspace \epsilon_{t-1}^\ast\in N(0, I) \cr  
&= ... \cr  
&= \sqrt{\bar\alpha_t}x_0 + \sqrt{(1 - \bar\alpha_t )}\ast\epsilon_0^\ast ; \space where \space \bar\alpha_t=\Pi_{i=1}^T{\sqrt\alpha_i}, \space \epsilon_0^\ast \in N(0, I) \cr  
&= N(\sqrt{\bar\alpha_t}x_0, (1 - \bar\alpha_t)I)\cr 
\end{align}  
$$  
<br>
In equation 5, we have utilized <a href="https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables" target="_blank" >sum of two independent Gaussian random variables.</a><br>
The latent variable $x_t$ follows a Gaussian distribution, its mean is $\sqrt{\bar\alpha_t}x_0$ and variance is $(1 - \bar\alpha_t)$.
The mean is a function of time step $t$ and the input $x_0$, The variance is only a function of the time step.</p>
<p>With this in place, let&rsquo;s put some code together.</p>
<p><strong>The diffusion schedule:</strong> $\alpha$</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-1-1"><a class="lnlinks" href="#hl-1-1"> 1</a>
</span><span class="lnt" id="hl-1-2"><a class="lnlinks" href="#hl-1-2"> 2</a>
</span><span class="lnt" id="hl-1-3"><a class="lnlinks" href="#hl-1-3"> 3</a>
</span><span class="lnt" id="hl-1-4"><a class="lnlinks" href="#hl-1-4"> 4</a>
</span><span class="lnt" id="hl-1-5"><a class="lnlinks" href="#hl-1-5"> 5</a>
</span><span class="lnt" id="hl-1-6"><a class="lnlinks" href="#hl-1-6"> 6</a>
</span><span class="lnt" id="hl-1-7"><a class="lnlinks" href="#hl-1-7"> 7</a>
</span><span class="lnt" id="hl-1-8"><a class="lnlinks" href="#hl-1-8"> 8</a>
</span><span class="lnt" id="hl-1-9"><a class="lnlinks" href="#hl-1-9"> 9</a>
</span><span class="lnt" id="hl-1-10"><a class="lnlinks" href="#hl-1-10">10</a>
</span><span class="lnt" id="hl-1-11"><a class="lnlinks" href="#hl-1-11">11</a>
</span><span class="lnt" id="hl-1-12"><a class="lnlinks" href="#hl-1-12">12</a>
</span><span class="lnt" id="hl-1-13"><a class="lnlinks" href="#hl-1-13">13</a>
</span><span class="lnt" id="hl-1-14"><a class="lnlinks" href="#hl-1-14">14</a>
</span><span class="lnt" id="hl-1-15"><a class="lnlinks" href="#hl-1-15">15</a>
</span><span class="lnt" id="hl-1-16"><a class="lnlinks" href="#hl-1-16">16</a>
</span><span class="lnt" id="hl-1-17"><a class="lnlinks" href="#hl-1-17">17</a>
</span><span class="lnt" id="hl-1-18"><a class="lnlinks" href="#hl-1-18">18</a>
</span><span class="lnt" id="hl-1-19"><a class="lnlinks" href="#hl-1-19">19</a>
</span><span class="lnt" id="hl-1-20"><a class="lnlinks" href="#hl-1-20">20</a>
</span><span class="lnt" id="hl-1-21"><a class="lnlinks" href="#hl-1-21">21</a>
</span><span class="lnt" id="hl-1-22"><a class="lnlinks" href="#hl-1-22">22</a>
</span><span class="lnt" id="hl-1-23"><a class="lnlinks" href="#hl-1-23">23</a>
</span><span class="lnt" id="hl-1-24"><a class="lnlinks" href="#hl-1-24">24</a>
</span><span class="lnt" id="hl-1-25"><a class="lnlinks" href="#hl-1-25">25</a>
</span><span class="lnt" id="hl-1-26"><a class="lnlinks" href="#hl-1-26">26</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">timestepts</span> <span class="o">=</span> <span class="mi">200</span>  
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## linear schedule  </span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">linear_beta_schedule</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>  
</span></span><span class="line"><span class="cl"> <span class="n">beta_start</span> <span class="o">=</span> <span class="mf">0.0001</span> 
</span></span><span class="line"><span class="cl"> <span class="n">beta_end</span> <span class="o">=</span> <span class="mf">0.02</span> 
</span></span><span class="line"><span class="cl"> <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">beta_start</span><span class="p">,</span> <span class="n">beta_end</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## cosine schedule as proposed in https://arxiv.org/abs/2102.09672  </span>
</span></span><span class="line"><span class="cl"><span class="c1">## The cosine schedule is recommened if timesteps &gt;&gt; 200. As it results in a gradual noisification of the input data  </span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">cosine_beta_schedule</span><span class="p">(</span><span class="n">timesteps</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">0.008</span><span class="p">):</span>  
</span></span><span class="line"><span class="cl"> <span class="n">steps</span> <span class="o">=</span> <span class="n">timesteps</span> <span class="o">+</span> <span class="mi">1</span> 
</span></span><span class="line"><span class="cl"> <span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="n">steps</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"> <span class="n">alphas_cumprod</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">cos</span><span class="p">(((</span><span class="n">x</span> <span class="o">/</span> <span class="n">timesteps</span><span class="p">)</span> <span class="o">+</span> <span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">s</span><span class="p">)</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> 
</span></span><span class="line"><span class="cl"> <span class="n">alphas_cumprod</span> <span class="o">=</span> <span class="n">alphas_cumprod</span> <span class="o">/</span> <span class="n">alphas_cumprod</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 
</span></span><span class="line"><span class="cl"> <span class="n">betas</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">alphas_cumprod</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">/</span> <span class="n">alphas_cumprod</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> 
</span></span><span class="line"><span class="cl"> <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## some handy variables  </span>
</span></span><span class="line"><span class="cl"><span class="n">betas</span> <span class="o">=</span> <span class="n">linear_beta_schedule</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl"><span class="n">alphas</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">betas</span>  
</span></span><span class="line"><span class="cl"><span class="n">alphas_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl"><span class="n">variance</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alphas_</span>  
</span></span><span class="line"><span class="cl"><span class="n">sd</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variance</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl"><span class="n">alphas_prev_</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">alphas_</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&#34;constant&#34;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>Diffusion Step:</strong> Given input data $x_0$, a time step $t$ and a schedule, the diffusion method should return the latent variable $x_t$.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-2-1"><a class="lnlinks" href="#hl-2-1">1</a>
</span><span class="lnt" id="hl-2-2"><a class="lnlinks" href="#hl-2-2">2</a>
</span><span class="lnt" id="hl-2-3"><a class="lnlinks" href="#hl-2-3">3</a>
</span><span class="lnt" id="hl-2-4"><a class="lnlinks" href="#hl-2-4">4</a>
</span><span class="lnt" id="hl-2-5"><a class="lnlinks" href="#hl-2-5">5</a>
</span><span class="lnt" id="hl-2-6"><a class="lnlinks" href="#hl-2-6">6</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># how to add noise to the data  </span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_noisy</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">timestep</span><span class="p">):</span>  
</span></span><span class="line"><span class="cl"> <span class="c1"># we will use the reparameterization trick </span>
</span></span><span class="line"><span class="cl"> <span class="n">noise_at_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">batch</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> 
</span></span><span class="line"><span class="cl"> <span class="n">added_noise_at_t</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alphas_</span><span class="p">[</span><span class="n">timestep</span><span class="p">]))</span> <span class="o">+</span> <span class="n">noise_at_t</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">sd</span><span class="p">[</span><span class="n">timestep</span><span class="p">])</span> 
</span></span><span class="line"><span class="cl"> <span class="k">return</span> <span class="n">added_noise_at_t</span><span class="p">,</span> <span class="n">noise_at_t</span>  
</span></span></code></pre></td></tr></table>
</div>
</div><p>In Variational Diffusion Models, <cite>Kingma et.al, 2022<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></cite> propose a way to learn the parameters of the schedule and provide additional insights helpful in understanding diffusion models.</p>
<h3 id="2-denoising-step">2. Denoising Step</h3>
<p>Let&rsquo;s recap, the denoising process ($p$) is responsible to generate synthetic data $\hat x_0$.</p>
<ol>
<li>Start with a completely noisy data, $x_T = N(0, I)$</li>
<li>Perform a Denoising Step:
<ol>
<li>Uses a Neural Network to predict $\hat x_T^0$.</li>
<li>We will use $\hat x_T^0$ to generate the latent $\hat x_{T-1}$.</li>
</ol>
</li>
<li>Repeat step 2 with input $\hat x_{T-1}$ to get $\hat x_{T-2}$ until t=1</li>
</ol>
<p>Refer, our mathematical setup in Figure 5. Let&rsquo;s look at the mathematical form that a denoising step $p(\hat x_t|\hat x_{t+1})$. This is also called the <strong>posterior distribution</strong>.</p>
<p>Because the diffusion process is well-defined, let&rsquo;s work out what a backward transition in the diffusion process:

$$
\begin{align}
q(x_{t-1}|x_{t}) = \frac{q(x_t|x_{t-1})\ast q(x_{t-1}|x_0)}{q(x_t|x_0)}\quad \text{Baye's theorem \&} \, \text{Note: } x_0 = x \cr 
= ... \cr
\varpropto N(x_{t-1};\underbrace{\frac{\sqrt\alpha_t(1-\bar\alpha_{t-1})x_t + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)x_0}{1-\bar\alpha_t}}_{\mu_q(x_t,x_0)}, \underbrace{\frac{(1-\alpha_t)(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}I}_{\sum_q(t)}) \cr
\end{align} 
$$

For the full derivation please refer <cite>Equation #71; Calvin Luo&rsquo;s tutorial<sup id="fnref3:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></cite></p>
<p>It is convenient to assume that the denoising step $p(\hat x_{t-1}|\hat x_t)$ also follows a Gaussian distribution like the forward process $q(x_{t-1}|x_{t})$. Note: we don&rsquo;t need to necessarily make this assumption. It&rsquo;s an <a href="https://towardsdatascience.com/the-inductive-bias-of-ml-models-and-why-you-should-care-about-it-979fe02a1a56" target="_blank" >inductive bias</a> that we are adding to the system and helps with the stability of training the model or help the model to converge faster.</p>
<p>We will assume the following:</p>
<ol>
<li>The mean $\mu_{p(x_{t-1}|x_{t})}$ ($\mu_p$ for short) is dependent on the input $x_0$ ($x_0$ is not known in the denoising step), thus the denoising Neural Network is tasked to make a prediction for the $\hat x_t^0$.</li>
<li>The variance is not dependent on the data. We will assume it&rsquo;s fixed and is only a function of the time step, $\sum_{p(x_{t-1}|x_t)} = \sum_q(t)$.</li>
</ol>
<p>This gives us the following equation for the denoising step:

$$
\begin{align}
p(\hat x_{t-1}| \hat x_t) \varpropto N(\hat x_{t-1};\frac{\sqrt\alpha_t(1-\bar\alpha_{t-1})\hat x_t + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)\hat x_t^0}{1-\bar\alpha_t}, \frac{(1-\alpha_t)(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}I)	
\end{align}
$$
</p>
<blockquote>
<p>As show in <cite>Improved Denoising Diffusion Probabilistic Models.<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup></cite> We could alternatively learn the posterior variance. In this case, we will make the neural network output the variance as well as the mean. If the data is d dimensional, the $NN$ will output 2d dimensions. First d dimensions for the mean, the 2nd d dimensions for the variance.</p>
</blockquote>
<p><strong>Diffusion: With Fixed Variance:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-3-1"><a class="lnlinks" href="#hl-3-1"> 1</a>
</span><span class="lnt" id="hl-3-2"><a class="lnlinks" href="#hl-3-2"> 2</a>
</span><span class="lnt" id="hl-3-3"><a class="lnlinks" href="#hl-3-3"> 3</a>
</span><span class="lnt" id="hl-3-4"><a class="lnlinks" href="#hl-3-4"> 4</a>
</span><span class="lnt" id="hl-3-5"><a class="lnlinks" href="#hl-3-5"> 5</a>
</span><span class="lnt" id="hl-3-6"><a class="lnlinks" href="#hl-3-6"> 6</a>
</span><span class="lnt" id="hl-3-7"><a class="lnlinks" href="#hl-3-7"> 7</a>
</span><span class="lnt" id="hl-3-8"><a class="lnlinks" href="#hl-3-8"> 8</a>
</span><span class="lnt" id="hl-3-9"><a class="lnlinks" href="#hl-3-9"> 9</a>
</span><span class="lnt" id="hl-3-10"><a class="lnlinks" href="#hl-3-10">10</a>
</span><span class="lnt" id="hl-3-11"><a class="lnlinks" href="#hl-3-11">11</a>
</span><span class="lnt" id="hl-3-12"><a class="lnlinks" href="#hl-3-12">12</a>
</span><span class="lnt" id="hl-3-13"><a class="lnlinks" href="#hl-3-13">13</a>
</span><span class="lnt" id="hl-3-14"><a class="lnlinks" href="#hl-3-14">14</a>
</span><span class="lnt" id="hl-3-15"><a class="lnlinks" href="#hl-3-15">15</a>
</span><span class="lnt" id="hl-3-16"><a class="lnlinks" href="#hl-3-16">16</a>
</span><span class="lnt" id="hl-3-17"><a class="lnlinks" href="#hl-3-17">17</a>
</span><span class="lnt" id="hl-3-18"><a class="lnlinks" href="#hl-3-18">18</a>
</span><span class="lnt" id="hl-3-19"><a class="lnlinks" href="#hl-3-19">19</a>
</span><span class="lnt" id="hl-3-20"><a class="lnlinks" href="#hl-3-20">20</a>
</span><span class="lnt" id="hl-3-21"><a class="lnlinks" href="#hl-3-21">21</a>
</span><span class="lnt" id="hl-3-22"><a class="lnlinks" href="#hl-3-22">22</a>
</span><span class="lnt" id="hl-3-23"><a class="lnlinks" href="#hl-3-23">23</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># fixed posterior variance</span>
</span></span><span class="line"><span class="cl"><span class="n">posterior_variance</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas_prev_</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_data</span><span class="p">(</span><span class="n">denoising_model</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># generating multiple samples at the same time</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size_generation</span> <span class="o">=</span> <span class="mi">2048</span><span class="o">*</span><span class="mi">5</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># x_T -- we will start from a Noisy sample</span>
</span></span><span class="line"><span class="cl">    <span class="n">data_noisy</span> <span class="o">=</span>  <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_size_generation</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data_noisy</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">/</span><span class="n">batch_size_generation</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">data_in_batch</span> <span class="o">=</span> <span class="n">data_noisy</span><span class="p">[:,</span> <span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batch_size_generation</span><span class="p">:(</span><span class="n">n</span><span class="o">*</span><span class="n">batch_size_generation</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">timestep</span> <span class="o">=</span> <span class="n">timesteps</span><span class="o">-</span><span class="n">t</span>
</span></span><span class="line"><span class="cl">            <span class="n">t_repeated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="n">timestep</span><span class="p">])</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch_size_generation</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">data_stacked</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">data_in_batch</span><span class="p">,</span> <span class="n">t_repeated</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># x_hat_0 prediction at time t</span>
</span></span><span class="line"><span class="cl">            <span class="n">pred_data</span> <span class="o">=</span> <span class="n">denoising_model</span><span class="p">(</span><span class="n">data_stacked</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># implementing equation above to get x_(t-1) from x_t and x_0</span>
</span></span><span class="line"><span class="cl">            <span class="n">mean_data_1</span> <span class="o">=</span> <span class="n">data_in_batch</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alphas</span><span class="p">[</span><span class="n">timestep</span><span class="p">])</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alphas_prev_</span><span class="p">[</span><span class="n">timestep</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="n">variance</span><span class="p">[</span><span class="n">timestep</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">            <span class="n">mean_data_2</span> <span class="o">=</span> <span class="n">pred_data</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alphas_prev_</span><span class="p">[</span><span class="n">timestep</span><span class="p">])</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alphas</span><span class="p">[</span><span class="n">timestep</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="n">variance</span><span class="p">[</span><span class="n">timestep</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">            <span class="n">mean_data</span> <span class="o">=</span> <span class="n">mean_data_1</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">mean_data_2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">posterior_data</span> <span class="o">=</span> <span class="n">posterior_variance</span><span class="p">[</span><span class="n">timestep</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">data_in_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean_data</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">posterior_data</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">data_in_batch</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="3-training-procedure">3. Training Procedure</h3>
<p>We have just defined the diffusion method in the <a href="/posts/diffusion-models/denoising-diffusion-models-1/#hl-0-1" >pseudocode</a>. Let&rsquo;s define the loss function and the Neural Network.</p>
<h4 id="neural-network">Neural Network:</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-4-1"><a class="lnlinks" href="#hl-4-1"> 1</a>
</span><span class="lnt" id="hl-4-2"><a class="lnlinks" href="#hl-4-2"> 2</a>
</span><span class="lnt" id="hl-4-3"><a class="lnlinks" href="#hl-4-3"> 3</a>
</span><span class="lnt" id="hl-4-4"><a class="lnlinks" href="#hl-4-4"> 4</a>
</span><span class="lnt" id="hl-4-5"><a class="lnlinks" href="#hl-4-5"> 5</a>
</span><span class="lnt" id="hl-4-6"><a class="lnlinks" href="#hl-4-6"> 6</a>
</span><span class="lnt" id="hl-4-7"><a class="lnlinks" href="#hl-4-7"> 7</a>
</span><span class="lnt" id="hl-4-8"><a class="lnlinks" href="#hl-4-8"> 8</a>
</span><span class="lnt" id="hl-4-9"><a class="lnlinks" href="#hl-4-9"> 9</a>
</span><span class="lnt" id="hl-4-10"><a class="lnlinks" href="#hl-4-10">10</a>
</span><span class="lnt" id="hl-4-11"><a class="lnlinks" href="#hl-4-11">11</a>
</span><span class="lnt" id="hl-4-12"><a class="lnlinks" href="#hl-4-12">12</a>
</span><span class="lnt" id="hl-4-13"><a class="lnlinks" href="#hl-4-13">13</a>
</span><span class="lnt" id="hl-4-14"><a class="lnlinks" href="#hl-4-14">14</a>
</span><span class="lnt" id="hl-4-15"><a class="lnlinks" href="#hl-4-15">15</a>
</span><span class="lnt" id="hl-4-16"><a class="lnlinks" href="#hl-4-16">16</a>
</span><span class="lnt" id="hl-4-17"><a class="lnlinks" href="#hl-4-17">17</a>
</span><span class="lnt" id="hl-4-18"><a class="lnlinks" href="#hl-4-18">18</a>
</span><span class="lnt" id="hl-4-19"><a class="lnlinks" href="#hl-4-19">19</a>
</span><span class="lnt" id="hl-4-20"><a class="lnlinks" href="#hl-4-20">20</a>
</span><span class="lnt" id="hl-4-21"><a class="lnlinks" href="#hl-4-21">21</a>
</span><span class="lnt" id="hl-4-22"><a class="lnlinks" href="#hl-4-22">22</a>
</span><span class="lnt" id="hl-4-23"><a class="lnlinks" href="#hl-4-23">23</a>
</span><span class="lnt" id="hl-4-24"><a class="lnlinks" href="#hl-4-24">24</a>
</span><span class="lnt" id="hl-4-25"><a class="lnlinks" href="#hl-4-25">25</a>
</span><span class="lnt" id="hl-4-26"><a class="lnlinks" href="#hl-4-26">26</a>
</span><span class="lnt" id="hl-4-27"><a class="lnlinks" href="#hl-4-27">27</a>
</span><span class="lnt" id="hl-4-28"><a class="lnlinks" href="#hl-4-28">28</a>
</span><span class="lnt" id="hl-4-29"><a class="lnlinks" href="#hl-4-29">29</a>
</span><span class="lnt" id="hl-4-30"><a class="lnlinks" href="#hl-4-30">30</a>
</span><span class="lnt" id="hl-4-31"><a class="lnlinks" href="#hl-4-31">31</a>
</span><span class="lnt" id="hl-4-32"><a class="lnlinks" href="#hl-4-32">32</a>
</span><span class="lnt" id="hl-4-33"><a class="lnlinks" href="#hl-4-33">33</a>
</span><span class="lnt" id="hl-4-34"><a class="lnlinks" href="#hl-4-34">34</a>
</span><span class="lnt" id="hl-4-35"><a class="lnlinks" href="#hl-4-35">35</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DenoisingModelSequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_units</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DenoisingModelSequential</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># hidden_units = 32</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_units</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">hidden_units</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_units</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">hidden_units</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_units</span><span class="o">/</span><span class="mi">8</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">hidden_units</span><span class="o">/</span><span class="mi">8</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_units</span><span class="o">/</span><span class="mi">16</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">hidden_units</span><span class="o">/</span><span class="mi">16</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_units</span><span class="o">/</span><span class="mi">8</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">hidden_units</span><span class="o">/</span><span class="mi">8</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_units</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">hidden_units</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_units</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">hidden_units</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>   
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl"><span class="n">denoising_model</span> <span class="o">=</span> <span class="n">DenoisingModelSequential</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>The input to the Neural Network is 3-dimensional. We are working with 2d dataset, so $\hat x_t$&rsquo;s is a 2d vector. In this blog post, we are going to pass $t$ as the 3rd dimension, we will pass it as a scalar. In the subsequent posts, we will see how to generate an embedding for a time step and concatenate/fuse it with the input.
The output of the $NN$ needs to be $\hat x_t^0$ which is a 2d vector. <a href="/posts/diffusion-models/denoising-diffusion-models-1/#2-denoising-step" >Refer to the denoising step section</a>.
The $NN$ architecture doesn&rsquo;t really matter. In this case, I&rsquo;ve used a basic Multi Layer Perceptron with <cite>GeLU activation units<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup></cite>.
We will use a U-net architecture for character generation using the EMNIST dataset. <a href="/posts/diffusion-models/denoising-diffusion-models-3/" >Checkout Part 3.</a></p>
<p><strong>Loss Function</strong>: At time step $t$ the loss is defined as follows

$$
Loss(x_0, \hat x_t, t) = 1/2\ast(\frac{\bar\alpha_{t-1}}{1-\bar\alpha_{t-1}} - \frac{\bar\alpha_t}{1-\bar\alpha_t})\ast\mid\mid x_0-\hat x_0^t\mid\mid_2^2
$$


$$
\text{SNR}_t =\frac{\bar\alpha_t}{1-\bar\alpha_t}
$$

SNR stands for Signal to Noise. In the case of the diffusion, the schedule must be chosen such that $SNR_t &lt; SNT_{t-1}$.</p>
<p><strong>Loss:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-5-1"><a class="lnlinks" href="#hl-5-1">1</a>
</span><span class="lnt" id="hl-5-2"><a class="lnlinks" href="#hl-5-2">2</a>
</span><span class="lnt" id="hl-5-3"><a class="lnlinks" href="#hl-5-3">3</a>
</span><span class="lnt" id="hl-5-4"><a class="lnlinks" href="#hl-5-4">4</a>
</span><span class="lnt" id="hl-5-5"><a class="lnlinks" href="#hl-5-5">5</a>
</span><span class="lnt" id="hl-5-6"><a class="lnlinks" href="#hl-5-6">6</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># instead of the l2 loss, I use the huber loss </span>
</span></span><span class="line"><span class="cl"><span class="c1"># https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html</span>
</span></span><span class="line"><span class="cl"><span class="n">loss_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">HuberLoss</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">imp_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span> <span class="o">*</span> <span class="p">((</span><span class="n">alphas_prev_</span><span class="p">[</span><span class="n">timestep</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas_prev_</span><span class="p">[</span><span class="n">timestep</span><span class="p">]))</span> <span class="o">-</span> <span class="p">(</span><span class="n">alphas_</span><span class="p">[</span><span class="n">timestep</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas_</span><span class="p">[</span><span class="n">timestep</span><span class="p">]))))</span>
</span></span><span class="line"><span class="cl"><span class="n">loss_</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">data_in_batch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">imp_weight</span><span class="p">),</span> <span class="n">pred_data</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">imp_weight</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>In <cite>Variation Diffusion Models<sup id="fnref1:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></cite>, authors propose using a separate Neural Network to model SNR as a function of $t$.  The Neural Network should be monotonically decreasing. In this setting, we do not need to specify the diffusion schedule, it will be learned along with the diffusion model.</p>
<h2 id="lets-look-at-some-outputs">Let&rsquo;s look at some outputs</h2>
<p>In this blog post, we will play with some 2d data-points.</p>
<p><strong>Generating data set for training:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-6-1"><a class="lnlinks" href="#hl-6-1">1</a>
</span><span class="lnt" id="hl-6-2"><a class="lnlinks" href="#hl-6-2">2</a>
</span><span class="lnt" id="hl-6-3"><a class="lnlinks" href="#hl-6-3">3</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">samples</span> <span class="o">=</span> <span class="mi">1024</span><span class="o">*</span><span class="mi">128</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">samples</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mf">0.25</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span><span class="o">*</span><span class="n">x</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-7-1"><a class="lnlinks" href="#hl-7-1">1</a>
</span><span class="lnt" id="hl-7-2"><a class="lnlinks" href="#hl-7-2">2</a>
</span><span class="lnt" id="hl-7-3"><a class="lnlinks" href="#hl-7-3">3</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">circles</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_circles</span><span class="p">(</span><span class="mi">1024</span><span class="o">*</span><span class="mi">128</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">make_moons</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1024</span><span class="o">*</span><span class="mi">128</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">complex_data</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">make_moons</span> <span class="o">+</span> <span class="mi">15</span><span class="p">,</span> <span class="n">circles</span> <span class="p">])</span> 
</span></span></code></pre></td></tr></table>
</div>
</div><p>With the code fragments in the blog, you should be able to build your very own diffusion model. You can find the <a href="/posts/diffusion-models/diffusion-models-notebooks" >Jupyter Notebook here</a> in case you need some help.</p>
<p><strong>Training Data compared to Diffusion generated data:</strong></p>
<p>Let&rsquo;s look at data I was able to generate using concepts discussed in this blog.</p>
<figure class="align-center ">
    <img loading="lazy" src="/images/parabola.png#center"
         alt="Figure 6: 2D Parabola vs Diffusion generated." width="80%"/> <figcaption>
            <p>Figure 6: 2D Parabola vs Diffusion generated.</p>
        </figcaption>
</figure>

<figure class="align-center ">
    <img loading="lazy" src="/images/complex.png#center"
         alt="Figure 7: 2D Complex vs Diffusion generated." width="80%"/> <figcaption>
            <p>Figure 7: 2D Complex vs Diffusion generated.</p>
        </figcaption>
</figure>

<figure class="align-center ">
    <img loading="lazy" src="/images/complex-data.gif#center"
         alt="Figure 8: A GIF show-casing the denoising process; We start from complete noise and make small improvements step by step." width="80%"/> <figcaption>
            <p>Figure 8: A GIF show-casing the denoising process; We start from complete noise and make small improvements step by step.</p>
        </figcaption>
</figure>

<p>See you in the <a href="/posts/diffusion-models/denoising-diffusion-models-2" >next part</a>.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Popov et al. 2021 <a href="https://arxiv.org/abs/2105.06337" target="_blank" >Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Yang et al. 2022 <a href="https://arxiv.org/abs/2209.00796" target="_blank" >Diffusion Models: A Comprehensive Survey of Methods and Applications</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Calvin Luo; 2019 <a href="https://arxiv.org/abs/2208.11970" target="_blank" >&ldquo;Understanding Diffusion Models: A Unified Perspective&rdquo;</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Vaswani et al. 2017 <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank" >Attention is all you need</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Kingma et al. 2022 <a href="https://arxiv.org/abs/2107.00630" target="_blank" >&ldquo;Variational Diffusion Models&rdquo;</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Nichol et al. 2021 <a href="https://arxiv.org/abs/2102.09672" target="_blank" >Improved Denoising Diffusion Probabilistic Models</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Hendrycks et al. 2016 <a href="https://arxiv.org/abs/1606.08415" target="_blank" >Gelu&rsquo;s</a>&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Denoising Diffusion Models Resources</title>
      <link>https://varun-ml.github.io/posts/diffusion-models/bonus-denoising-diffusion-models-resources/</link>
      <pubDate>Fri, 09 Dec 2022 17:39:20 +0530</pubDate>
      
      <guid>https://varun-ml.github.io/posts/diffusion-models/bonus-denoising-diffusion-models-resources/</guid>
      <description>Here are some resources that I have found useful/interesting. Highlighting ones that I recommend going over.
Papers Paper Title Paper Link Have I Read it? DDPM DDMP Yeah Improved DDPM IDDPM Yeah Stable Diffusion Stable Diffusion No Variational Diffusion Models VDM Yeah Cold Diffusion Cold Diffusion No Understanding Diffusion Models: A Unified Perspective Tutorial Yeah Glide Glide No Diffusion Models Survey A survey on Generative Diffusion Models No Score Prediction Diffusion Models Generative Modeling by Estimating Gradients of the Data Distribution No Blogs Author Description Link Lilian Weng Comprehensive coverage of Diffusion models theory (Advanced) lil&amp;rsquo;log diffusion models Yang Song This blog is about score based generative models, specifically about SDE&amp;rsquo;s (Advanced) score based generative models AI Summer School Easy to follow but comprehensive coverage of Diffusion models ai summer school Hugging Face Annotated discussion of diffusion model with code annotated diffusion models Alex Alemi Blog on Variational Diffusion Loss variational diffusion models Google AI Blog Cascaded Diffusion Models with Super Resolution High Fidelity Image Generation Using Diffusion Models YouTube Educators Channel Description Link AI Coffee Break with Letitia Byte sized content on Diffusion models diffusion models explained Yannic DDPM paper explained DDMP explained Aleksa Gordić - The AI Epiphany ML coding series on Improved DDPM codebase coding series GitHub Repos Repo Description Repo Link Colab Diffusion Models Tutorial Wity&amp;rsquo;AI tutorial Wity&amp;rsquo;AI tutorial Stable Diffusion Stable Diffusion LucidRains Denoising Diffusion Models LucidRains Variational Diffusion models VDM DDPM DDPM YiYi XU (Flax+JAX) Flax Denoising Diffusion Glide Glide Notebooks Play with Diffusion Models Description Link Play with Stable Diffusion v2 SD II Stable Boost: Personalized Photos Stable Boost Image variations with Stable Diffusion SD variations Gradio App for Stable Diffusion GitHub Repo </description>
      <content:encoded><![CDATA[<p>Here are some resources that I have found useful/interesting.
Highlighting ones that I recommend going over.</p>
<h2 id="papers">Papers</h2>
<table>
<thead>
<tr>
<th>Paper Title</th>
<th>Paper Link</th>
<th>Have I Read it?</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>DDPM</strong></td>
<td><a href="https://arxiv.org/abs/2006.11239" target="_blank" >DDMP</a></td>
<td>Yeah</td>
</tr>
<tr>
<td><strong>Improved DDPM</strong></td>
<td><a href="https://arxiv.org/abs/2102.09672" target="_blank" >IDDPM</a></td>
<td>Yeah</td>
</tr>
<tr>
<td>Stable Diffusion</td>
<td><a href="https://arxiv.org/abs/2112.10752" target="_blank" >Stable Diffusion</a></td>
<td>No</td>
</tr>
<tr>
<td>Variational Diffusion Models</td>
<td><a href="https://arxiv.org/abs/2107.00630" target="_blank" >VDM</a></td>
<td>Yeah</td>
</tr>
<tr>
<td>Cold Diffusion</td>
<td><a href="https://arxiv.org/abs/2208.09392" target="_blank" >Cold Diffusion</a></td>
<td>No</td>
</tr>
<tr>
<td><strong>Understanding Diffusion Models: A Unified Perspective</strong></td>
<td><a href="https://arxiv.org/abs/2208.11970" target="_blank" >Tutorial</a></td>
<td>Yeah</td>
</tr>
<tr>
<td>Glide</td>
<td><a href="https://arxiv.org/abs/2112.10741" target="_blank" >Glide</a></td>
<td>No</td>
</tr>
<tr>
<td>Diffusion Models Survey</td>
<td><a href="https://arxiv.org/abs/2209.02646" target="_blank" >A survey on Generative Diffusion Models</a></td>
<td>No</td>
</tr>
<tr>
<td>Score Prediction Diffusion Models</td>
<td><a href="https://arxiv.org/abs/1907.05600" target="_blank" >Generative Modeling by Estimating Gradients of the Data Distribution</a></td>
<td>No</td>
</tr>
</tbody>
</table>
<h2 id="blogs">Blogs</h2>
<table>
<thead>
<tr>
<th>Author</th>
<th>Description</th>
<th>Link</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Lilian Weng</td>
<td><strong>Comprehensive coverage of Diffusion models theory (Advanced)</strong></td>
<td><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" target="_blank" >lil&rsquo;log diffusion models</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Yang Song</td>
<td>This blog is about score based generative models, specifically about SDE&rsquo;s (Advanced)</td>
<td><a href="https://yang-song.net/blog/2021/score/" target="_blank" >score based generative models</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>AI Summer School</td>
<td><strong>Easy to follow but comprehensive coverage of Diffusion models</strong></td>
<td><a href="https://theaisummer.com/diffusion-models/" target="_blank" >ai summer school</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Hugging Face</td>
<td>Annotated discussion of diffusion model with code</td>
<td><a href="https://huggingface.co/blog/annotated-diffusion" target="_blank" >annotated diffusion models</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Alex Alemi</td>
<td>Blog on Variational Diffusion Loss</td>
<td><a href="https://blog.alexalemi.com/diffusion.html" target="_blank" >variational diffusion models</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Google AI Blog</td>
<td>Cascaded Diffusion Models with Super Resolution</td>
<td><a href="https://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html" target="_blank" >High Fidelity Image Generation Using Diffusion Models</a></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="youtube-educators">YouTube Educators</h2>
<table>
<thead>
<tr>
<th>Channel</th>
<th>Description</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://www.youtube.com/@AICoffeeBreak" target="_blank" >AI Coffee Break with Letitia</a></td>
<td>Byte sized content on Diffusion models</td>
<td><a href="https://youtu.be/344w5h24-h8" target="_blank" >diffusion models explained</a></td>
</tr>
<tr>
<td><a href="https://www.youtube.com/@YannicKilcher" target="_blank" >Yannic</a></td>
<td><strong>DDPM paper explained</strong></td>
<td><a href="https://youtu.be/W-O7AZNzbzQ" target="_blank" >DDMP explained</a></td>
</tr>
<tr>
<td><a href="https://www.youtube.com/@TheAIEpiphany" target="_blank" >Aleksa Gordić - The AI Epiphany</a></td>
<td>ML coding series on Improved DDPM codebase</td>
<td><a href="https://youtu.be/y7J6sSO1k50" target="_blank" >coding series</a></td>
</tr>
</tbody>
</table>
<h2 id="github-repos">GitHub Repos</h2>
<table>
<thead>
<tr>
<th>Repo Description</th>
<th>Repo Link</th>
<th>Colab</th>
</tr>
</thead>
<tbody>
<tr>
<td>Diffusion Models Tutorial</td>
<td><a href="/posts/diffusion-models/diffusion-models-notebooks/" >Wity&rsquo;AI tutorial</a></td>
<td><a href="/posts/diffusion-models/diffusion-models-notebooks/" >Wity&rsquo;AI tutorial</a></td>
</tr>
<tr>
<td>Stable Diffusion</td>
<td><a href="https://github.com/CompVis/stable-diffusion" target="_blank" >Stable Diffusion</a></td>
<td></td>
</tr>
<tr>
<td>LucidRains Denoising Diffusion Models</td>
<td><a href="https://github.com/lucidrains/denoising-diffusion-pytorch" target="_blank" >LucidRains</a></td>
<td></td>
</tr>
<tr>
<td>Variational Diffusion models</td>
<td><a href="https://github.com/google-research/vdm" target="_blank" >VDM</a></td>
<td><a href="https://colab.research.google.com/github/google-research/vdm/blob/main/colab/SimpleDiffusionColab.ipynb" target="_blank" >
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab">
</a> <a href="https://colab.research.google.com/github/google-research/vdm/blob/main/colab/2D_VDM_Example.ipynb" target="_blank" >
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab">
</a></td>
</tr>
<tr>
<td>DDPM</td>
<td><a href="https://github.com/hojonathanho/diffusion" target="_blank" >DDPM</a></td>
<td></td>
</tr>
<tr>
<td>YiYi XU (Flax+JAX)</td>
<td><a href="https://github.com/yiyixuxu/denoising-diffusion-flax" target="_blank" >Flax Denoising Diffusion</a></td>
<td><a href="https://colab.research.google.com/github/yiyixuxu/denoising-diffusion-flax/blob/main/ddpm_flax_oxford102_end_to_end.ipynb" target="_blank" >
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab">
</a></td>
</tr>
<tr>
<td>Glide</td>
<td><a href="https://github.com/openai/glide-text2im" target="_blank" >Glide</a></td>
<td><a href="https://github.com/openai/glide-text2im/tree/main/notebooks" target="_blank" >Notebooks</a></td>
</tr>
</tbody>
</table>
<h2 id="play-with-diffusion-models">Play with Diffusion Models</h2>
<table>
<thead>
<tr>
<th>Description</th>
<th>Link</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Play with Stable Diffusion v2</td>
<td><a href="https://huggingface.co/spaces/stabilityai/stable-diffusion" target="_blank" >SD II</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Stable Boost: Personalized Photos</td>
<td><a href="https://stableboost.ai/" target="_blank" >Stable Boost</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Image variations with Stable Diffusion</td>
<td><a href="https://huggingface.co/spaces/lambdalabs/stable-diffusion-image-variations" target="_blank" >SD variations</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Gradio App for Stable Diffusion</td>
<td><a href="https://github.com/qunash/stable-diffusion-2-gui/" target="_blank" >GitHub Repo</a> <a href="https://colab.research.google.com/github/qunash/stable-diffusion-2-gui/blob/main/stable_diffusion_2_0.ipynb#scrollTo=gId0-asCBVwL" target="_blank" >
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab">
</a></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
]]></content:encoded>
    </item>
    
    <item>
      <title>Diffusion Model Jupyter and Colab Notebooks</title>
      <link>https://varun-ml.github.io/posts/diffusion-models/diffusion-models-notebooks/</link>
      <pubDate>Mon, 05 Dec 2022 12:49:17 +0530</pubDate>
      
      <guid>https://varun-ml.github.io/posts/diffusion-models/diffusion-models-notebooks/</guid>
      <description>The code accompanying the tutorials on denoising diffusion models.
Notebook Description GitHub Link Colab Basic: Predicting Original Distribution Introduces Diffusion model concepts with PyTorch Vanilla Implementation Predicting Error and Score Function Diffusion models while predicting error with PyTorch Error / Score Prediction Classifier free Guidance and other improvements Diffusion models with Time Step Embeddings, Classifier Free Guidance, and time step striding to improve sampling from a diffusion model Advanced concepts EMINST Denoising and Conditional generation Working on EMNIST data Colab EMNIST If you have suggestions, please feel free to contribute to GitHub Repo.</description>
      <content:encoded><![CDATA[<p>The code accompanying the tutorials on denoising diffusion models.</p>
<table>
<thead>
<tr>
<th style="text-align:left">Notebook</th>
<th style="text-align:left">Description</th>
<th style="text-align:left">GitHub Link</th>
<th>Colab</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Basic: Predicting Original Distribution</td>
<td style="text-align:left">Introduces Diffusion model concepts with PyTorch</td>
<td style="text-align:left"><a href="https://github.com/varun-ml/diffusion-models-tutorial/blob/master/toy-density-estimation/vanilla_diffusion_model.ipynb" target="_blank" >Vanilla Implementation</a></td>
<td><a href="https://colab.research.google.com/github/varun-ml/diffusion-models-tutorial/blob/master/toy-density-estimation/colab_vanilla_diffusion_model.ipynb" target="_blank" >
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Colab (Large)">
</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:left">Predicting Error and Score Function</td>
<td style="text-align:left">Diffusion models while predicting error with PyTorch</td>
<td style="text-align:left"><a href="https://github.com/varun-ml/diffusion-models-tutorial/blob/master/toy-density-estimation/errors.diffusion_model_interpretations.ipynb" target="_blank" >Error / Score Prediction</a></td>
<td><a href="https://colab.research.google.com/github/varun-ml/diffusion-models-tutorial/blob/master/toy-density-estimation/colab_errors.diffusion_model_interpretations.ipynb" target="_blank" >
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Colab (Large)">
</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:left">Classifier free Guidance and other improvements</td>
<td style="text-align:left">Diffusion models with Time Step Embeddings, Classifier Free Guidance, and time step striding to improve sampling from a diffusion model</td>
<td style="text-align:left"><a href="https://github.com/varun-ml/diffusion-models-tutorial/blob/master/toy-density-estimation/guidance_free_classifiers.ipynb" target="_blank" >Advanced concepts</a></td>
<td><a href="https://colab.research.google.com/github/varun-ml/diffusion-models-tutorial/blob/master/toy-density-estimation/colab_guidance_free_classifiers.ipynb" target="_blank" >
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Colab (Large)">
</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:left">EMINST Denoising and Conditional generation</td>
<td style="text-align:left">Working on EMNIST data</td>
<td style="text-align:left"><a href="https://github.com/varun-ml/diffusion-models-tutorial/blob/master/emnist-colab-notebooks/colab_EMNIST_conditional.diffusion_model.large.with_batch_norm.ipynb" target="_blank" >Colab EMNIST</a></td>
<td><a href="https://colab.research.google.com/github/varun-ml/diffusion-models-tutorial/blob/master/emnist-colab-notebooks/colab_EMNIST_conditional.diffusion_model.large.with_batch_norm.ipynb" target="_blank" >
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Colab (Large)">
</a><a href="https://colab.research.google.com/github/varun-ml/diffusion-models-tutorial/blob/master/emnist-colab-notebooks/colab_EMNIST_conditional.diffusion_model.ipynb" target="_blank" >
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Colab (Small)">
</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>If you have suggestions, please feel free to contribute to <a href="https://www.github.com/varun-ml/diffusion-models-tutorial" target="_blank" >GitHub Repo</a>.</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
